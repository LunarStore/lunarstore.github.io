[{"title":"Buildroot实现stage/target安装阶段，在执行原项目当中cmake的instll代码外再执行自定义的安装代码","url":"/2025/09/28/buildroot/stage_install/","content":"正文实现方式其实是参考 buildroot&#x2F;package&#x2F;pkg-cmake.mk 的代码，你必需知道，我们再向buildroot添加一个包时，会编写mk文件，里面会有很多预定义的宏，最常见的：XX_INSTALL_STAGING_CMDS 和 XX_INSTALL_TARGET_CMDS，两者区别是：XX_INSTALL_STAGING_CMDS是往构建根目录安装一些文件、XX_INSTALL_TARGET_CMDS是往目标（下位机根文件系统）目录安装一些文件。一旦我们的mk文件定义了这两命令，这两命令的默认值就会被覆盖。\nXX_INSTALL_STAGING_CMDS 和 XX_INSTALL_TARGET_CMDS这两命令的默认行为就是使用原项目当中的cmake install代码。我们只需要参考 buildroot&#x2F;package&#x2F;pkg-cmake.mk 文件默认安装命令的实现，然后再此基础上自定义我们的内容就能实现“双install”的效果。\n废话不多说，直接上代码：\n\n\nConfig.in文件实现如下：\nconfig BR2_PACKAGE_MYPROJECT    bool &quot;my project&quot;    help        this configuration is used to enable or disable myproject.\n\n\n首先我们不定义XX_INSTALL_STAGING_CMDS命令，查看其默认安装行为：mk文件如下：\n  ifeq ($(BR2_PACKAGE_MYPROJECT), y)# ONVIF_SRVD_VERSION =MYPROJECT_SITE = $(TOPDIR)/../external/myprojectMYPROJECT_SITE_METHOD = localMYPROJECT_DEPENDENCIES = rockchip-mppMYPROJECT_INSTALL_STAGING = YESendififeq ($(BR2_PACKAGE_MYPROJECT), y)$(eval $(cmake-package))endif\n\n  编译效果如下：\n  \n  可以看到，默认的编译效果会使用cmake文件当中的安装代码。\n\n然后我们尝试定义XX_INSTALL_STAGING_CMDS命令，查看默认行为是否会被覆盖：\n  ifeq ($(BR2_PACKAGE_MYPROJECT), y)# ONVIF_SRVD_VERSION =MYPROJECT_SITE = $(TOPDIR)/../external/myprojectMYPROJECT_SITE_METHOD = localMYPROJECT_DEPENDENCIES = rockchip-mppMYPROJECT_INSTALL_STAGING = YESendifdefine MYPROJECT_INSTALL_STAGING_CMDS    echo &quot;ok&quot;endefifeq ($(BR2_PACKAGE_MYPROJECT), y)$(eval $(cmake-package))endif\n\n  效果如下\n  \n  很明显，默认的安装行为已经被覆盖，只执行了我们后来自定义的安装代码，也就是只打印了ok。\n\n最后我们在定义XX_INSTALL_STAGING_CMDS命令最前面，参考buildroot&#x2F;package&#x2F;pkg-cmake.mk加上默认安装行为，mk文件如下：\n  ifeq ($(BR2_PACKAGE_MYPROJECT), y)# ONVIF_SRVD_VERSION =MYPROJECT_SITE = $(TOPDIR)/../external/myprojectMYPROJECT_SITE_METHOD = localMYPROJECT_DEPENDENCIES = rockchip-mppMYPROJECT_INSTALL_STAGING = YESendifdefine MYPROJECT_INSTALL_STAGING_CMDS    $(TARGET_MAKE_ENV) $(MYPROJECT_MAKE_ENV) $(MYPROJECT_MAKE) $(MYPROJECT_MAKE_OPTS) $(MYPROJECT_INSTALL_STAGING_OPTS) -C $(MYPROJECT_BUILDDIR)    echo &quot;ok&quot;endefifeq ($(BR2_PACKAGE_MYPROJECT), y)$(eval $(cmake-package))endif\n\n  结果如下：\n  \n  结果正是我们想要的，不仅执行了cmake当中的安装行为，也执行了我们后来追加的安装行为。\n\n\n\n本章完结\n","tags":["Linux应用技术笔记"]},{"title":"有关线程局部变量的随记","url":"/2024/09/30/c++/TLS/","content":"最近也是在研究workflow框架的源码，看到里面的线程池也用到了线程局部变量，但是用法和sylar的不同，特此来简单记录一下。\n场景分析在实际开发中，存在这样一种需求：需要让每一个线程用拥有自己的“独有”的全局变量，听着视乎有些没有任何逻辑。拿sylar举例子，在封装线程的时候，考虑到每个线程的名字不同，所以需要某种方式，在“全局”上定义一个变量名为：thread_name的变量，但是，此全局仅针对某一个线程。再比如，sylar在编写协程调度器时，每一个线程都需要记录自己当前正在运行的协程，以及该线程所绑定的调度协程，基于这两个协程才能实现协程任务的调度。 具体细节读者可以参考sylar协程调度器的实现。这里同样需要在“全局”上定义一个变量，此全局是各个线程所独享的“全局”。这点很重要。\n实现的可选方案C++方式定义线程局部变量C++定义线程局部变量的方式特别简单，就是在全局声明一个变量的时候，在前面加上thread_local关键字即可。在每个线程被创建初始化时，会各自创建同名，但不同对象的变量。也即：线程内部共享，但是线程之间独享的“全局变量”。\n\nstatic thread_local Scheduler* t_scheduler = nullptr;static thread_local Fiber* t_scheduler_fiber = nullptr;\n\n这种用法是C++11补充的特性。thread_local可以定义在结构体内部或者类的内部。这样默认会加上static关键字。\n使用GCC提供的方式__thread是GCC的关键字，非Unix编程或C语言标准，属于编译器自己实现。__thread只能修饰基础数据类型或者POD类型。\n所谓POD就是C语言中传统的struct类型。即无拷贝、析构函数的结构体。\n__thread也是只能用于全局存储区的变量，比如普通的全局变量或者函数内的静态变量。 声明的时候最好进行初始化。\n__thread int t_cachedTid = 0;__thread char t_tidString[32];__thread int t_tidStringLength = 6;__thread const char* t_threadName = &quot;unknown&quot;;\n\nMuduo对线程的封装就使用了此方式。\n使用POSIX标准中定义的pthread_key_tUnix编程接口的POSIX标准中定义的pthread_key_t为代表的『线程特有存储』是最传统的线程本地存储，适用于所有Unix（含Mac）与Linux系统。\n使用pthread_key_t遵循如下步骤：\n定义key\npthread_key_t key;\n\n对线程定义的key进行初始化：\nint pthread_key_create(pthread_key_t *key, void (*destr_function) (void *))\n\n和pthread_key_create对称，销毁定义的key：\nint pthread_key_delete (pthread_key_t __key);\n\n设置线程的key的值（每个线程各是各的对象）：\nint pthread_setspecific (pthread_key_t __key,\t\t\t\tconst void *__pointer)\n\n获取线程key的值：\nvoid *pthread_getspecific (pthread_key_t __key)\n\n\n本章完结\n","tags":["C++"]},{"title":"CMU15445实验总结上(Spring 2023)","url":"/2024/02/16/cmu15445/Summary/","content":"背景\n菜鸟博主是2024届毕业生，学历背景太差，导致23年秋招无果，准备奋战春招。此前有读过LevelDB源码的经历，对数据库的了解也仅限于LevelDB。奔着”有对比才能学的深“的理念，以及缓解自身就业焦虑的想法，于是乎在2024.2.16日开始CMU15445（关系性数据库）实验之旅。目前进度：将P2做完了。\n所以，本博客仅是对p1（Buffer Pool）和p2（B+Tree）的总结。\n因为C++的基础还凑合，而且时间紧迫，于是跳过了p0实验，建议之前没学过C++同学，可以做做p0以熟悉现代C++的语法。\n资源分享：\n课程主页链接：https://15445.courses.cs.cmu.edu/spring2023/\nB站有一位up主“Moody-老师”，对着CMU15445的ppt按照自己的理解复现了每一次的讲座，链接如下：https://space.bilibili.com/23722270\n\n\nProject #1 - Buffer Pool总结\n该模块是基于LRU-K Replacement Policy实现了一个内存池。简单来讲LRU-K Replacement Policy就是类似操作系统的内存页面置换。\nP1模块实现的内存池，和LevelDB的Cache有相似的作用，只是LevelDB的Cache中实现的内存替换策略是最简单的LRU算法，同时，LevelDB并没有像本实验中那样一上来就分配那么多内存进行内存复用，而是采用了动态内存分配与释放的方式，新的Block(或者Table)加到Cache中时，使用malloc分配内存，淘汰时，使用free直接释放内存。可能这就是在BusTub中叫内存池，而在LevelDB中叫Cache的原因。\nBufferPoolManager最开始就会申请N个内存页，同时这N个内存页都会被挂到free_list（空闲链表），程序在读写磁盘页时只会通过内存页间接操作，最开始在需要读取磁盘页时会直接从BufferPoolManager的free_list（空闲链表）上去取空闲的内存页，随着程序的运行，空闲链表所有的内存页都会被占用，此时如果程序还需要从磁盘上读取一个内存中不存在的磁盘页page时，就会执行LRUKReplacer::Evict，该函数会在现有的内存页中选取前k次访问的时间间隔最久的那一个内存页frame，将该页淘汰回硬盘，获取的内存页frame就被磁盘页page使用。当然，内存中正在使用的磁盘页（具体是否正在使用可以通过引用计数来判断）不会参与淘汰。\n这里建议实现一个辅助函数：\nauto BufferPoolManager::NewFrameUnlocked(frame_id_t &amp;frame_id) -&gt; Page *\n\n该函数可以依据内存页的情况，直接到free_list（空闲链表）取内存页或者调用LRUKReplacer::Evict去在已有内存页上去淘汰磁盘页，从而获取空闲的内存页。\n本实验进行的比较顺利，唯一主要弄清楚的是Frame和Page的区别。\n\nFrame(4K): 就是内存页，相应的frame_id就是Buffer Manager最开始申请的每个内存页的唯一id号。\n\nPage(4K): 就是磁盘页，相应的page_id就是磁盘上每一页的id号。\n\n\n理清这两个术语，接下来直接复现本模块的业务代码即可。\n实现小技巧：\n\n在实现class BufferPoolManager时，可以实现一个NewFrameUnlocked成员函数，方便在BufferPoolManager中获得空闲内存页(Frame)。\n\n明确class Page的读写锁是保护data_的，在class BufferPoolManager中无需对Page再加读写锁，因为对Page除data_外的成员变量的写操作都是拿到BufferPoolManager的锁后才进行的，假设其他位置的读不会有影响，所以无需再加Page的读写锁，在实现的时候可能更好理解这点。\n\nLRUKReplacer的实现中记录每个frame的k次访问历史的时间戳，可以考虑使用逻辑时间戳（因为用实际意义的时间来作为时间戳的话，可能精度不太够，所以不现实），这里的逻辑时间戳的概念在LevelDB中也有用到（即SequenceNumber），做法是这样的：定义一个全局变量，每次调用RecordAccess函数都可以将逻辑时间戳自增一，并且将其记录到对应的frame上。\n\n\nGradescope测试\n关于6个Fail的解释\n前三个是关于代码规范的测试，没有通过。。。\n后三个是关于PageGuard的测试，我的实现参考了std::lock_guard，在构造时加读写锁、在析构时，解读写锁。但是因为出现了死锁，猜测测试程序可能不支持这么实现，但其实并没有错误。而且后续的B+树索引实验在使用PageGuard时并没有出现死锁。（续注，看了一下p3，好像确实不让在构造函数中获取读写锁，已改，并通过p1的这三个测试）\n\nProject #2 - B+Tree总结\n该模块就是基于磁盘（结合Buffer Pool Manager）实现一个B+树的增删查改，另外要保证线程安全。\n和LevelDB对比，LevelDB使用LSM Tree的结构，其数据结构使用的是跳表、内存按层的方式，每层内部存储SSTable文件的元数据，作为表级索引，SSTable文件尾部存储着数据块的索引，作为块级索引，而每个数据块的尾部存储着数据索引，作为数据索引。在检索一个key-value对时，由于LevelDB一层的各个部分之间是有序不重叠的，所以以二分为主。查询方面，可能LevelDB会略差，但是增删改，LevelDB可以做到“O(1)”（忽略内存插入跳表的操作）的时间复杂度，而使用B+的数据库增删查改时间复杂度都是O(logn)。LevelDB其实将真正的删改延迟到了压缩阶段。具体细节有兴趣的读者可以自行看LevelDB的源码。\nB+树的实现考虑到递归方式调试困难，我采用了迭代式实现了B+树\n由于B+树只在叶节点存数据，所有迭代式只需要保存从根节点定位到key的路径然后根据规则进行调整即可。\n约定：\n\ninternal_page的kv关系如下：… key1 &lt;&#x3D; value1（value所代表的page中的key） &lt; key2 &lt;&#x3D; value2 …\n\n需了解的是：B+树internal_page，索引为0的entry，其key是无效value是有效。即，B+树internal_page中，key的数量 &#x3D; value数量 - 1。而leaf page中，kv数量一样。也正是存在这种关系，使得在插入和删除时，internal page的处理更为复杂。\n关于对我帮助很大的链接：\n调试B+树可视化调试方式可以参考这篇文章：https://www.cnblogs.com/wangzming/p/17479777.html\n经验贴：https://zhuanlan.zhihu.com/p/665802858?utm_id&#x3D;0\nB+树-查找我实现了一个辅助函数：\nINDEX_TEMPLATE_ARGUMENTSvoid BPLUSTREE_TYPE::FindPath(const KeyType &amp;key, Context&amp; ctx, bool write, Transaction *txn)\n\n可以查找key并保存路径。后面的插入和删除都用到了该函数。\n流程如下：\n从root_page开始，根据key找到leaf_page，同时保存沿路的internal_page。\n官方提供查找的伪代码：\n\nB+树-插入插入也是先调用FindPath记录并锁住沿路的page，然后自下而上迭代操作。\n插入需要注意的是节点达到MAXSize时需要分裂。\n对于leaf page的分裂\n假设parent_page中有如下entry：\n\n… 、&lt;kn-2, vn-2&gt;、&lt;kn-1, vn-1&gt;、&lt;kn, vn&gt; 、&lt;kn+1, vn+1&gt;…\n\n要分裂page_id为vn-1的leaf_page，流程如下：\n\n以leaf_page的1&#x2F;2处的kv作为分裂点，假设为&lt;ki, vi&gt;\n\n将leaf_page节点中，索引为i（包括i）之后的所有的entry移动到new_leaf_page（index从0开始）中。\n\n将leaf_page的next_page_id赋值给new_leaf_page的next_page_id。\n\n将new_leaf_page_id赋值给leaf_page的next_page_id。\n\n左孩子为vn-1（leaf_page的id），key为ki，右孩子为new_leaf_page_id（new_leaf_page的id）\n\n将ki插到parent_page中。（即插到parent_page的index为n的地方）\n\n\n分裂后parent_page的entry如下：\n\n… 、&lt;kn-2, vn-2&gt;、&lt;kn-1, vn-1&gt;、&lt;ki, new_page_id&gt;、&lt;kn, vn&gt; 、&lt;kn+1, vn+1&gt;…\n\n由于new_leaf_page中index为0处key还是有效的，所以，leaf page的分裂中，分裂点ki是复制并上移的。\n对于internal page的分裂\n假设parent_page中有如下entry：\n\n… 、&lt;kn-2, vn-2&gt;、&lt;kn-1, vn-1&gt;、&lt;kn, vn&gt; 、&lt;kn+1, vn+1&gt;…\n\n要分裂page_id为vn-1的internal_page，流程如下：\n\n以internal_page的1&#x2F;2处的kv作为分裂点，假设为&lt;ki, vi&gt;\n\n将internal_page节点中，索引为i（包括i）之后的所有的entry移动到new_internal_page（index从0开始）中。\n\n左孩子为vn-1（internal_page的id），key为ki，右孩子为new_internal_page_id（new_internal_page的id）\n\n将ki插到parent_page中。（即插到parent_page的index为n的地方）\n\n\n分裂后parent_page的entry如下：\n\n… 、&lt;kn-2, vn-2&gt;、&lt;kn-1, vn-1&gt;、&lt;ki, new_page_id&gt;、&lt;kn, vn&gt; 、&lt;kn+1, vn+1&gt;…\n\n注意和leaf_page分裂时的区别。\n由于new_internal_page中index为0处key是无效的，所以，internal page的分裂中，分裂点ki是上移的。\n官方提供的插入伪代码：\n\n\nGradescope测试\n关于3个Fail的解释\n这三个是关于代码规范的测试，所以没有通过。\n\nB+树-删除删除也是先调用FindPath记录并锁住沿路的page，然后自下而上迭代操作。\n删除比较麻烦，需要考虑的情况比较多，但是一步一步，理清思路还是很好实现的。按规律来说，不能拆借就一定能合并，反之亦然。至于拆借和合并的时机，本文不过多赘述。\n对于leaf page的拆借与合并\n向left sibling拆借\n假设parent_page中有如下entry：\n\n… 、&lt;kn-2, vn-2&gt;、&lt;kn-1, vn-1&gt;、&lt;kn, vn&gt; 、&lt;kn+1, vn+1&gt;、…\n\npage_id为vn-1的leaf_page向left sibling借其最右端的entry，流程如下：\n\n找到left sibling的page_id假设中是vn-2。移除并获得其最右端的entryi，假设为&lt;ki, vi&gt;\n\n根据上面的[约定1]，将parent_page中的entryn-1（&lt;kn-1, vn-1&gt;）中的key更新为：ki。\n\n将&lt;ki, vi&gt;插到page_id为vn-1的leaf page最前方。\n\n\n向left sibling拆借后，parent_page的entry如下：\n\n… 、&lt;kn-2, vn-2&gt;、&lt;ki, vn-1&gt;、&lt;kn, vn&gt; 、&lt;kn+1, vn+1&gt;、…\n\n向left sibling合并\n我实现的合并，以大页向小页追加为原则\n假设parent_page中有如下entry：\n\n… 、&lt;kn-2, vn-2&gt;、&lt;kn-1, vn-1&gt;、&lt;kn, vn&gt; 、&lt;kn+1, vn+1&gt;、…\n\npage_id为vn-1的leaf_page和left sibling合并，流程如下：\n\n找到left sibling的page_id假设中是vn-2。\n\n将leaf_page所有的entry都追加到left_sibling中去。\n\n将leaf_page的next_page_id赋值给left sibling的next_page_id。\n\n删除parent_page中index为n-1的entry。\n\n\n和left sibling合并后，parent_page的entry如下：\n\n… 、&lt;kn-2, vn-2&gt;、&lt;kn, vn&gt; 、&lt;kn+1, vn+1&gt;、…\n\n向right sibling拆借\n假设parent_page中有如下entry：\n\n… 、&lt;kn-2, vn-2&gt;、&lt;kn-1, vn-1&gt;、&lt;kn, vn&gt; 、&lt;kn+1, vn+1&gt;、…\n\npage_id为vn-1的leaf_page向right sibling借其最左端的entry，流程如下：\n\n找到right sibling的page_id假设中是vn。移除并获得其最左端的entryi，假设为&lt;ki, vi&gt;，为方便将entryi的下一个entry设为entryi+1&lt;ki+1, vi+1&gt;。\n\n根据上面的[约定1]，将parent_page中的entryn（&lt;kn, vn&gt;）中的key更新为：ki+1。\n\n将&lt;ki, vi&gt;插到page_id为vn-1的leaf page最后方。\n\n\n向right sibling拆借后，parent_page的entry如下：\n\n… 、&lt;kn-2, vn-2&gt;、&lt;ki, vn-1&gt;、&lt;ki+1, vn&gt; 、&lt;kn+1, vn+1&gt;、…\n\n向right sibling合并\n还是以大页向小页追加为原则\n假设parent_page中有如下entry：\n\n… 、&lt;kn-2, vn-2&gt;、&lt;kn-1, vn-1&gt;、&lt;kn, vn&gt; 、&lt;kn+1, vn+1&gt;、…\n\npage_id为vn-1的leaf_page和right sibling合并，流程如下：\n\n找到right sibling的page_id假设中是vn。\n\n将right_sibling所有的entry都追加到leaf_page中去。\n\n将right_sibling的next_page_id赋值给leaf_page的next_page_id。\n\n删除parent_page中index为n的entry。\n\n\n和right sibling合并后，parent_page的entry如下：\n\n… 、&lt;kn-2, vn-2&gt;、&lt;kn-1, vn-1&gt;、&lt;kn+1, vn+1&gt;、…\n\n对于internal page的拆借与合并\n向left sibling拆借\n假设parent_page中有如下entry：\n\n… 、&lt;kn-2, vn-2&gt;、&lt;kn-1, vn-1&gt;、&lt;kn, vn&gt; 、&lt;kn+1, vn+1&gt;、…\n\npage_id为vn-1的internal_page向left sibling借其最右端的entry，流程如下：\n\n找到left sibling的page_id假设中是vn-2。移除并获得其最右端的entryi，假设为&lt;ki, vi&gt;\n\n根据上面的[约定1]，parent_page的key更新如下：\n\nentryn-1（&lt;kn-1, vn-1&gt;） -&gt; entryn-1（&lt;ki, vn-1&gt;）\nentryi（&lt;ki, vi&gt;） -&gt; entryi（&lt;kn-1, vi&gt;）（描述成&lt;vi, kn-1&gt;更合适）\n\n\n将&lt;kn-1, vi&gt;按kv关系插到page_id为vn-1的internal page最前方。\n\n\n向left sibling拆借后，parent_page的entry如下：\n\n… 、&lt;kn-2, vn-2&gt;、&lt;ki, vn-1&gt;、&lt;kn, vn&gt; 、&lt;kn+1, vn+1&gt;、…\n\n向left sibling合并\n以大页向小页追加为原则\n假设parent_page中有如下entry：\n\n… 、&lt;kn-2, vn-2&gt;、&lt;kn-1, vn-1&gt;、&lt;kn, vn&gt; 、&lt;kn+1, vn+1&gt;、…\n\npage_id为vn-1的internal_page和left sibling合并，流程如下：\n\n找到left sibling的page_id假设中是vn-2。\n\n将internal_page所有的entry（包括index为0，尽管key是无效的）都追加到left_sibling中去。\n\n在left sibling中找到原来internal_page中index为0的entry（其key是无效key），将kn-1设为其key。\n\n删除parent_page中index为n-1的entry。\n\n\n和left sibling合并后，parent_page的entry如下：\n\n… 、&lt;kn-2, vn-2&gt;、&lt;kn, vn&gt; 、&lt;kn+1, vn+1&gt;、…\n\n向right sibling拆借\n假设parent_page中有如下entry：\n\n… 、&lt;kn-2, vn-2&gt;、&lt;kn-1, vn-1&gt;、&lt;kn, vn&gt; 、&lt;kn+1, vn+1&gt;、…\n\npage_id为vn-1的internal_page向right sibling借其最左端的entry，流程如下：\n\n找到right sibling的page_id假设中是vn。取right sibling的entry0的value，以及entry1的key，组成entryi，假设为&lt;k1, v0&gt;。（描述成&lt;v0, k1&gt;更合适）\n\n根据上面的[约定1]，parent_page的key更新如下：\n\nentryn（&lt;kn, vn&gt;） -&gt; entryn（&lt;k1, vn&gt;）\nentryi（&lt;k1, v0&gt;） -&gt; entryi（&lt;kn, v0&gt;）\n\n\n将&lt;k1, v0&gt;插到page_id为vn-1的internal page最后方。\n\n\n向right sibling拆借后，parent_page的entry如下：\n\n… 、&lt;kn-2, vn-2&gt;、&lt;kn-1, vn-1&gt;、&lt;k1, vn&gt; 、&lt;kn+1, vn+1&gt;、…\n\n向right sibling合并\n还是以大页向小页追加为原则\n假设parent_page中有如下entry：\n\n… 、&lt;kn-2, vn-2&gt;、&lt;kn-1, vn-1&gt;、&lt;kn, vn&gt; 、&lt;kn+1, vn+1&gt;、…\n\npage_id为vn-1的internal_page和right sibling合并，流程如下：\n\n找到right sibling的page_id假设中是vn。\n\n将right_sibling所有的entry（包括index为0，尽管key是无效的）都追加到internal_page中去。\n\n在internal_page中找到原来right_sibling中index为0的entry（其key是无效key），将kn设为其key。\n\n删除parent_page中index为n的entry。\n\n\n和right sibling合并后，parent_page的entry如下：\n\n… 、&lt;kn-2, vn-2&gt;、&lt;kn-1, vn-1&gt;、&lt;kn+1, vn+1&gt;、…\n\n官方提供的删除伪代码如下：\n\nGradescope测试\n关于3个Fail的解释\n这三个是关于代码规范的测试，所以没有通过。\n\n最终B+树的样子就是这样：\n\n大总结p1+p2两个lab，大概花了10天，效率还是比较满意的。后面还剩两个project、目前核心在春招，所以准备放一放了。\nCMU15445的lab做的还是爽的，就调试而言，起码比6.824的lab友好很多了。\n\n本章完结\n","tags":["数据库"]},{"title":"Payload-SDK自动升级","url":"/2025/06/28/dji/auto_upgrade/","content":"前言自动升级旨在通过无人机更新负载上的软件，包括不限于：Payload-SDK应用、配置文件等。对于文件的传输，大疆的Payload-SDK给我们提供了两种方式：使用FTP协议和使用大疆自研的DCFTP。我们实现的自动升级是基于FTP。所以自动升级的实现可以分成3个部分：\n\nFTP服务的搭建\nPayload-SDK应用的修改\nsh包的制作与sh打包脚本的编写\n\nFTP服务的搭建参考大疆官方做法：链接。\n首先我们需要在负载上搭建一个vsftp服务。我司使用的是正点原子的rk3588，而rk3588默认已经在buildroot当中引入vsftp服务，我们重点只需要配置ftp配置文件：&#x2F;etc&#x2F;vsftpd.conf即可。配置参考如下（完整配置，可直接copy使用）：\n\n\n# 允许匿名用户anonymous_enable=YES# 允许本地用户登录local_enable=YES# 允许用户写（即上传文件）write_enable=YES# 允许为目录配置显示信息,显示每个目录下面的message_file文件的内容dirmessage_enable=YES# 是否让系统自动维护上传和下载的日志文件xferlog_enable=YES# 是否设定FTP服务器将启用FTP数据端口的连接请求connect_from_port_20=YES# 是否禁止用户离开设置的根目录chroot_local_user=NOchroot_list_enable=NOlisten=YESallow_writeable_chroot=YES\n\n然后，参考dji本地升级文档，增加一个用户，该用户会被大疆无人机所使用：\nadduser psdk_payload_ftp --home /upgrade# 用户密码设置为：DJi_#$31# 可以使用该命令删除用户，但经过测试，正点原子的rk3588上并不支持该命令。userdel -r\n\n重启开发板，使用 ps aux | grep vsftpd 可看到运行起来的vsftpd服务。客户端使用FileZilla软件可使用用户账号psdk_payload_ftp登录ftp。\n在修改Payload-SDK应用前，需要将应用设置为开机自启动，方法如下：\n# 新建一个文件，文件前面的数字代表优先级，数值越小优先级越大，# 脚本当中添加如下内容：/usr/local/bin/dji_sdk_demo_linux&amp;# 将开发的Payload-SDK应用放到/usr/local/bin/，每次开发板启动# 时会自动在后台启动应用。vi /etc/init.d/S99autorun.shchmod +x /etc/init.d/S99autorun.sh\n\nPayload-SDK应用的修改我们使用的SDK版本为：3.11.1，确保manifold2&#x2F;application&#x2F;dji_sdk_config.h定义了CONFIG_MODULE_SAMPLE_UPGRADE_ON。然后，在main函数当中，找到自动升级初始化的代码，主要做如下修改：\nT_DjiTestUpgradeConfig testUpgradeConfig = &#123;    .firmwareVersion = firmwareVersion,    .transferType = DJI_FIRMWARE_TRANSFER_TYPE_DCFTP,    .needReplaceProgramBeforeReboot = true&#125;;        |        |        VT_DjiTestUpgradeConfig testUpgradeConfig = &#123;    .firmwareVersion = firmwareVersion,    .transferType = DJI_FIRMWARE_TRANSFER_TYPE_FTP,    .needReplaceProgramBeforeReboot = true&#125;;\n\n将transferType修改为DJI_FIRMWARE_TRANSFER_TYPE_FTP，文件使用vsftp传输。\n对于升级功能，我们主要将注意集中在upgrade目录下的代码。\n在test_upgrade.c&#x2F;h当中，首先注册了四个回调：\nT_DjiUpgradeHandler s_upgradeHandler = &#123;    .EnterUpgradeMode = DjiTest_EnterUpgradeMode,    .CheckFirmware = DjiTest_CheckFirmware,    .StartUpgrade = DjiTest_StartUpgrade,    .FinishUpgrade = DjiTest_FinishUpgrade&#125;;\n\nDjiTest_EnterUpgradeMode、DjiTest_CheckFirmware回调主要做升级前预处理。可根据实际情况实现。\nDjiTest_StartUpgrade函数在升级包被FTP传输完毕后会被回调。\nDjiTest_FinishUpgrade ？？？在升级被用户打断被调用？？？\nDjiTest_StartUpgrade函数实现如下：\nstatic T_DjiReturnCode DjiTest_StartUpgrade(void)&#123;    T_DjiOsalHandler *osalHandler = DjiPlatform_GetOsalHandler();    osalHandler-&gt;MutexLock(s_upgradeStateMutex);    s_upgradeState.upgradeOngoingInfo.upgradeProgress = 0;    s_upgradeState.upgradeStage = DJI_UPGRADE_STAGE_ONGOING;    osalHandler-&gt;MutexUnlock(s_upgradeStateMutex);    return DJI_ERROR_SYSTEM_MODULE_CODE_SUCCESS;&#125;\n\n做了两件事：首先将升级进度条置为0，然后将状态置为ONGOING。这儿状态的改变会被 DjiTest_UpgradeStartService 函数最后创建的 DjiTest_UpgradeProcessTask 线程所探测到。后面将详细讨论 DjiTest_UpgradeProcessTask 线程。\n在 DjiTest_UpgradeStartService 当中，还有一段微妙而重要的代码，如下：\n/* ... */returnCode = DjiTest_GetUpgradeRebootState(&amp;isUpgradeReboot, &amp;upgradeEndInfo);if (returnCode != DJI_ERROR_SYSTEM_MODULE_CODE_SUCCESS) &#123;    USER_LOG_ERROR(&quot;Get upgrade reboot state error&quot;);    isUpgradeReboot = false;&#125;returnCode = DjiTest_CleanUpgradeRebootState();if (returnCode != DJI_ERROR_SYSTEM_MODULE_CODE_SUCCESS) &#123;    USER_LOG_ERROR(&quot;Clean upgrade reboot state error&quot;);&#125;osalHandler-&gt;MutexLock(s_upgradeStateMutex);if (isUpgradeReboot == true) &#123;    s_upgradeState.upgradeStage = DJI_UPGRADE_STAGE_END;    s_upgradeState.upgradeEndInfo = upgradeEndInfo;&#125; else &#123;    s_upgradeState.upgradeStage = DJI_UPGRADE_STAGE_IDLE;&#125;osalHandler-&gt;MutexUnlock(s_upgradeStateMutex);/* ... */\n\nDjiTest_GetUpgradeRebootState 和 DjiTest_CleanUpgradeRebootState 函数实现在：upgrade&#x2F;test_upgrade_platform_opt.c&#x2F;h -&gt; linux&#x2F;commom&#x2F;upgrade_platform_opt&#x2F;upgrade_platform_opt_linux.c&#x2F;h。\n这段代码先读了一个本地文件，从里面获取一些升级状态，读完后立马将状态文件删除，如果文件不存在，说明是一次正常的开启启动，而不是因上一次升级而启动；如果文件存在，说明是应自动升级而重启，我们需要获取最后一次升级的状态，然后修改升级状态为END，同样的，DjiTest_UpgradeProcessTask线程会探测到升级状态的改变，然后向无人机报告自动升级结束。电脑上的DJI Assistant 2软件就会显示升级成功的画面。\n因为重启的过程也算自动升级的一部分，所以存在这样的设计：重启前保存升级状态，重启后读取上一次升级状态，通知无人机升级完成。\n下面详细看看 DjiTest_UpgradeProcessTask 函数的实现：\nstatic void *DjiTest_UpgradeProcessTask(void *arg)&#123;    T_DjiOsalHandler *osalHandler = DjiPlatform_GetOsalHandler();    T_DjiUpgradeState tempUpgradeState;    T_DjiUpgradeEndInfo upgradeEndInfo;    T_DjiReturnCode returnCode;    while (1) &#123;        osalHandler-&gt;MutexLock(s_upgradeStateMutex);        tempUpgradeState = s_upgradeState;        osalHandler-&gt;MutexUnlock(s_upgradeStateMutex);        if (tempUpgradeState.upgradeStage == DJI_UPGRADE_STAGE_ONGOING) &#123;            if (s_isNeedReplaceProgramBeforeReboot) &#123;                // Step 1 : 替换最新文件                returnCode = DjiTest_ReplaceOldProgram();                osalHandler-&gt;TaskSleepMs(1000);                osalHandler-&gt;MutexLock(s_upgradeStateMutex);                s_upgradeState.upgradeStage = DJI_UPGRADE_STAGE_ONGOING;                s_upgradeState.upgradeOngoingInfo.upgradeProgress = 20;                DjiUpgrade_PushUpgradeState(&amp;s_upgradeState);                osalHandler-&gt;MutexUnlock(s_upgradeStateMutex);                // Step 2 : 清空升级目录                returnCode = DjiTest_CleanUpgradeProgramFileStoreArea();                osalHandler-&gt;TaskSleepMs(1000);                osalHandler-&gt;MutexLock(s_upgradeStateMutex);                s_upgradeState.upgradeStage = DJI_UPGRADE_STAGE_ONGOING;                s_upgradeState.upgradeOngoingInfo.upgradeProgress = 30;                DjiUpgrade_PushUpgradeState(&amp;s_upgradeState);                osalHandler-&gt;MutexUnlock(s_upgradeStateMutex);            &#125;            // Step 3 :模拟升级过程            do &#123;                osalHandler-&gt;TaskSleepMs(1000);                osalHandler-&gt;MutexLock(s_upgradeStateMutex);                s_upgradeState.upgradeStage = DJI_UPGRADE_STAGE_ONGOING;                s_upgradeState.upgradeOngoingInfo.upgradeProgress += 10;                tempUpgradeState = s_upgradeState;                DjiUpgrade_PushUpgradeState(&amp;s_upgradeState);                osalHandler-&gt;MutexUnlock(s_upgradeStateMutex);            &#125; while (tempUpgradeState.upgradeOngoingInfo.upgradeProgress &lt; 100);            // Step 4 :将升级状态持久化保存到状态文件当中。            osalHandler-&gt;MutexLock(s_upgradeStateMutex);            s_upgradeState.upgradeStage = DJI_UPGRADE_STAGE_DEVICE_REBOOT;            s_upgradeState.upgradeRebootInfo.rebootTimeout = DJI_TEST_UPGRADE_REBOOT_TIMEOUT;            DjiUpgrade_PushUpgradeState(&amp;s_upgradeState);            osalHandler-&gt;MutexUnlock(s_upgradeStateMutex);            osalHandler-&gt;TaskSleepMs(1000); // sleep 1000ms to ensure push send terminal.            upgradeEndInfo.upgradeEndState = DJI_UPGRADE_END_STATE_SUCCESS;            returnCode = DjiTest_SetUpgradeRebootState(&amp;upgradeEndInfo);            // Step 5 :重启设备            returnCode = DjiTest_RebootSystem();            while (1) &#123;                osalHandler-&gt;TaskSleepMs(500);            &#125;        &#125; else if (s_upgradeState.upgradeStage == DJI_UPGRADE_STAGE_END) &#123;            // Step 6 :升级重启完成，升级完成被探测到。通知无人机，反馈到DJI Assistant 2 (Enterprise Series)，反馈升级成功完成。            osalHandler-&gt;MutexLock(s_upgradeStateMutex);            DjiUpgrade_PushUpgradeState(&amp;s_upgradeState);            osalHandler-&gt;MutexUnlock(s_upgradeStateMutex);        &#125;        osalHandler-&gt;TaskSleepMs(500);    &#125;&#125;\n\n完整的升级流程是：\n\nFTP文件传输完毕\n\n回调DjiTest_StartUpgrade，设置升级进度为0，升级状态为ONGOING。\n\nDjiTest_UpgradeProcessTask线程探测到升级状态为ONGOING。\n\n升级应用。反馈进度。\n\n清空升级临时文件。反馈进度。\n\n保存升级状态到状态文件。\n\n重启系统。\n\n初始化时DjiTest_UpgradeStartService读取状态文件。并设置升级状态为END。\n\nDjiTest_UpgradeProcessTask线程探测到升级状态为END。\n\n反馈进度，DJI Assistant 2收到反馈，显示升级完成。\n\n\n因为我们的升级包使用的shell嵌入二进制数据的方式，所以，相应的，DjiTest_ReplaceOldProgram的逻辑被替换为：将升级包命名为upgrade.sh，并赋予可执行权限，然后执行sh脚本，sh脚本会自动将各个文件替换为最新的。由于rk3588 reboot命令并不支持任何参数，所以需要将upgrade_platform_opt_linux.c当中DjiUpgradePlatformLinux_RebootSystem函数实现的reboot命令后面的参数删除掉。还需要注意的是，升级包一定要命名为 PSDK_APPALIAS_V01.00.00.00.bin 形式，后面的版本用户可以根据实际情况修改。\nsh包的制作与sh打包脚本的编写所谓sh包，就是将我们的二进制文件（不管是可执行文件，还是tarboll等）嵌入到shell脚本当中，简单来说，就是一个开头是一小段是shell脚本，末尾都是二进制数据（也可以是base64加密后的数据）的文件。\n通常来说，开头的那一段shell脚本是固定套路如下：\n#!/bin/shPATH=/usr/bin:/binumask 022md5=3e2ec953a6505b0ef8ad4e53babd4b43pre_install()&#123;    echo &quot;Preparing installation environment (simplified)...&quot;    mkdir ./install.tmp.$$&#125;check_sum()&#123;    if [ -x /usr/bin/md5sum ]&amp;&amp;[ -f &quot;install.tmp.$$/extract.$$&quot; ]; then        echo &quot;Checking md5...&quot;        sum_tmp=$(/usr/bin/md5sum install.tmp.$$/extract.$$ | awk &#x27;&#123;print $1&#125;&#x27;)        if [ $sum_tmp != $md5 ]; then            echo &quot;File md5 mismatch, please check file integrity, exiting!&quot;            exit 1        fi    else        echo &quot;Cannot find md5sum command or file not extracted, exiting&quot;        exit 1    fi&#125;extract()&#123;    echo &quot;Extracting files from script&quot;    line_number=`awk &#x27;/^__BIN_FILE_BEGIN__/ &#123;print NR + 1; exit 0; &#125;&#x27; &quot;$0&quot;`        # tail -n +$line_number &quot;$0&quot; &gt;./install.tmp.$$/extract.$$    tail -n +$line_number &quot;$0&quot; &gt;./install.tmp.$$/extract_tmp.$$    base64 -d ./install.tmp.$$/extract_tmp.$$ &gt;./install.tmp.$$/extract.$$&#125;install()&#123;    echo &quot;Installing (simplified)...&quot;    mv install.tmp.$$/extract.$$ install.tmp.$$/extract.tar.gz    tar -xvf install.tmp.$$/extract.tar.gz -C install.tmp.$$/    # do something    # install file...    # 将旧文件替换为./install.tmp.$$/dc_app/目录下的文件。    # mv、cp等，如下：    # mv -f ./install.tmp.$$/dc_app/dji_sdk_demo_linux /usr/local/bin/dji_sdk_demo_linux    # chmod 777 /usr/local/bin/dji_sdk_demo_linux&#125;post_install()&#123;    echo &quot;Configuring (simplified)...&quot;     echo &quot;Cleaning up temporary files&quot;    rm -rf install.tmp.$$&#125;main()&#123;    pre_install    extract    check_sum    install    post_install    exit 0&#125;main#The binary file starts below__BIN_FILE_BEGIN__\n最后的一个回车换行不要删除！！！\n我们以tarboll为例，将需要升级的文件按约定好的命名，并放到dc_app（名字可以随便取）目录下面，使用tar命令打包\ntar -czvf dc_app.tar.gz ./dc_app\n\nsh包制作流程如下：\n\n新建一个文件object.sh，将上面这段代码包括末尾的回车换行拷贝到object.sh当中。\n\n计算tarboll的md5值：\n lunar@lunar-ThinkStation-K-C2N00:~/workspace/uav_temp/build_package$ md5sum ./dc_app.tar.gze12bfd83e61975032cbc03bc570002ec  ./dc_app.tar.gz\n\n将上面sh脚本当中的全局变量md5，替换为e12bfd83e61975032cbc03bc570002ec。\n\n使用base64命令，将tarboll进行base64编码，并追加到sh文件末尾。\n base64 dc_app.tar.gz &gt;&gt; object.sh\n\n将object.sh更名为PSDK_APPALIAS_V01.00.00.01.bin。\n\n\n最后，拿到PSDK_APPALIAS_V01.00.00.01.bin升级包后，可以通过DJI Assistant 2 连接到无人机对负载进行升级。\n当然，上面1~5打包的过程可以另外编写一个shell脚本，输入需要升级的文件，然后直接输出.bin文件。参考如下：\n#!/bin/bash# =============================================# Script: create_pack.sh# Description: Package files, calculate MD5, modify object.sh and append base64 encoded data# Usage: ./script.sh &lt;version&gt; &lt;file1&gt; &lt;file2&gt; &lt;file3&gt; ...# Example: ./create_pack.sh V01.02.03.04 app.cpp app app.conf# =============================================# Validate argumentsif [ $# -lt 2 ]; then    echo &quot;Error: Insufficient arguments!&quot;    echo &quot;Usage: $0 &lt;version&gt; &lt;file1&gt; &lt;file2&gt; ...&quot;    echo &quot;Example: $0 V01.02.03.04 app.cpp app app.conf&quot;    exit 1fiVERSION=&quot;$1&quot;shift # Remove version argument, remaining are filesSOURCE_FILES=(&quot;$@&quot;)OBJECT_SCRIPT=&quot;object.sh&quot;TEMP_DIR=&quot;temp_pack_dir_$$&quot;OUTPUT_NAME=&quot;PSDK_APPALIAS_$&#123;VERSION&#125;.bin&quot;# Check required toolsif ! command -v md5sum &amp;&gt; /dev/null || ! command -v base64 &amp;&gt; /dev/null; then    echo &quot;Error: Required tools (md5sum/base64) not found!&quot;    exit 1fi# Create temporary directory structuremkdir -p &quot;$&#123;TEMP_DIR&#125;/dc_app&quot; || &#123; echo &quot;Error: Failed to create temp directory&quot;; exit 1; &#125;echo &quot;Step 1/6: Copying files to temporary directory...&quot;for file in &quot;$&#123;SOURCE_FILES[@]&#125;&quot;; do    if [ ! -f &quot;$file&quot; ]; then        echo &quot;Error: File $file not found!&quot;        exit 1    fi    cp -v &quot;$file&quot; &quot;$&#123;TEMP_DIR&#125;/dc_app/&quot; || exit 1doneecho &quot;Step 2/6: Creating compressed archive...&quot;TARBALL_NAME=&quot;dc_app_$&#123;VERSION&#125;.tar.gz&quot;tar -czvf &quot;$TARBALL_NAME&quot; -C &quot;$TEMP_DIR&quot; dc_app || &#123; echo &quot;Error: Compression failed&quot;; exit 1; &#125;echo &quot;Step 3/6: Calculating MD5 checksum...&quot;NEW_MD5=$(md5sum &quot;$TARBALL_NAME&quot; | awk &#x27;&#123;print $1&#125;&#x27;)echo &quot;Generated MD5: $NEW_MD5&quot;if [ ! -f &quot;$OBJECT_SCRIPT&quot; ]; then    echo &quot;Error: $&#123;OBJECT_SCRIPT&#125; not found!&quot;    exit 1fiecho &quot;Step 4/6: Updating MD5 in $&#123;OBJECT_SCRIPT&#125;...&quot;cp -p &quot;$OBJECT_SCRIPT&quot; &quot;$&#123;OBJECT_SCRIPT&#125;.bak&quot; || exit 1sed -i &quot;s/^md5=.*/md5=$&#123;NEW_MD5&#125;/&quot; &quot;$OBJECT_SCRIPT&quot; || &#123; echo &quot;Error: MD5 replacement failed&quot;; exit 1; &#125;echo &quot;Step 5/6: Generating final output $&#123;OUTPUT_NAME&#125;...&quot;&#123;    cat &quot;$OBJECT_SCRIPT&quot;    base64 &quot;$TARBALL_NAME&quot; || exit 1&#125; &gt; &quot;$OUTPUT_NAME&quot; || &#123; echo &quot;Error: Failed to create output file&quot;; exit 1; &#125;chmod +x &quot;$OUTPUT_NAME&quot;echo &quot;Step 6/6: Cleaning temporary files...&quot;rm -rf &quot;$TEMP_DIR&quot; &quot;$TARBALL_NAME&quot;echo &quot;========================================&quot;echo &quot;Operation completed successfully!&quot;echo &quot;Output file: $&#123;OUTPUT_NAME&#125;&quot;echo &quot;MD5 checksum: $&#123;NEW_MD5&#125;&quot;echo &quot;Backup created: $&#123;OBJECT_SCRIPT&#125;.bak&quot;echo &quot;========================================&quot;\n\n\n本章完结\n","tags":["dji-PSDK"]},{"title":"LevelDB源码阅读笔记（1、整体架构）","url":"/2024/04/18/leveldb/Framework/","content":"LeveDB源码笔记系列：\nLevelDB源码阅读笔记（0、下载编译leveldb）\nLevelDB源码阅读笔记（1、整体架构）\n前言\n对LevelDB源码的博客，我准备采用总-分的形式进行记录，我觉得先了解一下LevelDB整体流程，再往后讲解各个基础组件的话，读者会更容易理解这样设计的用意。\n性能简介数据参考自leveldb的README：https://github.com/google/leveldb。\n\n\n写性能分析如下：\n\n顺序写：保证数据按key递增插入到数据库中。因为一个sst中的数据密集，key之间差距不会特别大，所以压缩的时候相邻两层参与压缩的sst不会很多，压缩的压力不会特别大。\n\n每次写入执行sync同步操作：不测也能知道性能不会高到哪里去。\n\n随机写入：程序随机生成key值，插到leveldb中，相对于顺序写，这样会导致一个sst中的数据稀疏，key之间间隔过大，压缩的时候参与压缩的sst可能会更多，所以压缩的压力就会相对大点。\n\n修改或删除数据：结果和随机写入类似，插入和删除会导致一个sst文件的key变的稀疏，导致和随机写差不多的性能。\n\n\n数据如下：\n# 顺序写fillseq      :       1.765 micros/op;   62.7 MB/s# 每次写入执行sync同步操作fillsync     :     268.409 micros/op;    0.4 MB/s (10000 ops)# 随机写入fillrandom   :       2.460 micros/op;   45.0 MB/s# 修改或删除数据overwrite    :       2.380 micros/op;   46.5 MB/s\n\n读性能分析如下：\n\n随机读：具备不确定性，可能需要读n多个sst文件，如果sst文件不在cache中，又会涉及单个sst文件data index block的磁盘寻道和解压缩（可能需要解析n多个不在cache中的sst文件。）。所以性能上不去。\n\n顺序读：level读数据是创建一个外部归并迭代器，顺序读从一定程度上减少了对一个sst文件的反复解析（因为cache有限，被缓存的sst会被lru算法置换出去），同时也减轻了data_block的反复解压和磁盘寻道的代价。\n\n反向读：因为leveldb的sst文件data_block它的key是存在前缀压缩的，每个key相当于是一个增量key，想从一个key知道前一个key，必须从重启点重新解析，这个过程涉及到一个while循环，所以反向读性能会比顺序读要差点。\n\n\n数据如下：\n# 随机读readrandom  : 16.677 micros/op;  (approximately 60,000 reads per second)# 顺序读readseq     :  0.476 micros/op;  232.3 MB/s# 反向读readreverse :  0.724 micros/op;  152.9 MB/s\n\n根据leveldb的源码实现，考虑数据量很大，cache不命中的情况下，开销主要在每次读key至少都要进行的2次磁盘寻道（读data index block和data block），同时带来的data block的反复解压也是瓶颈所在，所以增大cache的大小一定程度上能改善leveldb的读性能。如下：\nreadrandom  : 9.775 micros/op;  (approximately 100,000 reads per second before compaction)readrandom  : 5.215 micros/op;  (approximately 190,000 reads per second after compaction)\n\n架构简介图片引用自知乎（https://zhuanlan.zhihu.com/p/206608102）若有侵权，可联系我将其删除，另外这篇文章也是讲LevelDB的，讲的非常好，推荐大家也可以去看看。LevelDB架构图如下：\n\nLevelDB内存中使用了跳表：mem_（内存跳表）、imm（只读内存跳表）。\nLevelDB在磁盘数据结构上：设计了SSTable只读磁盘数据结构。并且SST是按分层组织的。\n使用两种形式的内存跳表：mem_和imm_，mem_相当于是前台的buff供用户读写数据，而imm_为写满了的mem_充当一种临时容器，这样做的好处是能让imm_和后台Compaction线程打交道（置换）。当后台Compaction线程繁忙时，不至于说写满了的mem_没地方放，从而需要阻塞去等待后台Compaction线程的压缩。imm_的角色本质上和Muduo异步日志的AsyncLogging::buffers_是一样的。当然LevelDB的前台缓存：mem_、imm_，后台Compaction线程的设计，是双缓冲技术的实践。\nLevelDB的快照读流程\n获取快照时的SequenceNumber，不存在默认为最新的SequenceNumber。\n\n先读取内存中的跳表：mem_（内存跳表），找到返回，没找到继续。\n\n再读内存中的只读跳表：imm_（只读内存跳表），找到返回，没找到继续。\n\n按层查找sst文件。\n\n对于第0层sst文件，由于第0层sst文件之间是存在重叠的，所以会顺序查找第0层的每个和key有重叠的sst文件。（涉及多个sst文件）。\n\n对于其他层，由于leveldb在压缩的时候就保证了其sst之间是不存在重叠的，所以，其他层的查找就会采用二分的形式去定位可能包含key的sst文件，当然，这种情况下，其他层的key的查找最多涉及一个sst文件。（最多涉及一个sst文件）。\n\n\n\n找到返回OK和对应的value，找不到返回NotFound。\n\n\n从后面SST以及双层迭代器的源码分析我们就可以看到，因为sst结构设计的精妙，LevelDB对SST的查找也是使用了二分。这里就不过多赘述。\nLevelDB的读流程图如下：\n\nLevelDB的写流程（删除，插入，修改\n如果存在多个线程并发写，将请求写的所有线程放到一个队列里面，仅让位于对头的线程执行下面真正的写操作，其他线程阻塞在条件变量上。\n\n执行MakeRoomForWrite，确保mem_有足够的空间写。如果第0层的sst文件数达到阈值config::kL0_SlowdownWritesTrigger(默认8个sst)，就触发慢写（sleep1000微妙后再来尝试MakeRoomForWrite）；如果第0层的sst文件达到阈值config::kL0_StopWritesTrigger（默认12个sst），就触发停止写（阻塞在条件变量上，等待Compaction线程的唤醒）；如果mem_满了 &amp;&amp; imm_为nullptr，就将mem_转换为imm_，并清空mem_；如果mem_满了 &amp;&amp; imm_非nullptr，也即后台Compaction线程繁忙，来不及压缩imm_，就阻塞在条件变量上，等待后台Compaction线程的唤醒。\n\n做一个BuildBatchGroup的操作：将其他线程请求写的kv对 合并。\n\n将3合并的数据写到WAL日志中。\n\n将3合并的数据统一写到mem_中。\n\n唤醒其他阻塞的线程，并且从队列中移除。同时，必要的话，唤醒下一个队头执行写操作。\n\n\n在LevelDB中，删除、插入、修改均是向mem_中插入一条包装过的key（InternalKey）。\nInternalKey的组成：\n\n\nUserKey：用户提供的key。\n\nSequenceNumber：由全局逻辑计时器提供，能保证key不重复，并方便实现快照读，有效解决了并发情况下的幻读、脏读的问题。\n\nValueType：存在两个值：kTypeDeletion和kTypeValue。\n\n\n删除：是插入一条ValueType为kTypeDeletion的InternalKey。插入和修改：都是插入一条带kTypeValue的InternalKey。\n那么LevelDB是如何确定一个key最新状态是删除还是插入呢？如果对key进行了修改，又如何知道它的最新值能？删除也是插入key的话，怎么保证删除之前的key删除干净呢？\n要解决上面的问题，SequenceNumber字段就起到了关键作用。LevelDB整体上会以UserKey升序，当UserKey相同时，会以SequenceNumber降序排列。因为SequenceNumber最大的靠前排，这样就保证了在查询的时候最先获取到一个key的最新状态（是否删除、没删除的话最近一次修改的Value是多少？）。而真正的删除会在压缩阶段进行。\n同时SequenceNumber可以很好的实现快照读，而快照读就是，用户可以拍下数据库某一时刻的快照（会记录当时的SequenceNumber），然后拿着这份快照（SequenceNumber）可以随时访问当时的数据。\n引用陈硕的话，这其实是一种记流水账的思想（追加写），不管是删除、插入还是修改，都是在末尾记录一下，而不会去动以前的记录。这也是LevelDB精华所在。\nLevelDB的压缩流程压缩作为LSMTree的核心组件之一，在LevelDB中由后台专门的一个线程执行。本质上讲压缩就是剔除最老的快照时的的冗余key，只保留key最新的记录。压缩过程中同样重要的有：压缩时机？压缩哪一层的哪一个sst？对sst以什么策略进行分割？如何控制sst和祖父间的关系？为什么LevelDB设置成7层？sst文件大小为什么默认2M？每层的sst文件数量为什么默认是那么多？\n这些问题LevelDB都有具体的策略和参数进行控制，目前作者的功力有限，有些地方还未能完全明白用意，所以此处仅简述一下大概的压缩流程。\n压缩流程图如下：\n\n具体流程如下述：\n\n确定要压缩的sst文件，假设该sst文件在level层，收集level层和level + 1层和sst文件有重叠的所有sst文件。\n\n获取最老的快照的SequenceNumber，不存在默认为最新的SequenceNumber这里假设为snapshot_oldest。\n\n将1中收集到的sst文件建立成一个多路归并迭代器。并将迭代器指向首个元素（最小的）。\n\n遍历归并迭代器，迭代器的元素按UserKey升序 &amp;&amp; UserKey相同的按SequenceNumber降序排列。就同一个key来说，用以下规则决定其去留。\n\n对于SequenceNumber大于snapshot_oldest的，全部保留。而小于等于snapshot_oldest的以如下规则压缩。\n\n只对第一个小于等于snapshot_oldest的key继续下面的判断，其他的太老的直接忽略。\n\n对于带TypeDeletion标签的key（假设Internal Key为kD），如果level + 2以及以外的层不可能也包含k，该key就会在本次压缩中直接删除。如果level + 2以及以外的层可能也包含k，kD会保留下来，写到sst文件中，在随后的压缩中再一起删除干净。\n\n对于带kTypeValue标签的key保留即可。\n\n\n\n在遍历归并迭代器过程中，会按序建立sst文件，建立sst文件的过程中，还会按设定大小对sst文件进行一个切割，防止单个sst文件过大。单个sst文件与祖父层重叠大小太大也会被切割，重新建立一个新的sst文件。\n\n将输出的sst文件放到level + 1层。\n\n\n总结理想情况下:\n\nLevelDB的读操作：最多会涉及2次磁盘IO（读data index block和data block）。\n\nLevelDB的写操作（插入、修改、删除）：是直接插到内存，不涉及磁盘IO。但在随后的压缩的过程中，可能会涉及一些磁盘IO（在归并迭代器中遍历sst时涉及磁盘读，将压缩的数据输出到磁盘时涉及磁盘顺序写）。\n\n\n\n本章完结\n","tags":["存储"]},{"title":"VS2022搭建FFMPEG + Opencv开发环境 + 如何打包项目让程序也能独立跑在其他人的电脑上？","url":"/2024/10/22/else/win_vs_package/","content":"前言本文的名字应该是我所写过的博客当中最长的，但内容以精简且保证实用为原则！\n正文首先是ffmpeg环境搭建流程如下：\n\n在网上下载已经被编译成动态库版的ffmpeg，我的是：ffmpeg-N-113099-g46775e64f8-win64-gpl-shared。\n\n\n\n\n将 ffmpeg-N-113099-g46775e64f8-win64-gpl-shared&#x2F;include 和 ffmpeg-N-113099-g46775e64f8-win64-gpl-shared&#x2F;lib 两个目录都复制到项目源文件当中即和.vcxproj后缀的文件同一级。\n\n将 ffmpeg-N-113099-g46775e64f8-win64-gpl-shared&#x2F;bin目录下，所有的.dll后缀的文件复制到.vcxproj后缀的文件同一级目录中。\n\n在vs2022中，右键项目，选择properties -&gt; Configuration Properties -&gt; VC++ Directories：\n\n修改 General， 在Include Directories当中添加一项：.&#x2F;include。\n\n修改 General， 在Library Directories当中添加：.&#x2F;lib。\n\n\n\n选择properties -&gt; Configuration Properties -&gt; C&#x2F;C++ -&gt; General：\n\n在Additional Include Directories中添加一项：.&#x2F;include。\n\n\n选择properties -&gt; Configuration Properties -&gt; Linker -&gt; General：\n\n在Additional Library Directories中添加一项：.&#x2F;lib。\n\n\n选择properties -&gt; Configuration Properties -&gt; Linker -&gt; Input：\n\n在Additional Dependencies中添加依赖库的名称：\n avcodec.libavformat.libavutil.libavdevice.libavfilter.libpostproc.libswresample.libswscale.lib\n\n\n点击右下角的应用按钮，保存退出。\n\n\n运行如下测试代码：\n#include &lt;iostream&gt;extern &quot;C&quot; &#123;#include &quot;libavcodec/avcodec.h&quot;#include &quot;libavformat/avformat.h&quot;&#125;#include&lt;opencv2/core/core.hpp&gt;#include&lt;opencv2/highgui/highgui.hpp&gt;#include&lt;opencv2/imgproc.hpp&gt;int main()&#123;   std::cout &lt;&lt; &quot;Hello World!\\n&quot;;   printf(&quot;%s\\n&quot;, avcodec_configuration());   return 0;&#125;\n\n\n输出一堆有关ffmpeg的版本以及参数信息即为配置成功\n\n然后是opencv的运行环境配置：环境搭建流程如下：\n\n同样可以在网上找到动态库版的opencv。\n\n将 opencv&#x2F;build&#x2F;include 和 opencv\\build\\x64\\vc15\\lib 两个目录都复制到项目源文件当中即和.vcxproj后缀的文件同一级。（PS，如果项目目录因为引入其他头文件或库，include或lib目录已经存在，则将opencv&#x2F;build&#x2F;include和opencv\\build\\x64\\vc15\\lib下的所有文件手动复制到项目中对应的目录即可）\n\n将 opencv\\build\\x64\\vc15\\bin 目录下，所有的.dll（更严谨一点是非.exe的所有文件）后缀的文件复制到.vcxproj后缀的文件同一级目录中。\n\n重复上节3 ~ 5步骤。\n\n选择properties -&gt; Configuration Properties -&gt; Linker -&gt; Input：\n\n在Additional Dependencies中添加依赖库的名称：\n opencv_world440.lib# 如果你需要同时安装opencv和ffmpeg的话，可以直接一次性添加如下依赖# avcodec.lib# avformat.lib# avutil.lib# avdevice.lib# avfilter.lib# postproc.lib# swresample.lib# swscale.lib# opencv_world440.lib\n\n\n点击右下角的应用按钮，保存退出。\n\n\n运行如下测试代码：\n#include &lt;opencv2/opencv.hpp&gt;using namespace cv;int main() &#123;\tconst char* pic_path = &quot;任意一张你电脑上的图片路径&quot;;\tMat pic = imread(pic_path, 1);\timshow(&quot;Hello World!&quot;, pic);\twaitKey();\treturn 0;&#125;\n\n\n可以看到用opencv的api成功显示了一张图片，即为配置成功。\n\n在windows下对VS2022项目程序进行打包最后就是对项目进行打包，实现让其有完整的依赖库，在其他人的电脑也能运行你的应用程序。 说简单点其实这个过程就各种动态库、静态库的拷贝。你找一台没任何环境的新电脑作为测试环境，让你的程序在它上面运行，运行的时候会崩溃，根据报错来一点一点将所缺失的库拷贝到应用程序所在的目录当中。 这里记录了一下只引入opencv和ffmpeg情况下打包的流程。当然微软还提供了更为强大的打包方式：Microsoft Visual Studio Installer Projects。本文所讲解的打包方式是为这些特定需求人群服务的：不需要花里胡朝的方式，只求方便的一个打包方式。\n\n将上方菜单栏的Debug改成Release。\n\n再次根据在配置ffmpeg和opencv时的过程重新配置项目的properties。\n\n最后修改：properties -&gt; Configuration Properties -&gt; C&#x2F;C++ -&gt; Code Generation -&gt; Runtime Library -&gt; Multi-threaded DLL (&#x2F;MD)\n\n编译无报错\n\n新建一个目录app\n\n将项目根目录x64&#x2F;Release&#x2F;下所有文件拷贝到app\n\n将前面配置的include、lib文件夹拷贝到app\n\n将.dll文件拷贝到app\n\n完成迁移，app可独立在任何人的电脑上运行。\n\n\n\n本章完结\n","tags":["杂项"]},{"title":"LevelDB源码阅读笔记（2、SSTable源码分析）","url":"/2024/04/21/leveldb/SSTable/","content":"LeveDB源码笔记系列：\nLevelDB源码阅读笔记（0、下载编译leveldb）\nLevelDB源码阅读笔记（1、整体架构）\nLevelDB源码阅读笔记（2、SSTable源码分析）\n前言\n本文讨论的Key都是LevelDB里面的Internal Key，至于什么是Internal Key请看LevelDB源码阅读笔记（1、整体架构）。另外本文的源码分析涉及Bloom过滤器的应用，不懂什么是Bloom过滤器的读者需自行学习。\nSSTable的结构有关的的文件全部放在table目录下：\n\n\n\nblock_builder：组建一个block结构。data_block、filter_index_block、data_indedx_block都在使用该结构。\n\nblock：创建一个迭代器，对一个block结构进行读取。该迭代器也是LevelDB最小单元的迭代器。\n\nfilter_block：在LevelDB中就是bloom data block，区别于block结构。\n\nformat：对SSTable的Footer进行序列化和反序列化，此外还有从sst文件中读取block结构的功能。\n\niterator：对单层迭代器做定义。\n\nmerger：对归并迭代器做定义。\n\ntable_builder：组建一个sst文件。\n\ntable：解析一个sst文件。\n\ntwo_level_iterator：对双层迭代器做定义。该迭代器的设计非常精妙，是LevelDB查找的基石。\n\n\nSSTable整体结构总览SSTable文件结构如图所示：\n\n对各个部分的解释：\n\nData Block：递增的存储键值对。\n\nFilter Data Block：本质是bloom过滤器映射块，映射了Data Block中的每一条键值对，减少了无效查询的IO，优化了查询性能。\n\nFilter Data Index Block：内部结构和Data Block是一样的，只不过Key代表Filter Data Block的名字，而Value存的是Filter Data Block的文件偏移和大小。\n\nData Index Block：内部结构和Data Block是一样的，只不过Key是Data Block最后一条键值对的Key，而Value存的是Data Block的文件偏移和大小。\n\nFooter：存放Meta Index Handle + Data Index Handle + Magic Number。\n\n\n注意，这里的Meta Index Handle就是Filter Data Index Block的文件偏移和大小； Data Index Handle就是Data Index Block的文件偏移和大小。\nSSTable的写入流程通过leveldb::TableBuilder组建一个sst的流程如下：\n\n通过Add函数，不断将输入的&lt;key, value&gt;写入data_block，同时在filter_data_block（（bloom过滤器）中记录该key。当一个data_block大于4k（默认情况下是4k）时，就将data_block  flush到文件并切换新的data_block继续写。同时还会记录每块data_block最后一条key值以及data_block到文件开始的偏移 和 其大小（ Data Block Handle）。将这些信息也作为一对键值对（&lt;data_block_last_key, data_block_handle&gt;）写入到data_index_block中。\n\n当sst达到2M（默认是2M）就会调用Finish函数结束sst的构建，结束过程如下。\n\n将不足4k的data_block 像1一样Flush到文件。\n\n将filter_data_block写到文件，创建filter_data_block_handle记录其文件偏移和大小\n\n将filter_name作为key、filter_data_block_handle作为value写到一个filter_data_index_block中，并将filter_data_index_block写入文件，创建filter_data_index_block_handle记录filter_data_index_block的文件偏移和大小。\n\n将data_index_block写到文件，创建data_index_block_handle记录其偏移和大小。\n\n将filter_data_index_block_handle（Meta Index Handle）、data_index_block_handle（Data Index Handle）以及MagicNumber作为Footer一起写到文件中。至此sst文件构建完毕。\n\n\n部分代码如下：\nvoid TableBuilder::Add(const Slice&amp; key, const Slice&amp; value) &#123;  Rep* r = rep_;  assert(!r-&gt;closed);  if (!ok()) return;  if (r-&gt;num_entries &gt; 0) &#123;    // 输入的key一定是递增的！    assert(r-&gt;options.comparator-&gt;Compare(key, Slice(r-&gt;last_key)) &gt; 0);  &#125; // 延迟追加index_block，因为需要计算合适的key  if (r-&gt;pending_index_entry) &#123;     // 下一个datablock写入第一条数据之前，data_block为空    assert(r-&gt;data_block.empty());    // 保证大于等于真正的last_key且小于key的前提压缩last_key    r-&gt;options.comparator-&gt;FindShortestSeparator(&amp;r-&gt;last_key, key);    std::string handle_encoding;    r-&gt;pending_handle.EncodeTo(&amp;handle_encoding);    r-&gt;index_block.Add(r-&gt;last_key, Slice(handle_encoding));    r-&gt;pending_index_entry = false;  &#125;  if (r-&gt;filter_block != nullptr) &#123;    // filter和datablock对应    r-&gt;filter_block-&gt;AddKey(key);  &#125;  r-&gt;last_key.assign(key.data(), key.size());  r-&gt;num_entries++;  r-&gt;data_block.Add(key, value);  const size_t estimated_block_size = r-&gt;data_block.CurrentSizeEstimate();  if (estimated_block_size &gt;= r-&gt;options.block_size) &#123;  // 未压缩的block    Flush();  &#125;&#125;Status TableBuilder::Finish() &#123;  Rep* r = rep_;   // 将不足4k的datablock存到文件中 data_block  Flush();   assert(!r-&gt;closed);  r-&gt;closed = true;  BlockHandle filter_block_handle, metaindex_block_handle, index_block_handle;  // Write filter block// 将filter_data_block写到文件，记录其文件偏移和大小  if (ok() &amp;&amp; r-&gt;filter_block != nullptr) &#123; //filter_data_block    WriteRawBlock(r-&gt;filter_block-&gt;Finish(), kNoCompression,                  &amp;filter_block_handle);  &#125;  // Write metaindex block// 将filter_data_block的信息写到一个filter_data_index_block中  if (ok()) &#123;     // filter_data_index_block    BlockBuilder meta_index_block(&amp;r-&gt;options);    if (r-&gt;filter_block != nullptr) &#123;      // Add mapping from &quot;filter.Name&quot; to location of filter data      std::string key = &quot;filter.&quot;;      key.append(r-&gt;options.filter_policy-&gt;Name());      std::string handle_encoding;      filter_block_handle.EncodeTo(&amp;handle_encoding);      meta_index_block.Add(key, handle_encoding);    &#125;    // 并将filter_data_index_block写入文件，记录filter_data_index_block的文件偏移和大小。    // TODO(postrelease): Add stats and other meta blocks    WriteBlock(&amp;meta_index_block, &amp;metaindex_block_handle);  &#125;  // Write index block  if (ok()) &#123;     // data_index_block    if (r-&gt;pending_index_entry) &#123;      r-&gt;options.comparator-&gt;FindShortSuccessor(&amp;r-&gt;last_key);      std::string handle_encoding;      r-&gt;pending_handle.EncodeTo(&amp;handle_encoding);      r-&gt;index_block.Add(r-&gt;last_key, Slice(handle_encoding));      r-&gt;pending_index_entry = false;    &#125;    // 将data_index_block写到文件，并记录其偏移和大小。    WriteBlock(&amp;r-&gt;index_block, &amp;index_block_handle);  &#125;  // Write footer   // 将filter_data_index_block、data_index_block的文件偏移和大小以及MagicNumber作为Footer一起写到文件中。至此sst文件构建完毕。  if (ok()) &#123;    Footer footer;    footer.set_metaindex_handle(metaindex_block_handle);    footer.set_index_handle(index_block_handle);    std::string footer_encoding;    footer.EncodeTo(&amp;footer_encoding);    r-&gt;status = r-&gt;file-&gt;Append(footer_encoding);    if (r-&gt;status.ok()) &#123;      r-&gt;offset += footer_encoding.size();    &#125;  &#125;  return r-&gt;status;&#125;\n\n这里还需注意，凡是block的结构在写入SSt文件前都会被压缩，比如data_block、filter_data_index_block、data_index_block，而filter data block是未被压缩的！\n我们可以看到Add函数中有对key做了一个断言，断言内容大致是要保证key必须是递增的，所以SST文件内部就是有序的键值对，data_block内部是递增 -&gt; 每个data_block最后一条key之间也是递增 -&gt; data_index_block的key值也会是递增，这样带来的好处就是可以通过二分查找的方式帮助我们快速定位一个key的位置，在下面Block块的解析我们就会恍然大悟。\nSSTable的解析流程了解了SSTable的写入流程，其解析流程就非常明了了，如下。\n\n先读取SSTable的48字节的Footer，可以获取到Meta Index Handle和Data Index Handle。\n\n通过Data Index Handle可以读到SSTable文件中的data_index_block。并将data_index_block存到rep_成员中。有了data_index_block就可以定位所有的data_block了。\n\n通过Meta Index Handle找到filter_data_index_block。\n\n通过filter_data_index_block从sst中读出filter_data_block，并将其存入rep_的成员中。在查找sst的键值对时，先会在filter_data_block（bloom过滤器）中查看键是否存在，存在，才再磁盘上去读取和解压具体的data_block。\n\n至此解析完毕，有了filter_data_block和data_index_block足以定位每一个data_block以及里面的键值对。\n\n\n需要注意的是和构建SSTable对称的是，每一个data_block、data_index_block、filter_data_index_block从文件中读取后都会涉及到解压！\nSSTable的查询流程不使用双层迭代器\n\n调用Table::InternalGet接口。\n\n获取data_index_block（结构和data_block一样）的迭代器，对其进行二分查找，找到第一个大于等于traget的data_block的handle_value。\n\n到filter_data_lock中验证traget的存在性，不存在直接返回，否者继续。\n\n根据handle_value从磁盘中读取data_block。（当然data_block可能被LRU Cache缓存下来）。\n\n根据data_block创建一层迭代器对traget进行二分查找。\n\n\n使用双层迭代器\n调用Table::NewIterator接口，创建一个二层迭代器，该迭代器里面有两个迭代器，一个index迭代器，另一个是data迭代器。基于SSTable创建的双层迭代器其index迭代器是index_data_block的单层迭代器，data迭代器是每一个动态创建的data_block的单层迭代器。双层迭代器的巧妙之处就是能够根据index迭代器，去动态创建data迭代器。这样的特性在进行外部归并排序时是非常有用的！此外双层迭代器还有嵌套的玩法，LevelDB就有利用嵌套双层迭代器去遍历LevelDB的某一层（level &gt; 0，不包括0层，因为0层不具备有序不重叠的特性）。\nData Block的整体结构总览Data Block的具体细节如下图：\n\n这里难理解的可能就是前缀压缩那块，举个例子有连续的键值对如下：\n\n{ &lt;abcdef, 12345&gt;, &lt;abcdga, 67890&gt;, &lt;abcff, 119&gt;, &lt;abcfff, 200&gt; }\n\n重启点间隔为2的话，在data_block中会被前缀压缩成如下结构：\n\n前缀压缩的好处是，sst消耗的磁盘空间减少了，但其坏处是在查找的时候，考虑到复原重启点间隔内的键需要参考前一个条目的键，就需要从重启点开始进行一段顺序查找。幸运的是，LevelDB提供了设置重启点间隔的参数，Options::block_restart_interval默认值是16，当block_restart_interval太大，会导致压缩率更大而查询性能会降低；当block_restart_interval太小，会导致压缩率更小而查询性能会增加。用户可以根据实际的需求调节参数进行适当的取舍。\nData Block的写入流程Data Block的写入流程相对容易理解，如下：\n\n写入key的共享长度。当然如果位于重启点，共享长度恒为0\n\n写入key的非共享长度。\n\n写入value长度。\n\n写入key的非共享长度。\n\n写入value值。\n\n当data_block大于4K，就会调用Finish函数，Finish函数会将重启数组以及其大小追加到data_block的末尾。\n\ndata_block在写入文件前会进行一次算法层面的压缩（kSnappyCompression &#x2F; kZstdCompression），并会进行校验和的计算。压缩类型和校验和 会作为data_block的Footer一起写入sst文件中。\n\n\n代码如下：\nclass BlockBuilder &#123;    /*    *   ...    */ private:  const Options* options_;  std::string buffer_;              // Destination buffer  std::vector&lt;uint32_t&gt; restarts_;  // Restart points  int counter_;                     // Number of entries emitted since restart  bool finished_;                   // Has Finish() been called?  std::string last_key_;&#125;;void BlockBuilder::Reset() &#123;  buffer_.clear();  restarts_.clear();  restarts_.push_back(0);  // First restart point is at offset 0  counter_ = 0;  finished_ = false;  last_key_.clear();&#125;size_t BlockBuilder::CurrentSizeEstimate() const &#123;  return (buffer_.size() +                       // Raw data buffer          restarts_.size() * sizeof(uint32_t) +  // Restart array          sizeof(uint32_t));                     // Restart array length&#125;Slice BlockBuilder::Finish() &#123;  // Append restart array  for (size_t i = 0; i &lt; restarts_.size(); i++) &#123;    PutFixed32(&amp;buffer_, restarts_[i]);  &#125;  PutFixed32(&amp;buffer_, restarts_.size());  finished_ = true;  return Slice(buffer_);&#125;//递增的顺序加进来//使用前缀共享的方式微压缩了一下void BlockBuilder::Add(const Slice&amp; key, const Slice&amp; value) &#123;   Slice last_key_piece(last_key_);  assert(!finished_);  assert(counter_ &lt;= options_-&gt;block_restart_interval);  assert(buffer_.empty()  // No values yet?         || options_-&gt;comparator-&gt;Compare(key, last_key_piece) &gt; 0);  size_t shared = 0;  if (counter_ &lt; options_-&gt;block_restart_interval) &#123;    // See how much sharing to do with previous string    const size_t min_length = std::min(last_key_piece.size(), key.size());    while ((shared &lt; min_length) &amp;&amp; (last_key_piece[shared] == key[shared])) &#123;      shared++;    &#125;  &#125; else &#123;    // 记录重启点    // Restart compression    restarts_.push_back(buffer_.size());    counter_ = 0;  &#125;  const size_t non_shared = key.size() - shared;  // Add &quot;&lt;shared&gt;&lt;non_shared&gt;&lt;value_size&gt;&quot; to buffer_  PutVarint32(&amp;buffer_, shared);  PutVarint32(&amp;buffer_, non_shared);  PutVarint32(&amp;buffer_, value.size());  // Add string delta to buffer_ followed by value  buffer_.append(key.data() + shared, non_shared);  buffer_.append(value.data(), value.size());  // Update state  last_key_.resize(shared);  last_key_.append(key.data() + shared, non_shared);  assert(Slice(last_key_) == key);  counter_++;&#125;\n\n在Add函数的前面，我们又可以看到保证key递增的断言。\nData Block的解析和查询流程解析\n先根据data_block的Footer核对校验和，然后根据Footer的指示对data_block进行解压。\n\n获取重启数组大小，计算重启数组在data_block的偏移。将重启数组的偏移记录在Block::restart_offset_中.\n\n\n伪代码如下：\ninline uint32_t Block::NumRestarts() const &#123;  assert(size_ &gt;= sizeof(uint32_t));  return DecodeFixed32(data_ + size_ - sizeof(uint32_t));&#125;//Block ReaderBlock::Block(const BlockContents&amp; contents)    : data_(contents.data.data()),      size_(contents.data.size()),      owned_(contents.heap_allocated) &#123;        // 这里的size_就是data_block的大小，减去重启数组本身和重启数组大小值的大小，即可定位到重启数组的偏移。    restart_offset_ = size_ - (1 + NumRestarts()) * sizeof(uint32_t);&#125;// 从sst中根据handle去解析一个data_blockStatus ReadBlock(RandomAccessFile* file, const ReadOptions&amp; options,                 const BlockHandle&amp; handle, BlockContents* result) &#123;  result-&gt;data = Slice();  result-&gt;cachable = false;\t// 是否可cache  result-&gt;heap_allocated = false;\t// 是否需要手动free？  // Read the block contents as well as the type/crc footer.  // See table_builder.cc for the code that built this structure.  size_t n = static_cast&lt;size_t&gt;(handle.size());  char* buf = new char[n + kBlockTrailerSize];  Slice contents;  Status s = file-&gt;Read(handle.offset(), n + kBlockTrailerSize, &amp;contents, buf);  if (!s.ok()) &#123;    delete[] buf;    return s;  &#125;  if (contents.size() != n + kBlockTrailerSize) &#123;    delete[] buf;    return Status::Corruption(&quot;truncated block read&quot;);  &#125;\t// 检测校验和。  // Check the crc of the type and the block contents  const char* data = contents.data();  // Pointer to where Read put the data  if (options.verify_checksums) &#123;    const uint32_t crc = crc32c::Unmask(DecodeFixed32(data + n + 1));    const uint32_t actual = crc32c::Value(data, n + 1);    if (actual != crc) &#123;      delete[] buf;      s = Status::Corruption(&quot;block checksum mismatch&quot;);      return s;    &#125;  &#125;  switch (data[n]) &#123;    case kNoCompression:\t// 解压...      break;    case kSnappyCompression: &#123;\t// 解压...      break;    &#125;    case kZstdCompression: &#123;\t// 解压...      break;    &#125;    default:      delete[] buf;      return Status::Corruption(&quot;bad block type&quot;);  &#125;  return Status::OK();&#125;\n\n至此data_block的解析完毕，接下来介绍data_block如何查询一个key。\n查询首先要明确的是，重启点的key是完整的，可以直接进行比较。\n\n对重启数组进行二分，查询最后一个小于target的重启点\n\n从被定位到的重启点开始，顺序扫描去解析还原每一个key，知道找到了第一个大于等于target的entry。\n\n\n二分代码如下：\nclass Block::Iter : public Iterator &#123;  void Seek(const Slice&amp; target) override &#123;    uint32_t left = 0;    uint32_t right = num_restarts_ - 1;    int current_key_compare = 0;    if (Valid()) &#123;      current_key_compare = Compare(key_, target);      if (current_key_compare &lt; 0) &#123;    // 初步缩小二分范围        // key_ is smaller than target        left = restart_index_;      &#125; else if (current_key_compare &gt; 0) &#123;        right = restart_index_;      &#125; else &#123;        // We&#x27;re seeking to the key we&#x27;re already at.        return;      &#125;    &#125;    //找最后一个小于target的重启点    while (left &lt; right) &#123;      //向上取整以免只有两个元素时死循环      uint32_t mid = (left + right + 1) / 2;      uint32_t region_offset = GetRestartPoint(mid);      uint32_t shared, non_shared, value_length;      const char* key_ptr =          DecodeEntry(data_ + region_offset, data_ + restarts_, &amp;shared,                      &amp;non_shared, &amp;value_length);      if (key_ptr == nullptr || (shared != 0)) &#123;        CorruptionError();        return;      &#125;      Slice mid_key(key_ptr, non_shared);   //重启点的非共享key就是完整的key      if (Compare(mid_key, target) &lt; 0) &#123;        left = mid;      &#125; else &#123;        right = mid - 1;      &#125;    &#125;    assert(current_key_compare == 0 || Valid());    bool skip_seek = left == restart_index_ &amp;&amp; current_key_compare &lt; 0;   //如果target所在重启区域和原本的key所在重启区域相同，且target在原本key的右方，就不用重置重启点    if (!skip_seek) &#123;      SeekToRestartPoint(left);    &#125;    // 顺序解析，找第一个大于等于target的key    while (true) &#123;         if (!ParseNextKey()) &#123;        return;      &#125;      if (Compare(key_, target) &gt;= 0) &#123;        return;      &#125;    &#125;  &#125;&#125;;\n\nFilter Data Block整体结构总览filter_data_block的结构如下：\n\n注意，这里filter data本质就是bloom过滤器的位数组。\n简单理解，LevelDB为每个data_block块都分配了一个filter_data，深入源码分析，其实随着用户对data_block默认大小的调节，当data_block够小时，不同的data_block可能共用同一个filter_data的（多个data_block共用偏移数组的同一个entry）；当data_block够大时，一个data_block可能会占用偏移数组的多个entry（1个、2个、3个…） 我们在filter_data_block的写入分析中做更详细的解释。\nFilter Data Block的写入流程filter_data_block的写入流程如下：\n\n搜集传进来的key。\n\n每当一个data_block写入完毕，SST那里就会调用一下FilterBlockBuilder::StartBlock，该函数会根据传进来的下一个data_block在文件中的偏移（假设为data_block_next_offset）计算data_block_next_offset在偏移数组中的下标。这里的计算是指求data_block_next_offset &#x2F; kFilterBase值，并且其间的间隔，都会以上一个filter_data的结尾偏移来填充。\n\nSSTable调用的TableBuilder::Finish()时，filter_data_block会调用自己的FilterBlockBuilder::Finish()，计算剩余的filter_data的bloom位数组，然后将偏移数组、将偏移数组本身的偏移、kFilterBaseLg（11）写到filter_block中。\n\n\n代码如下：\n// Generate new filter every 2KB of datastatic const size_t kFilterBaseLg = 11;static const size_t kFilterBase = 1 &lt;&lt; kFilterBaseLg;FilterBlockBuilder::FilterBlockBuilder(const FilterPolicy* policy)    : policy_(policy) &#123;&#125;// block_offset 是一个data块（被压缩过的）在sst文件中的偏移void FilterBlockBuilder::StartBlock(uint64_t block_offset) &#123;  uint64_t filter_index = (block_offset / kFilterBase);  assert(filter_index &gt;= filter_offsets_.size());  // 重点!!!  // block_ofsset就是block_data到文件头的偏移，filter_offset.size()要和每次传进来的block_offset / kFilterBase算出来的index保持一致,  // 过大就用filterdata结束偏移填充。  // 两次传入的block_offset计算出来的filter_index差值可能过大（大于2），  // 此时除了第一次调用的GenerateFilter填充上一个bllom_filter的起点外，  // 其余的GenerateFilter循环直接用上一个bloom_filter终点填充  while (filter_index &gt; filter_offsets_.size()) &#123;    GenerateFilter();  &#125;&#125;void FilterBlockBuilder::AddKey(const Slice&amp; key) &#123;  Slice k = key;  start_.push_back(keys_.size());  keys_.append(k.data(), k.size());&#125;Slice FilterBlockBuilder::Finish() &#123;  if (!start_.empty()) &#123;    GenerateFilter();  &#125;  // Append array of per-filter offsets  const uint32_t array_offset = result_.size();  for (size_t i = 0; i &lt; filter_offsets_.size(); i++) &#123;    PutFixed32(&amp;result_, filter_offsets_[i]);  &#125;  PutFixed32(&amp;result_, array_offset);  result_.push_back(kFilterBaseLg);  // Save encoding parameter in result  return Slice(result_);&#125;void FilterBlockBuilder::GenerateFilter() &#123;  const size_t num_keys = start_.size();  if (num_keys == 0) &#123;    // Fast path if there are no keys for this filter    filter_offsets_.push_back(result_.size());    return;  &#125;  // Make list of keys from flattened key structure  start_.push_back(keys_.size());  // Simplify length computation // 方便长度计算  tmp_keys_.resize(num_keys);  for (size_t i = 0; i &lt; num_keys; i++) &#123;   // 从keys中提取每个key，并保存到tmp_keys中    const char* base = keys_.data() + start_[i];    size_t length = start_[i + 1] - start_[i];    tmp_keys_[i] = Slice(base, length);  &#125;  // Generate filter for current set of keys and append to result_.  filter_offsets_.push_back(result_.size());// 压入当前bloom块的偏移量  policy_-&gt;CreateFilter(&amp;tmp_keys_[0], static_cast&lt;int&gt;(num_keys), &amp;result_);  tmp_keys_.clear();  keys_.clear();  start_.clear();&#125;\n\n这段代码可能难理解的就是：FilterBlockBuilder::StartBlock函数里面的while循环，使用while循环的目的是，data_block的文件偏移到filter_data_block的偏移数组映射的同步。如果data_block的默认大小被用户调的特别大，每个data_block的 data_block的文件偏移 &#x2F; kFilterBase 的值差异会非常大，反映在filter_data_block的偏移数组上就是：相邻两个data_block在偏移数组上的索引相差会很大（相差3个、4个、n个索引。）而这段filter data offset所有的值会用后一个data_block的filter_data的起始偏移填充。所以while循环就是起到同步索引的效果。这里要好好体会其涉设计的妙处。\nFilter Data Block的读取流程Filter Data Block在构造函数中会：提取偏移数组的偏移、kFilterBaseLg（11），计算偏移数组的大小。\n\n根据传进来的block_offset计算它在偏移数组的下标。\n\n读取filter_data。\n\n通过bloom位数组判断target是否存在。\n\n\n部分代码如下：\nFilterBlockReader::FilterBlockReader(const FilterPolicy* policy,                                     const Slice&amp; contents)    : policy_(policy), data_(nullptr), offset_(nullptr), num_(0), base_lg_(0) &#123;  size_t n = contents.size();  if (n &lt; 5) return;  // 1 byte for base_lg_ and 4 for start of offset array  base_lg_ = contents[n - 1];  uint32_t last_word = DecodeFixed32(contents.data() + n - 5);  if (last_word &gt; n - 5) return;  data_ = contents.data();  offset_ = data_ + last_word;  num_ = (n - 5 - last_word) / 4;&#125;bool FilterBlockReader::KeyMayMatch(uint64_t block_offset, const Slice&amp; key) &#123;  uint64_t index = block_offset &gt;&gt; base_lg_;  if (index &lt; num_) &#123;    uint32_t start = DecodeFixed32(offset_ + index * 4);    uint32_t limit = DecodeFixed32(offset_ + index * 4 + 4);    if (start &lt;= limit &amp;&amp; limit &lt;= static_cast&lt;size_t&gt;(offset_ - data_)) &#123;      Slice filter = Slice(data_ + start, limit - start);      return policy_-&gt;KeyMayMatch(key, filter);    &#125; else if (start == limit) &#123;      // Empty filters do not match any keys      return false;    &#125;  &#125;  return true;  // Errors are treated as potential matches&#125;\n\n\n\n\n本章完结\n","tags":["存储"]},{"title":"LevelDB源码阅读笔记（0、下载编译leveldb）","url":"/2024/02/15/leveldb/Start/","content":"LeveDB源码笔记系列：\nLevelDB源码阅读笔记（0、下载编译leveldb）\n本博客环境如下[root@localhost build]# cat /etc/redhat-releaseCentOS Linux release 7.9.2009 (Core)\n\n简介\nLevelDB是由Google使用C++开发的磁盘kv存储引擎。基于LSMTree使用顺序写（追加写）的方式实现了极高的写性能。RocksDB，Ceph等都可以看到它的身影。\n环境搭建以及下载安装命令如下：\n# 下载git clone https://github.com/google/leveldb.gitcd leveldbmkdir buildcd buildcmake ..\n\n不出所料，会报错如下：\n...CMake Error at CMakeLists.txt:303 (add_subdirectory):  The source directory    /root/workspace/leveldb/third_party/googletest  does not contain a CMakeLists.txt file.CMake Error at CMakeLists.txt:307 (set_property):  set_property could not find TARGET gtest.  Perhaps it has not yet been  created.CMake Error at CMakeLists.txt:309 (set_property):  set_property could not find TARGET gmock.  Perhaps it has not yet been  created.CMake Error at CMakeLists.txt:411 (add_subdirectory):  The source directory    /root/workspace/leveldb/third_party/benchmark  does not contain a CMakeLists.txt file.-- Looking for sqlite3_open in sqlite3-- Looking for sqlite3_open in sqlite3 - not found...\n\n此时需要进入项目的third_party目录下载三方依赖：\ncd leveldb/third_partygit clone https://github.com/google/googletest.gitgit clone https://github.com/google/benchmark.git\n\n由于centos7.9下载的g++默认版本（默认是c++98）是：\n[root@localhost build]# g++ --versiong++ (GCC) 4.8.5 20150623 (Red Hat 4.8.5-44)Copyright (C) 2015 Free Software Foundation, Inc.This is free software; see the source for copying conditions.  There is NOwarranty; not even for MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.\n\n直接make也会看到报错，这是因为leveldb要求至少是c++14，故需要升级g++：\nyum install -y http://mirror.centos.org/centos/7/extras/x86_64/Packages/centos-release-scl-2-3.el7.centos.noarch.rpmyum install devtoolset-[g++版本号]-gcc-c++# 切换g++版本，仅对当前会话有效# 也可以使用：source /opt/rh/devtoolset-[版本号]/enablescl enable devtoolset-[g++版本号] bash\n\n开始编译：\ncd leveldb/buildcmake ..# 编译make -j4# 安装（可选make install## 在build目录下会生成一个静态库libleveldb.a\n\nleveldb使用的小deamo，该测试文件位于leveldb项目的根目录：\n// 文件名为：test.cc#include &lt;cassert&gt;#include &lt;iostream&gt;#include &lt;string&gt;#include &quot;include/leveldb/db.h&quot; int main() &#123;    leveldb::DB* db;    leveldb::Options options;    options.create_if_missing = true;    leveldb::Status status = leveldb::DB::Open(options, &quot;./data&quot;, &amp;db);    assert(status.ok());    std::string key = &quot;test_key&quot;;    std::string write_value = &quot;test_value&quot;;    std::string read_value;    leveldb::Status s = db-&gt;Put(leveldb::WriteOptions(), key, write_value);    if (s.ok())&#123;        s = db-&gt;Get(leveldb::ReadOptions(), key, &amp;read_value);    &#125;    if (s.ok())&#123;        std::cout &lt;&lt; &quot;key=&quot; &lt;&lt; key &lt;&lt; &quot;\\nvalue=&quot; &lt;&lt; read_value  &lt;&lt; std::endl;    &#125;else&#123;        std::cout &lt;&lt; &quot;failed to find the key!&quot; &lt;&lt; std::endl;    &#125;    delete db;    return 0;&#125;\n\n编译&amp;运行：\n// 使用如下命令也可以编译：// g++ -I ./include -Wall -std=c++11 -o test.bin test.cc -L ./build/ -lpthread -lleveldb[root@localhost leveldb]# g++ -I ./include -Wall -std=c++11 -o test.bin test.cc ./build/libleveldb.a -lpthread[root@localhost leveldb]# ./test.bin key=test_keyvalue=test_value\n\n\n本章完结\n","tags":["存储"]},{"title":"Linux驱动开发——设备树随记","url":"/2024/11/17/linux_driver/device_tree/","content":"前言在嵌入式Linux这块，对设备树一直都没怎么去了解，一直是模模糊糊的。所以最近也是被老大赶鸭子上架，快速跟着正点原子的驱动开发的课程学了一下。感觉对设备树的认识也是更清晰了一点。同样借着此篇博客记录了一下我的理解。起一个备忘的作用也希望能帮到其他人。\n正文其实类比理解的话DTS相当于.c源文件，文件描述板级设备信息。一个平台或机器对应一个.dts源文件。\n\nDTI相当于c语言的头文件。\n\nDTC相当于于gcc，可以将dts文件编译生成dtb文件\n\nDTB相当于二进制.o文件，由DTC将DTS编译生成。\n\n\n严格来讲，DTI是描述芯片以及芯片周围的一些外设（片上外设）的，比如：CPU的一些参数、总线、总线上的中断控制器、时钟、GPIO的参数、UART控制器、I2C控制器、SPI控制器等等。这些东西都是和芯片强绑定的。只要是你用IMX6ULL这颗芯片，那么它的片上外设就是这些。不存在不同。\n而DTS则会描述具体的片外外设的一些参数信息。比如这个外设接在哪个GPIO口？这个外设要设置什么样的GPIO属性？等等。片外外设是围绕着IMX6ULL这颗芯片来设计不同的板载。比如利用IMX6ULL芯片设计出一个路由器、摄像头、交换机等。因为共用一个芯片。所以它们一定会使用同一个DTI。\n一个节点名（node name）命名形如name@unit_addr，从命名上可以分成两个部分：@前面代表name（可重复）、@后面代表该节点外设在内存当中对应的首地址。特别的，如下所示name之前有个冒号和简称。冒号前面的称为标签（也可以理解为别名，不可重复），可以代替节点名来访问改节点。当节点代表一个设备时，比如一个I2C设备，@后面的数字代表设备的从机地址。\n/&#123;\tintc: interrupt-controller@00a01000 &#123;        /* ... */\t&#125;;&#125;\n\n使用&amp;符号可以向标签所代表的节点当中添加一些所需要设置的属性。比如\n在开发板启动后，可以在文件系统当中，看到设备树的一些信息。在目录&#x2F;proc&#x2F;device-tree下，使用文件树的方式构建了设备树（节点作为目录、属性作为文件）。\n两个特殊的节点：aliases和chosen对于aliases节点其实翻译过来就是别名，以imx6ull.dtsi文件为例：\n/ &#123;\taliases &#123;\t\tgpio0 = &amp;gpio1;\t\tgpio1 = &amp;gpio2;\t\ti2c0 = &amp;i2c1;\t\ti2c1 = &amp;i2c2;\t\tserial0 = &amp;uart1;\t\tserial1 = &amp;uart2;\t\tserial2 = &amp;uart3;\t\tserial7 = &amp;uart8;        /* ... */\t&#125;;    soc &#123;        aips1 &#123;            gpio1: gpio@0209c000 &#123;                /* ... */\t\t\t&#125;;            gpio2: gpio@020a0000 &#123;                /* ... */\t\t\t&#125;;            /* ... */        &#125;    &#125;&#125;\n\n可以看到，其实就是为各个标签起了一个别名。但是这就有一个疑问：标签和别名之间的区别是什么？\n根据其他人提供的线索去查阅文档：https://elinux.org/Device_Tree_Mysteries#Label_vs_aliases_node_property\n其中关键的一段话是：The aliases are not used directly in the device tree source, but are instead dereferenced by the Linux kernel. When a path is provided to of_find_node_by_path() or of_find_node_opts_by_path(), if the path does not begin with a “&#x2F;“ then the first element of the path must be a property name in the “&#x2F;aliases” node. That element is replaced with the full path from the alias.\n简单来讲，DTS、DTI文件无法使用别名，只能使用标签。标签最终会被解释为节点的绝对路径。而别名是被内核所使用的，当内核调用of_find_node_by_path或of_find_node_opts_by_path函数时，如果提供的的节点的路径不是绝对路径的话，就会把它视作在aliases节点下定义的别名，通过别名来获得节点的绝对路径。\n对于chosen节点，查阅正点原子的IMX6ULL驱动开发指南得43.6.2小结得知，Uboot在启动内核前会向chosen添加一个bootargs属性，其内容为Uboot环境变量当中的bootargs的值。同时，bootargs也会作为内核启动的cmdline参数。\n标准属性compatible属性compatible属性会维护一个形如：manufacture,model的驱动兼容列表，驱动程序会根据该列表判断是否与设备兼容。\n例如现在有一个设备节点compatible属性值如下：\ncompatible = &quot;fsl,imx6ul-evk-wm8960&quot;,&quot;fsl,imx-audio-wm8960&quot;;\n\n根据正点原子手册描述得知：上述compatible属性值有两个，分别为“fsl,imx6ul-evk-wm8960”和“fsl,imx-audio-wm8960”，其中“fsl”表示厂商是飞思卡尔，“imx6ul-evk-wm8960”和“imx-audio-wm8960”表示驱动模块名字。sound这个设备首先使用第一个兼容值在 Linux 内核里面查找，看看能不能找到与之匹配的驱动文件，如果没有找到的话就使用第二个兼容值查。\n一般驱动程序文件都会有一个 OF 匹配表，此 OF 匹配表保存着一些 compatible 值，如果设备节点的 compatible 属性值和 OF 匹配表中的何一个值相等，那么就表示设备可以使用这个驱动。\nmodel属性代表设备名\nstatus属性表示设备可操作的状态：\n|\t值\t\t|\t含义\t\t\t\t||\t:-:\t\t|\t:-:\t\t\t\t\t||\tokay\t|\t表示设备是可操作的\t||\tdisable\t|\t设备当前不可操作，但未来可能可操作，比如那些热插拔的设备\t\t||\tfail&#x2F;fail-xxx\t|\t设备出错了\t|\nreg、#address-cells和#size-cells 属性#address-cells 和#size-cells 这两个属性可以用在任何拥有子节点的设备中。一般和reg属性配合使用使用。reg属性一般格式如下：\nreg = &lt;address1 length1&gt;,\t// cell1: addr + len\t&lt;address2 length2&gt;,\t\t// cell2: addr + len\t&lt;address3 length3&gt;,\t\t// cell3: addr + len\t...;\n\n当父节点定义了#address-cells和#size-cells 属性，子节点在定义reg属性时一个cell就受到父节点定义的#address-cells和#size-cells 属性的约束，比如当父节点定义#address-cells为2、#size-cells为1时，就说明子节点reg属性当中一个cell由：两个地址 + 一个长度组成，当然典型的#address-cells和#size-cells 属性的值分别为1、1，这样reg值就和上面代码块所展示的一样。\n这里还是一三个示例来说明一下：\n对于#address-cells为1，#size-cells为0的情况：\nspi4 &#123;\tcompatible = &quot;spi-gpio&quot;;\t#address-cells = &lt;1&gt;;\t#size-cells = &lt;0&gt;;\tgpio_spi: gpio_spi@0 &#123;\t\tcompatible = &quot;fairchild,74hc595&quot;;\t\treg = &lt;0&gt;;\t&#125;;&#125;;\n\n表示gpio_spi当中的reg属性cell只有address值。\nreg = &lt;address1&gt;,\t// cell1: addr\t&lt;address2&gt;,\t\t// cell2: addr\t&lt;address3&gt;,\t\t// cell3: addr\t...;\n\n对于#address-cells为1，#size-cells为1的情况：\nspba-bus@02000000 &#123;\tcompatible = &quot;fsl,spba-bus&quot;, &quot;simple-bus&quot;;\t#address-cells = &lt;1&gt;;\t#size-cells = &lt;1&gt;;\treg = &lt;0x02000000 0x40000&gt;;\tranges;\tecspi1: ecspi@02008000 &#123;\t\t#address-cells = &lt;1&gt;;\t\t#size-cells = &lt;0&gt;;\t\tcompatible = &quot;fsl,imx6ul-ecspi&quot;, &quot;fsl,imx51-ecspi&quot;;\t\treg = &lt;0x02008000 0x4000&gt;;\t\tstatus = &quot;disabled&quot;;\t&#125;;&#125;\n\n表示ecspi1当中的reg属性一个cell组成是：address length。\nreg = &lt;address1 length1&gt;,\t// cell1: addr + len\t&lt;address2 length2&gt;,\t\t// cell2: addr + len\t&lt;address3 length3&gt;,\t\t// cell3: addr + len\t...;\n\n对于#address-cells为2，#size-cells为1的情况：\nexternal-bus &#123;         #address-cells = &lt;2&gt;         #size-cells = &lt;1&gt;;        ...         ethernet@0,0 &#123;             compatible = &quot;smc,smc91c111&quot;;             reg = &lt;0 0 0x1000&gt;;         &#125;;         i2c@1,0 &#123;             compatible = &quot;acme,a1234-i2c-bus&quot;;             #address-cells = &lt;1&gt;;             #size-cells = &lt;0&gt;;             reg = &lt;1 0 0x1000&gt;;         &#125;;         flash@2,0 &#123;             compatible = &quot;samsung,k8f1315ebm&quot;, &quot;cfi-flash&quot;;             reg = &lt;2 0 0x4000000&gt;;         &#125;;\t&#125;;    \n\n表示i2c当中的reg属性一个cell组成是：address_MSB（64位地址最高有效位） address_LSB（64位地址最低有效位） length。（注：一个cell当中一个单元是32位的）\nreg = &lt;address1_MSB address1_LSM length1&gt;,\t&lt;address2_MSB address2_LSB length2&gt;,\t&lt;address3_MSB address3_LSB length3&gt;,\t...;\n\n更一般的：只要子节点需要描述多个设备区域，就可以在 reg 属性中连续定义多个地址和大小。\nparent &#123;    #address-cells = &lt;2&gt;;    #size-cells = &lt;1&gt;;    child@0 &#123;        reg = &lt;0x00000000 0x10000000 0x1000&gt;,  // 第一段地址 + 大小              &lt;0x00000000 0x20000000 0x2000&gt;,  // 第二段地址 + 大小              &lt;0x00000001 0x30000000 0x3000&gt;;  // 第三段地址 + 大小    &#125;;&#125;;\n\n解释：\n\n每个设备区域由 #address-cells 和 #size-cells 定义的单元描述。\n\n比如 0x00000000 0x10000000 是地址，0x1000 是大小。\n\n\nreg 属性用逗号分隔，表示多个设备区域。\n\n每个设备区域对应硬件设备的不同地址空间。\n\n\n特别注意的是#address-cells、#size-cells定义的都是子节点的reg规则，而不是本节点！！！\n其他属性对于range属性，IMX6ULL设备树当中是没有使用的（有，但都为空），它的值一般格式为：\n&lt;child-bus-address,parent-bus-address,length&gt;\n\n当父节点定义此属性时，代表将子节点从child-bus-address地址开始，映射到父节点起始地址parent-bus-address处，并映射length这么长的一个范围。\n对于name属性：name 属性值为字符串，name 属性用于记录节点名字，name 属性已经被弃用，不推荐使用name 属性，一些老的设备树文件可能会使用此属性\ndevice_type属性：属性值为字符串，IEEE 1275 会用到此属性，用于描述设备的 FCode，但是设备树没有 FCode，所以此属性也被抛弃了。此属性只能用于 cpu 节点或者 memory 节点。imx6ull.dtsi 的 cpu0 节点用到了此属性，内容如下所示：\ncpu0: cpu@0 &#123;\tcompatible = &quot;arm,cortex-a7&quot;;\tdevice_type = &quot;cpu&quot;;&#125;\n\n对于根节点的compatible属性：。Linux 内核会通过根节点的 compoatible 属性查看是否支持此设备，如果支持的话才会启动 Linux 内核。\nOF函数 —— 驱动和设备树交互的桥梁OF函数定义的头文件：include&#x2F;linux&#x2F;of.h\n查找节点的 OF 函数\nstruct device_node *of_find_node_by_name(struct device_node *from,    const char *name)  描述：通过节点名字查找指定的节点。\n\nfrom：开始查找的节点，如果为 NULL 表示从根节点开始查找整个设备树。\nname：要查找的节点名字。\n返回值：找到的节点，如果为 NULL 表示查找失败。\n\n\nstruct device_node *of_find_node_by_type(struct device_node *from,    const char *type)  描述：通过 device_type 属性查找指定的节点。（因为device_type用到很少，所以该函数用的也很少）\n\nfrom：开始查找的节点，如果为 NULL 表示从根节点开始查找整个设备树。\ntype：要查找的节点对应的 type 字符串，也就是 device_type 属性值。\n返回值：找到的节点，如果为 NULL 表示查找失败。\n\n\nstruct device_node *of_find_compatible_node(struct device_node *from,   const char *type,    const char *compatible)  描述：根据 device_type 和 compatible 这两个属性查找指定的节点。\n\nfrom：开始查找的节点，如果为 NULL 表示从根节点开始查找整个设备树。\ntype：要查找的节点对应的 type 字符串，也就是 device_type 属性值，可以为 NULL，表示忽略掉 device_type 属性。\ncompatible：要查找的节点所对应的 compatible 属性列表。\n返回值：找到的节点，如果为 NULL 表示查找失败\n\n\nstruct device_node *of_find_matching_node_and_match(struct device_node *from,   const struct of_device_id *matches,   const struct of_device_id **match)  描述：通过 of_device_id 匹配表来查找指定的节点。\n\nfrom：开始查找的节点，如果为 NULL 表示从根节点开始查找整个设备树。\nmatches：of_device_id 匹配表，也就是在此匹配表里面查找节点。\nmatch：找到的匹配的 of_device_id。\n返回值：找到的节点，如果为 NULL 表示查找失败\n\n\ninline struct device_node *of_find_node_by_path(const char *path)\n  描述：通过路径来查找指定的节点。\n\npath：带有全路径的节点名，可以使用节点的别名，比如“&#x2F;backlight”就是 backlight 这个节点的全路径。\n返回值：找到的节点，如果为 NULL 表示查找失败。\n\n\nstruct device_node *of_get_parent(const struct device_node *node)\n  描述：用于获取指定节点的父节点(如果有父节点的话)。\n\nnode：要查找的父节点的节点。\n返回值：找到的父节点。\n\n\nstruct device_node *of_get_next_child(const struct device_node *node,   struct device_node *prev)  描述：数用迭代的方式查找子节点。\n\nnode：父节点。\nprev：前一个子节点，也就是从哪一个子节点开始迭代的查找下一个子节点。可以设置为NULL，表示从第一个子节点开始。\n返回值：找到的下一个子节点。\n\n\n\n获取属性值的 OF 函数设备树的属性在内核当中以一个结构体的形式存在，它的定义如下：\nstruct property &#123;\tchar *name; /* 属性名字 */\tint length; /* 属性长度 */\tvoid *value; /* 属性值 */\tstruct property *next; /* 下一个属性 */\tunsigned long _flags;\tunsigned int unique_id;\tstruct bin_attribute attr;&#125;;\n\n\nproperty *of_find_property(const struct device_node *np,   const char *name,   int *lenp)  描述：查找指定节点的属性名为name的属性值。\n\nnp：设备节点。\nname： 属性名字。\nlenp：属性值的字节数。\n返回值：找到的属性。\n\n\nint of_property_count_elems_of_size(const struct device_node *np,   const char *propname,   int elem_size)  描述：获取属性中元素的数量，比如 reg 属性值是一个数组，那么使用此函数可以获取到这个数组的大小。\n\nnp：设备节点。\nproname： 需要统计元素数量的属性名字。\nelem_size：元素长度。\n返回值：得到的属性元素数量。\n\n\nint of_property_read_u32_index(const struct device_node *np,   const char *propname,   u32 index,    u32 *out_value)  描述：从属性中获取指定标号的 u32 类型数据值(无符号 32位)，比如某个属性有多个 u32 类型的值，那么就可以使用此函数来获取指定标号的数据值。\n\nnp：设备节点。\nproname： 要读取的属性名字。\nindex：要读取的值标号。\nout_value：读取到的值  返回值：0 读取成功，负值，读取失败，-EINVAL 表示属性不存在，-ENODATA 表示没有要读取的数据，-EOVERFLOW 表示属性值列表太小。\n\n\nint of_property_read_ux_array(const struct device_node *np,   const char *propname,    ux *out_values,    size_t sz)（x &#x3D; 8, 16, 32, 64）  描述：可以读取属性中 u8、u16、u32 和 u64 类型的数组数据，比如大多数的 reg 属性都是数组数据，可以使用这 4 个函数一次读取出 reg 属性中的所有数据。\n\nnp：设备节点。\nproname： 要读取的属性名字。\nout_value：读取到的数组值，分别为 u8、u16、u32 和 u64。\nsz：要读取的数组元素数量。\n返回值：0，读取成功，负值，读取失败，-EINVAL 表示属性不存在，-ENODATA 表示没有要读取的数据，-EOVERFLOW 表示属性值列表太小。\n\n\nint of_property_read_ux(const struct device_node *np,    const char *propname,   ux *out_value)（x &#x3D; 8, 16, 32, 64）  描述：是用于读取这种只有一个整形值的属性，可以读取 u8、u16、u32 和 u64 类型属性值。\n\nnp：设备节点。\nproname： 要读取的属性名字。\nout_value：读取到的数组值。\n返回值：0，读取成功，负值，读取失败，-EINVAL 表示属性不存在，-ENODATA 表示没有要读取的数据，-EOVERFLOW 表示属性值列表太小。\n\n\nint of_property_read_string(struct device_node *np,    const char *propname,   const char **out_string)  描述：读取属性中字符串值。\n\nnp：设备节点。\nproname： 要读取的属性名字。\nout_string：读取到的字符串值。\n返回值：0，读取成功，负值，读取失败。\n\n\nint of_n_addr_cells(struct device_node *np)、int of_n_size_cells(struct device_node *np)\n  描述：分别可以获取设备的#address-cells、#size-cells属性值。\n\nnp：设备节点。\n返回值：#address-cells、#size-cells属性值。\n\n\nint of_device_is_compatible(const struct device_node *device,   const char *compat)  描述：查看节点的 compatible 属性是否有包含 compat 指定的字符串。\n\ndevice：设备节点。\ncompat：要查看的字符串。\n返回值：0，节点的 compatible 属性中不包含 compat 指定的字符串；正数，节点的 compatible\n属性中包含 compat 指定的字符串。\n\n\n\n了解完操作设备树的OF函数之后，其实我们就应该知道，所谓设备树的属性，除了常用的reg之外，在设备树文件当中，存在很多其他的一些厂商自定义的属性，这些属性专门为他们的芯片&#x2F;设备服务的。我们也可以为一个节点自定义一个属性。最开始接触设备树的时候，因为没有任何单片机的基础，对于设备树文件当中出现的GPIO、I2C、SPI、PWM、UART等陌生的词汇没有任何概念，在有了一点单片机的基础后，再来看设备树这些概念，其实就很好理解了，对于某一个节点，为什么要有这些属性都大概能知道其原因。\n\n本章完结\n","tags":["驱动开发"]},{"title":"Linux驱动开发快速入门——字符设备驱动","url":"/2024/11/20/linux_driver/char_device/","content":"前言笔者使用开发板型号：正点原子的IMX6ULL-alpha开发板。ubuntu版本为：20.04。写此文也是以备忘为目的。\n字符设备驱动本小结将以直接操作寄存器的方式控制一个LED灯，可以通过read系统调用可以获取灯的状态（开&#x2F;关）；通过write系统调用可以控制灯的开关。\nLED原理图如下：\n\n从图中可以了解到，我们需要控制LED0所接的GPIO口输出低电平可以点亮led灯，输出高电平可以关闭led灯。查手册得知LED0对应为GPIO1_IO03。\n\n\n驱动程序如下：\n#define LED_MAJOR\t\t200\t\t/* 主设备号 */#define LED_NAME\t\t&quot;led&quot; \t/* 设备名字 */#define LEDOFF \t0\t\t\t\t/* 关灯 */#define LEDON \t1\t\t\t\t/* 开灯 */#define LEDSTA  3               /* 获取开关灯状态 */ /* 寄存器物理地址 */#define CCM_CCGR1_BASE\t\t\t\t(0X020C406C)\t#define SW_MUX_GPIO1_IO03_BASE\t\t(0X020E0068)#define SW_PAD_GPIO1_IO03_BASE\t\t(0X020E02F4)#define GPIO1_DR_BASE\t\t\t\t(0X0209C000)#define GPIO1_GDIR_BASE\t\t\t\t(0X0209C004)/* 映射后的寄存器虚拟地址指针 */static void __iomem *IMX6U_CCM_CCGR1;static void __iomem *SW_MUX_GPIO1_IO03;static void __iomem *SW_PAD_GPIO1_IO03;static void __iomem *GPIO1_DR;static void __iomem *GPIO1_GDIR;int led_switch(u8 sta) &#123;    int rt = -1;\tu32 val = 0;\tif (sta == LEDON) &#123;\t\tval = readl(GPIO1_DR);\t\tval &amp;= ~(1 &lt;&lt; 3);\t\t\twritel(val, GPIO1_DR);        rt = LEDON;\t&#125; else if (sta == LEDOFF) &#123;\t\tval = readl(GPIO1_DR);\t\tval|= (1 &lt;&lt; 3);\t\t\twritel(val, GPIO1_DR);        rt = LEDOFF;\t&#125; else &#123;        val = readl(GPIO1_DR);        rt = val &amp; (1 &lt;&lt; 3) ? LEDOFF : LEDON;    &#125;    return rt;&#125;static ssize_t led_read(struct file *filp, char __user *buf, size_t cnt, loff_t *offt) &#123;\tint retvalue;\tunsigned char databuf[1];\tunsigned char ledstat;    retvalue = led_switch(LEDSTA);    if (retvalue &lt; 0) &#123;        retvalue = -EIO;        goto error;    &#125;    databuf[0] = retvalue;\tretvalue = copy_to_user(buf, databuf, cnt);error:\treturn retvalue;&#125;static ssize_t led_write(struct file *filp, const char __user *buf, size_t cnt, loff_t *offt) &#123;\tint retvalue;\tunsigned char databuf[1];\tunsigned char ledstat;\tretvalue = copy_from_user(databuf, buf, cnt);\tif(retvalue &lt; 0) &#123;\t\tprintk(&quot;kernel write failed!\\r\\n&quot;);\t\treturn -EFAULT;\t&#125;\tledstat = databuf[0];\t\t/* 获取状态值 */\tif(ledstat == LEDON) &#123;\t\t\tled_switch(LEDON);\t\t/* 打开LED灯 */\t&#125; else if(ledstat == LEDOFF) &#123;\t\tled_switch(LEDOFF);\t\t/* 关闭LED灯 */\t&#125;\treturn 0;&#125;/* 设备操作结构体 */static struct file_operations led_fops = &#123;\t.owner = THIS_MODULE,\t.read = led_read,\t.write = led_write,&#125;;static int __init led_init(void) &#123;\tint retvalue = 0;\tu32 val = 0;\t/* 初始化LED */\t/* 1、寄存器地址映射 */  \tIMX6U_CCM_CCGR1 = ioremap(CCM_CCGR1_BASE, 4);\tSW_MUX_GPIO1_IO03 = ioremap(SW_MUX_GPIO1_IO03_BASE, 4);  \tSW_PAD_GPIO1_IO03 = ioremap(SW_PAD_GPIO1_IO03_BASE, 4);\tGPIO1_DR = ioremap(GPIO1_DR_BASE, 4);\tGPIO1_GDIR = ioremap(GPIO1_GDIR_BASE, 4);\t/* 2、使能GPIO1时钟 */\tval = readl(IMX6U_CCM_CCGR1);\tval &amp;= ~(3 &lt;&lt; 26);\t/* 清楚以前的设置 */\tval |= (3 &lt;&lt; 26);\t/* 设置新值 */\twritel(val, IMX6U_CCM_CCGR1);\t/* 3、设置GPIO1_IO03的复用功能，将其复用为\t *    GPIO1_IO03，最后设置IO属性。\t */\twritel(5, SW_MUX_GPIO1_IO03);\t\t/*寄存器SW_PAD_GPIO1_IO03设置IO属性\t *bit 16:0 HYS关闭\t *bit [15:14]: 00 默认下拉     *bit [13]: 0 kepper功能     *bit [12]: 1 pull/keeper使能     *bit [11]: 0 关闭开漏输出 --- 推挽输出     *bit [7:6]: 10 速度100Mhz     *bit [5:3]: 110 R0/6驱动能力     *bit [0]: 0 低转换率\t */\twritel(0x10B0, SW_PAD_GPIO1_IO03);\t/* 4、设置GPIO1_IO03为输出功能 */\tval = readl(GPIO1_GDIR);\tval &amp;= ~(1 &lt;&lt; 3);\t/* 清除以前的设置 */\tval |= (1 &lt;&lt; 3);\t/* 设置为输出 */\twritel(val, GPIO1_GDIR);\t/* 5、默认关闭LED */\tval = readl(GPIO1_DR);\tval |= (1 &lt;&lt; 3);\t\twritel(val, GPIO1_DR);\t/* 6、注册字符设备驱动 */\tretvalue = register_chrdev(LED_MAJOR, LED_NAME, &amp;led_fops);\tif(retvalue &lt; 0)&#123;\t\tprintk(&quot;register chrdev failed!\\r\\n&quot;);\t\treturn -EIO;\t&#125;\treturn 0;&#125;static void __exit led_exit(void) &#123;\t/* 取消映射 */\tiounmap(IMX6U_CCM_CCGR1);\tiounmap(SW_MUX_GPIO1_IO03);\tiounmap(SW_PAD_GPIO1_IO03);\tiounmap(GPIO1_DR);\tiounmap(GPIO1_GDIR);\t/* 注销字符设备驱动 */\tunregister_chrdev(LED_MAJOR, LED_NAME);&#125;module_init(led_init);module_exit(led_exit);MODULE_LICENSE(&quot;GPL&quot;);MODULE_AUTHOR(&quot;lulang&quot;);\n\n应用程序如下：\n#define LEDOFF \t0#define LEDON \t1#define LEDSTA  3int main(int argc, char *argv[]) &#123;\tint fd, retvalue;\tchar *filename;\tunsigned char databuf[1], state_buf = LEDSTA;\t\tif(argc != 3) &#123;\t\tprintf(&quot;Error Usage!\\r\\n&quot;);\t\treturn -1;\t&#125;\tfilename = argv[1];\t/* 打开led驱动 */\tfd = open(filename, O_RDWR);\tif(fd &lt; 0) &#123;\t\tprintf(&quot;file %s open failed!\\r\\n&quot;, argv[1]);\t\treturn -1;\t&#125;\tdatabuf[0] = atoi(argv[2]);\t/* 要执行的操作：打开或关闭 */    retvalue = read(fd, &amp;state_buf, sizeof(state_buf));\tif(retvalue &lt; 0) &#123;\t\tprintf(&quot;LED read Failed!\\r\\n&quot;);\t\tclose(fd);\t\treturn -1;\t&#125;    printf(&quot;LED before is %s\\n&quot;, state_buf == LEDON ? &quot;ON&quot; : &quot;OFF&quot;);\t/* 向/dev/led文件写入数据 */\tretvalue = write(fd, databuf, sizeof(databuf));\tif(retvalue &lt; 0)&#123;\t\tprintf(&quot;LED Control Failed!\\r\\n&quot;);\t\tclose(fd);\t\treturn -1;\t&#125;\tretvalue = close(fd);\treturn 0;&#125;\n\n为操作方便，这里我写了一个同时能编译驱动和应用程序的Makefile脚本：\nKERNELDIR := /home/lunar/workspace/imx6ull/resource/linux_quickstart/CURRENT_PATH := $(shell pwd)LUNAR_GCC = arm-linux-gnueabihf-gccLUNAR_CFLAGS = -Wall -OOUTPUT_DIR = $&#123;CURRENT_PATH&#125;/buildobj-m := chardev/led.oLED_CHARDEV = build/chardev/led_app.c.obuild/%.c.o: %.c\tmkdir -p $&#123;@D&#125;\t$&#123;LUNAR_GCC&#125; $&#123;LUNAR_CFLAGS&#125; -o $@ -c $&lt;bin/chardev_app.bin: $&#123;LED_CHARDEV&#125;\tmkdir -p $&#123;@D&#125;\t$&#123;LUNAR_GCC&#125; $&#123;LUNAR_CFLAGS&#125; -o $@ $^all: kernel_modules \\\tbin/chardev_app.binkernel_modules:\t$(MAKE) -C $(KERNELDIR) M=$(CURRENT_PATH) modulesclean:\t$(MAKE) -C $(KERNELDIR) M=$(CURRENT_PATH) clean\trm -rf ./bin/*\trm -rf ./build/*\n\nbash脚本作为简化编译流程的工具：\n# ! /bin/bashnfs_dest=&quot;/home/lunar/workspace/nfs_root/rootfs/lib/modules/4.1.15&quot;if [ &quot;$1&quot; == &quot;clear&quot; ]; then    echo &quot;clear ...&quot;    make clean    echo &quot;rm app in $&#123;nfs_dest&#125;&quot;    rm -rf $&#123;nfs_dest&#125;/bin/*    echo &quot;rm driver in $&#123;nfs_dest&#125;&quot;    rm -f  $&#123;nfs_dest&#125;/*.ko    exit 0else    make allfiecho &quot;cp app to $&#123;nfs_dest&#125;&quot;cp -rf ./bin $&#123;nfs_dest&#125;echo &quot;cp driver to $&#123;nfs_dest&#125;&quot;cp -f ./chardev/led.ko $&#123;nfs_dest&#125;echo &quot;done&quot;\n\n\n\n测试：\n\n使用命令：sudo env PATH=$PATH bash ./build.sh，编译驱动程序和app。（前面一定要接sudo env PATH=$PATH bash，否则会报没有权限的错误！）\n\n在下位机创建modules目录：&#x2F;lib&#x2F;modules&#x2F;4.1.15&#x2F;bin。（第一次需要）\n\n下位机执行depmod命令。（每次有新驱动都需要）\n\n加载驱动：modprobe led.ko。\n\n下位机创建设备节点：mknod /dev/led c 200 0。\n\n下位机控制led灯：./ledApp /dev/led 1。\n \n 同时可以看到灯确实会受到app的控制。\n\n\n这里备忘几条命令：\n# 可以查看系统当前设备号的分配情况cat /proc/devicesCharacter devices:  1 mem  4 /dev/vc/0  4 tty  5 /dev/tty  5 /dev/console  5 /dev/ptmx...200 led...# 查看系统当前加载的驱动lsmodModule                  Size  Used by    Tainted: Gled                     1854  0# 卸载驱动rmmod led.ko\n\n\n在IMX6ULL当中，直接操作GPIO寄存器点亮一个LED主要分6步：\n\n将寄存器物理地址映射为内核态的虚拟地址。\n使能GPIO（所在总线）的时钟。\n配置GPIO1_IO03复用为GPIO。\n配置GPIO1_IO03本身的属性。\n配置GPIO1_IO03的方向（输入&#x2F;输出）。\n通过写入GPIO1_IO03的数据寄存器达到控制输出的目的。\n\nlinux下驱动开发再直接操作寄存器笔者表示有点HOLD不住了！因为这太原始了。这里使用直接操作寄存器编写驱动的目的是考虑到：到目前为止你对linux驱动是完全一无所知的。后面的小结会慢慢将这段原始的代码变得更优雅。\n这里的驱动代码缺点有三个：\n\n直接操作寄存器。\n一个主设备号将其下的所有次设备号都占用了。\n设备节点需要我们手动创建。\n\n下面两小节会一一解决这两个问题。\npinctrl子系统对于pinctrl子系统主要工作内容如下：\n\n获取设备树中 pin 信息。\n根据获取到的 pin 信息来设置 pin 的复用功能。\n根据获取到的 pin 信息来设置 pin 的电气特性，比如上&#x2F;下拉、速度、驱动能力等。\n\n对于我们使用者来讲，只需要在设备树里面设置好某个 pin 的相关属性即可，其他的初始化工作均由 pinctrl 子系统来完成，pinctrl 子系统源码目录为 drivers&#x2F;pinctrl。当然pinctrl最终还是会操作寄存器，而pingctrl最底层的驱动，也即操作寄存器的这部分代码芯片厂家一般会提供好。这里体现了linux分层分离的思想。极大减轻了驱动开发人员的负担，开发驱动的专心开发驱动。\n在imx6ull.dtsi文件下搜索可以找到iomuxc节点，该节点下面有各种pinctrl的配置，从iomuxc节点的compatible属性可以找到pinctrl的最底层驱动，iomuxc节点部分属性如下：\n/ &#123; /* 整个芯片 */\tsoc &#123;\t/* 片上系统 */\t\taips1: aips-bus@02000000 &#123;\t/* 片上系统的总线 */\t\t\tiomuxc: iomuxc@020e0000 &#123;\t/* 片上系统的总线的io复用控制寄存器 */\t\t\t\tcompatible = &quot;fsl,imx6ul-iomuxc&quot;;\t\t\t\treg = &lt;0x020e0000 0x4000&gt;;\t\t\t&#125;;\t\t&#125;\t&#125;&#125;\n\n可以看到iomuxc的compatible属性值为：”fsl,imx6ul-iomuxc”，暗示着imx6ull芯片厂家所提供的pinctrl的底层驱动当中必定有该字符串——“fsl,imx6ul-iomuxc”。通过搜索该字符串可以定位到pinctrl最底层的驱动。感兴趣的读者可以去研读一下。\ngpio子系统如果 pinctrl 子系统将一个 PIN 复用为 GPIO 的话，那么接下来就要用到 gpio 子系统了。\n对于gpio子系统主要工作内容如下：\n\n设置gpio输入输出状态。设置gpio中断模式（上升沿、下降沿、双边沿触发）。\n控制gpio输入输出。\n读取gpio状态。\n\n同样的，和pinctrl类似，gpio 子系统的主要目的就是方便驱动开发者使用 gpio，驱动开发者在设备树中添加 gpio 相关信息，然后就可以在驱动程序中使用 gpio 子系统提供的 API函数来操作 GPIO。不同点在于用户是需要操作gpio的，所以gpio子系统提供了供用户使用的统一接口，而pinctrl是针对一个gpio做配置，一旦配置完毕，无需再修改，所以只要你配置好设备树，整个初始化过程都是自动的，无需用户干什么。两者底层最终肯定是操作寄存器的，而且最底层操作寄存器的驱动代码完全由厂家提供好了。\n/ &#123; /* 整个芯片 */\tsoc &#123; /* 片上系统 */\t\taips1: aips-bus@02000000 &#123; /* 片上系统的总线 */\t\t\tgpio1: gpio@0209c000 &#123; /* 片上系统的总线上的gpio1的寄存器 */\t\t\t\tcompatible = &quot;fsl,imx6ul-gpio&quot;, &quot;fsl,imx35-gpio&quot;;\t\t\t\treg = &lt;0x0209c000 0x4000&gt;;\t\t\t\tinterrupts = &lt;GIC_SPI 66 IRQ_TYPE_LEVEL_HIGH&gt;,\t\t\t\t\t     &lt;GIC_SPI 67 IRQ_TYPE_LEVEL_HIGH&gt;;\t\t\t\tgpio-controller;\t\t\t\t#gpio-cells = &lt;2&gt;;\t\t\t\tinterrupt-controller;\t\t\t\t#interrupt-cells = &lt;2&gt;;\t\t\t&#125;;\t\t&#125;\t&#125;&#125;&amp;tsc &#123;\tpinctrl-names = &quot;default&quot;;\tpinctrl-0 = &lt;&amp;pinctrl_tsc&gt;;\txnur-gpio = &lt;&amp;gpio1 3 GPIO_ACTIVE_LOW&gt;;\t/* 对gpio1的引用 */\tmeasure-delay-time = &lt;0xffff&gt;;\tpre-charge-time = &lt;0xfff&gt;;\tstatus = &quot;okay&quot;;&#125;;\n\n对于上面的配置，重点关注一下gpio1节点下的#gpio-cells属性，#name-cells的命名方式和设备树当中的#address-cells和#size-cells 属性很类似。不同的是：#address-cells和#size-cells作用于定义节点的子节点，描述子节点reg属性的格式。而#gpio-cells属性，就我目前所知，它定义了引用了gpio1后描述gpio的格式。比如上面代码片段当中，gpio1定义#gpio-cells为2，在tsc这个设备节点当中引用了gpio1，那么在&amp;gpio1之后需要两个元素来组成一个cell。\n如果你前面了解过#address-cells，那么肯定会有疑问：为什么#gpio-cells被定义成2，在tsc引用的时候不是填了3个元素吗？\n实际上#gpio-cells描述的数值是从第二个元素开始算起的。#gpio-cells &#x3D; &lt;2&gt; 的意思是，这个 GPIO 控制器（gpio1）需要两个单元来描述每个 GPIO 引脚。通常这两个单元分别代表：\n第一个单元：GPIO 引脚的编号（通常是一个整数值，表示 GPIO 引脚的具体编号）。\n第二个单元：GPIO 的属性，通常是一个标志，表示 GPIO 的电平状态、触发类型等，像是 GPIO_ACTIVE_LOW 或 GPIO_ACTIVE_HIGH，以及是否启用中断等。\n在 cd-gpios &#x3D; &lt;&amp;gpio1 19 GPIO_ACTIVE_LOW&gt; 中：\n\n&amp;gpio1 是一个指向 gpio1 节点的引用。\n19 是 GPIO 引脚的编号。\nGPIO_ACTIVE_LOW 表示该引脚的电平为低电平有效。\n\n这三个元素看起来像是三个单元，但实际上是将两个单元（引脚编号和电平状态）传递给了 gpio1 节点中的 GPIO 控制器。\n字符设备驱动最终版本首先添加pinctrl节点。先熟悉一下IMX6ULL本身自带的dts是如何配置pnctrl的，挑了一个最简单的pinctrl节点，它的配置参考如下这段代码：\n&amp;iomuxc &#123;\tpinctrl-names = &quot;default&quot;;\tpinctrl-0 = &lt;&amp;pinctrl_hog_1&gt;;\timx6ul-evk &#123;\t\t/* ... */\t\tpinctrl_wdog: wdoggrp &#123;\t\t\tfsl,pins = &lt;\t\t\t\tMX6UL_PAD_LCD_RESET__WDOG1_WDOG_ANY    0x30b0\t\t\t&gt;;\t\t&#125;;\t&#125;&#125;\n\n重点观察一下wdoggrp节点的配置，从整体上看，可以了解到：\n\n&amp;iomuxc代表这一大段内容会被追加到iomuxc节点。iomuxc节点定义在imx6ull.dti头文件。头文件当中定义的iomuxc节点的内容很少，可以参考上面pinctrl小结所贴出来的代码。\n\nwdoggrp节点标签为：pinctrl_wdog，以pinctrl作为前缀，节点名为：wdoggrp。\n\nwdoggrp属性只有一个：fsl,pins，它的值比较关键，当设备树当中有某一处以形如：\n &amp;wdog1 &#123;\tpinctrl-names = &quot;default&quot;;\tpinctrl-0 = &lt;&amp;pinctrl_wdog&gt;;\tfsl,wdog_b;&#125;;\n 方式引用了wdoggrp节点，pinctrl子系统就会根据wdoggrp节点的fsl,pins属性值来初始化复用&#x2F;电气属性控制寄存器。从上面pinctrl_wdog的配置可以了解到它的值为：MX6UL_PAD_LCD_RESET__WDOG1_WDOG_ANY  0x30b0，那么这代表什么含义呢？这就需要找到MX6UL_PAD_LCD_RESET__WDOG1_WDOG_ANY的定义在哪儿。一路从头文件往上找：imx6ull.dtsi -&gt; imx6ull-pinfunc.h -&gt; imx6ul-pinfunc.h\n 从imx6ul-pinfunc.h头文件的开头我们可以了解到宏定义有5个单元，每个单元的含义如下：\n /** The pin function ID is a tuple of* &lt;mux_reg conf_reg input_reg mux_mode input_val&gt;*/\n |\tmux_reg\t|\tconf_reg\t|\tinput_reg\t|\tmux_mode\t|\tinput_val\t| |\t:-:\t\t|\t:-:\t\t\t|\t:-:\t\t\t|\t:-:\t\t\t|\t:-:\t\t\t| |\t复用寄存器（偏移）地址（引脚复用为什么功能？）|\t引脚电器属性配置寄存器（偏移）地址（上拉&#x2F;下拉，阻抗多大？推挽开漏输出？）\t|\t暂时还没能去了解:-(\t\t|\tmux_reg的值\t|\tinput_reg寄存器的值\t|\n 比如MX6UL_PAD_LCD_RESET__WDOG1_WDOG_ANY宏定义展开后如下：\n #define MX6UL_PAD_LCD_RESET__WDOG1_WDOG_ANY                       0x0114 0x03A0 0x0000 0x4 0x0\n\nmux_reg &#x3D; 0x0114：通过查阅芯片手册得知该值为IOMUXC_SW_MUX_CTL_PAD_LCD_RESET寄存器地址，该寄存器可选功能如下：\n\n \n 选中部分对应mux_mode &#x3D; 0x4。\n\nconf_reg &#x3D; 0x03A0：对应IOMUXC_SW_PAD_CTL_PAD_LCD_RESET寄存器地址，该寄存器设置引脚电器属性，参考如下表（截取部分配置）：\n\n \n\ninput_reg &#x3D; 0x0000：可以看到wdoggrp节点将其设置为零，我们可以大胆猜测0x0000是无意义值，类比于nullptr。所以在pnctrl初始化wdoggrp时，并没使用input_reg。并且input_val &#x3D; 0x0。\n\n 细心的读者一定会发现：MX6UL_PAD_LCD_RESET__WDOG1_WDOG_ANY宏定义展开后并没有发现conf_reg（引脚电器属性）寄存器对应的值。它的值其实交由用户去定义，在wdoggrp节点当中，fsl,pins属性值最后一个数值0x30b0定义了conf_reg寄存器的值。\n\n\n了解完了pinctrl_wdog: wdoggrp节点的来龙去脉，我们可以试着手动添加一个led灯的pnctrl，并通过gpio子系统来点亮led灯。具体步骤如下：\n\n添加pinctrl节点，通过文章开头的led原理图得知，我们需要在imx6ul-pinfunc.h找到GPIO1_IO03相关的宏定义，将其复用为GPIO1_IO03，然后修改设备树，在系统移植时创建的.dts文件下的&amp;iomuxc节点后面追加一条pinctrl配置：\n \n &amp;iomuxc &#123;\tpinctrl_led: ledgrp &#123;\t\tfsl,pins = &lt;\t\t\tMX6UL_PAD_GPIO1_IO03__GPIO1_IO03    0x10b0\t\t&gt;;\t&#125;;&#125;;\n 经查表得知将GPIO配置成推挽输出。其他电气属性位读者可自行查阅芯片手册进行验证。\n\n在根节点下添加设备节点，所谓添加设备节点，其实就是配置设备所需的gpio编号并引用一下相应gpio的pinctrl，确保pinctrl子系统将gpio的复用&#x2F;电气属性寄存器正确初始化。配置如下：\n /&#123;\tled_dev &#123;\t\tcompatible = &quot;company,imx6ull-led&quot;;\t\tpinctrl-names = &quot;default&quot;;\t\tpinctrl-0 = &lt;&amp;pinctrl_led&gt;;\t\tled-gpio = &lt;&amp;gpio1 3 GPIO_ACTIVE_LOW&gt;;\t\tstatus = &quot;okay&quot;;\t&#125;;&#125;\n 正常情况下，我们需要一个gpio口控制灯，我们认为灯打开就是active状态。对于一个程序员来说，我们可以封装一个函数，写1就是打开灯，写0就是关灯。但是对于硬件来说，变化的是gpio口的电平状态。如果gpio输出高电平灯亮，那么这就是高有效。如果硬件设计是gpio输出低电平灯亮，那么就是低有效。对于一个软件工程师来说，我们的期望是写1就是亮灯，写0就是关灯。GPIO_ACTIVE_LOW的意思就是物理硬件在低电平是有效的，当应用程序通过gpio_set_value接口写逻辑1时，实际上对应物理的0。（好像并非如此，经过测试，GPIO_ACTIVE_LOW和GPIO_ACTIVE_HIGH测试结果都一样，gpio_set_value写0才是开、写1才是关。需要进一步探究）\n\n检查设备树当中有没有其他设备占用了GPIO1_IO03，如果有，需要将设备屏蔽。\n\n\n接下来就是led点灯驱动程序的最终版，其应用程序同上：\n#define CHARLED_CNT\t\t\t1\t/* 设备号个数 */#define LED_MAJOR\t\t200\t\t/* 主设备号 */#define LED_NAME\t\t&quot;led_device_tree&quot; \t/* 设备名字 */#define LEDOFF \t0\t\t\t\t/* 关灯 */#define LEDON \t1\t\t\t\t/* 开灯 */#define LEDSTA  3               /* 获取开关灯状态 *//* newchrled设备结构体 */struct chrled_dev&#123;\tdev_t devid;\t\t\t/* 设备号 \t */\tstruct cdev cdev;\t\t/* cdev \t*/\tstruct class *class;\t\t/* 类 \t\t*/\tstruct device *device;\t/* 设备 \t */\tint major;\t\t\t\t/* 主设备号\t  */\tint minor;\t\t\t\t/* 次设备号   */\tstruct device_node\t*nd; /* 设备节点 */\tint led_gpio;\t\t\t/* led所使用的GPIO编号\t\t*/&#125;;struct chrled_dev chrled;\t/* led设备 */static ssize_t led_read(struct file *filp, char __user *buf, size_t cnt, loff_t *offt)&#123;\tint retvalue;\tunsigned char databuf[1];    retvalue = (gpio_get_value(chrled.led_gpio) == 0) ? LEDON : LEDOFF;    if (retvalue &lt; 0) &#123;        retvalue = -EIO;        goto error;    &#125;    databuf[0] = retvalue;\tretvalue = copy_to_user(buf, databuf, cnt);error:\treturn retvalue;&#125;static ssize_t led_write(struct file *filp, const char __user *buf, size_t cnt, loff_t *offt)&#123;\tint retvalue;\tunsigned char databuf[1];\tunsigned char ledstat;\tretvalue = copy_from_user(databuf, buf, cnt);\tif(retvalue &lt; 0) &#123;\t\tprintk(&quot;kernel write failed!\\r\\n&quot;);\t\treturn -EFAULT;\t&#125;\tledstat = databuf[0];\t\t\t\t\t\t/* 获取状态值 */\tif(ledstat == LEDON) &#123;\t        gpio_set_value(chrled.led_gpio, 0);\t\t/* 打开LED灯 */\t&#125; else if(ledstat == LEDOFF) &#123;        gpio_set_value(chrled.led_gpio, 1);\t\t/* 关闭LED灯 */\t&#125;\treturn 0;&#125;static struct file_operations led_fops = &#123;\t.owner = THIS_MODULE,\t.read = led_read,\t.write = led_write,&#125;;static int __init led_init(void)&#123;    int ret = 0;\t/* 根据设备树配置，申请GPIO */\t/* 1、获取设备节点：gpioled */\tchrled.nd = of_find_node_by_path(&quot;/led_dev&quot;);\tif(chrled.nd == NULL) &#123;\t\tprintk(&quot;gpioled node not find!\\r\\n&quot;);\t\treturn -EINVAL;\t&#125; else &#123;\t\tprintk(&quot;gpioled node find!\\r\\n&quot;);\t&#125;\t/* 2、 获取设备树中的gpio属性，得到LED所使用的LED编号 */\tchrled.led_gpio = of_get_named_gpio(chrled.nd, &quot;led-gpio&quot;, 0);\tif(chrled.led_gpio &lt; 0) &#123;\t\tprintk(&quot;can&#x27;t get led-gpio&quot;);\t\treturn -EINVAL;\t&#125;\tprintk(&quot;led-gpio num = %d\\r\\n&quot;, chrled.led_gpio);    /* 3、申请gpio，避免多个驱动同时操作一个gpio口的情况发生 */\tret = gpio_request(chrled.led_gpio, &quot;led_gpio&quot;);\tif (ret) &#123;\t\tpr_err(&quot;could not request power off GPIO\\n&quot;);\t\treturn -EINVAL;\t&#125;\t/* 4、设置GPIO1_IO03为输出，并且输出高电平，默认关闭LED灯 */\tret = gpio_direction_output(chrled.led_gpio, 1);\tif(ret &lt; 0) &#123;\t\tprintk(&quot;can&#x27;t set gpio!\\r\\n&quot;);\t&#125;\t/* 注册字符设备驱动 */\t/* 1、创建设备号 */\tif (chrled.major) &#123;\t\t\t\t\t\t\t/*  定义了设备号 */\t\tchrled.devid = MKDEV(chrled.major, 0);\t\tregister_chrdev_region(chrled.devid, CHARLED_CNT, LED_NAME);\t&#125; else &#123;\t\t\t\t\t\t\t\t\t/* 没有定义设备号 */\t\talloc_chrdev_region(&amp;chrled.devid, 0, CHARLED_CNT, LED_NAME);\t/* 申请设备号 */\t\tchrled.major = MAJOR(chrled.devid);\t\t/* 获取分配好的主设备号 */\t\tchrled.minor = MINOR(chrled.devid);\t\t/* 获取分配好的次设备号 */\t&#125;\tprintk(&quot;newcheled major=%d,minor=%d\\r\\n&quot;,chrled.major, chrled.minor);\t\t\t/* 2、初始化cdev */\tchrled.cdev.owner = THIS_MODULE;\tcdev_init(&amp;chrled.cdev, &amp;led_fops);\t\t/* 3、添加一个cdev */\tcdev_add(&amp;chrled.cdev, chrled.devid, CHARLED_CNT);\t/* 4、创建类 */\tchrled.class = class_create(THIS_MODULE, LED_NAME);\tif (IS_ERR(chrled.class)) &#123;\t\treturn PTR_ERR(chrled.class);\t&#125;\t/* 5、创建设备 */\tchrled.device = device_create(chrled.class, NULL, chrled.devid, NULL, LED_NAME);\tif (IS_ERR(chrled.device)) &#123;\t\treturn PTR_ERR(chrled.device);\t&#125;\t\treturn 0;&#125;static void __exit led_exit(void)&#123;    gpio_set_value(chrled.led_gpio, 1);    gpio_free(chrled.led_gpio);\tdevice_destroy(chrled.class, chrled.devid);\tclass_destroy(chrled.class);\t/* 注销字符设备驱动 */\tcdev_del(&amp;chrled.cdev);\t\t\t\t\t\t\t\t/*  删除cdev */\tunregister_chrdev_region(chrled.devid, CHARLED_CNT); /* 注销设备号 */&#125;module_init(led_init);module_exit(led_exit);MODULE_LICENSE(&quot;GPL&quot;);MODULE_AUTHOR(&quot;lulang&quot;);\n\n设备注册流程如下：\n\n（在设备树当中）查找设备节点。\n（根据设备节点）获取gpio编号。\n申请gpio。\n（如果未指定的话）动态分配设备号。\n初始化cdev。\n添加一个cdev\n创建类。\n创建设备。（此处会自动在dev目录下创建一个设备文件。\n\n注销过程依照注册过程逆序注销。实验表明，如果在输出模式下读取gpio的状态，led_read函数当中gpio_get_value函数返回值是恒为零的。\n\n本章完结\n","tags":["驱动开发"]},{"title":"DBus快速入门","url":"/2024/07/28/dbus/Start/","content":"参考链接：\n\n中文博客：\n https://www.e-learn.cn/topic/1808992\n https://blog.csdn.net/u011942101/article/details/123383195\n https://blog.csdn.net/weixin_44498318&#x2F;article&#x2F;details&#x2F;115803936\n https://www.e-learn.cn/topic/1808992\n https://blog.csdn.net/Dontla/article/details/122530765\n\n\n\n\n\nD-Bus Specification：\n https://dbus.freedesktop.org/doc/dbus-specification.html\n https://pythonhosted.org/txdbus/dbus_overview.html\n https://dbus.freedesktop.org/doc/dbus-tutorial.html\n\n\n本文主要记录一下我对dbus的认知，当然可能会存在一些错误，欢迎读者指正。\nDBus安装命令如下：\n# 安装dbussudo apt-get install dbus# 安装d-feet工具，用于查看 session bus 和 system bussudo apt-get install d-feet# 安装glib2.0sudo apt-get install libgtk2.0-dev# 安装 dbus-glibapt-get install libdbus-glib-1-dev# for dbus-launchapt install dbus-x11\n\nc_cpp_properties.json配置：\n主要注意includePath\n&#123;    &quot;configurations&quot;: [        &#123;            &quot;name&quot;: &quot;Linux&quot;,            &quot;includePath&quot;: [                &quot;$&#123;workspaceFolder&#125;/**&quot;,                &quot;/usr/include/dbus-1.0/&quot;,                &quot;/usr/include/glib-2.0/&quot;,                &quot;/usr/lib/x86_64-linux-gnu/glib-2.0/include/&quot;            ],            &quot;defines&quot;: [],            &quot;compilerPath&quot;: &quot;/usr/bin/clang&quot;,            &quot;cStandard&quot;: &quot;c17&quot;,            &quot;cppStandard&quot;: &quot;c++14&quot;,            &quot;intelliSenseMode&quot;: &quot;linux-clang-x64&quot;        &#125;    ],    &quot;version&quot;: 4&#125;\n\nDBus整体结构DBus是基于本地套接字实现的IPC框架，可用于进程间的通信或进程与内核的通信。\n这里贴一张官方提供的dbus整体架构图：\n\nDBUS的优点：\n\n因为DBUS主要应用在同一台机器上，不会跨主机进行IPC通信。所以使用二进制协议，省去序列化的代价。但是如果需要传递像字符串指针的话，就需要通过地址将字符串拷贝到Message中，所以或多或少还是存在一些序列化的过程。\n\nDBUS的“报文”在传递时，避免了往返的开销。并且支持异步操作。\n\n报文以消息的方式传递而不是（类似于TCP）字节流。\n\nD-Bus 库的封装方式可以让开发人员利用其框架现有的对象&#x2F;类型系统，而无需学习新的IPC的特点，然后重构他们的项目。\n\n\n这里简单解释一下dbus整体架构图的一个交互流程：\n在DBUS中，一个完整的IPC交互流程包括三个部分：Bus Daemon Process、Client Application Process、Server Application Process。Bus Daemon Process有两类：Session Bus、System Bus，这里本文主要讨论Session Bus，这两类Bus主要区别在于：Session Bus是主要负责应用程序之间的一个交互，而System Bus主要负责内核和应用程序之间的一个交互。Bus Daemon Process在两个应用程序交互之中起路由作用。从图中可以看到所有的应用程序和Bus Daemon Process的连接都是通过本地socket。\n主要流程：\n\napp1（client）和app2（server）都通过Bus Daemon Process的套接字地址连接上去。\n\napp1请求一个叫com.app1.client的Bus Name，app2请求一个叫com.app2.server的Bus Name。\n\napp1构造一个method calls消息，method calls消息至少包括：Dest Bus Name（目的地址，假设这里是com.app2.server）、Object Path、Interface Name 、Method Name（包括参数，这里假设是int add(int n1, int n2)），然后将消息报文发送给Bus Daemon Process。\n\nBus Daemon Process根据消息的Dest Bus Name，将消息路由给app2。\n\napp2根据消息的Object Path、Interface Name 、Method Name调用相应的方法（add函数），然后根据返回值以及接收到的method calls消息封装成一条method returns消息，发回给Bus Daemon Process，进而路由给app1。\n\napp1接收到method returns消息后解析出里面的返回值，于此一次交互完成。\n\n\ndbus中消息有四种类型：\n\nsignals：信号，一般用于广播，不会有目的地址和返回值。\n\nmethod calls：方法调用，定向传播，有目的地址，可能有返回值。\n\nmethod returns：方法返回，简单理解就是method calls的返回值。\n\nerrors：错误，简单理解就是错误码。\n\n\n上面流程主要用到method calls、method returns两种消息类型。\n梳理了一下dbus的交互流程，其实会发现dbus和RPC非常像。不同点在于dbus更像是一种本地专用的RPC。\ndbus-daemon的地址保存在环境变量DBUS_SESSION_BUS_ADDRESS中，用于表示当前登录用户的session的dbus-daemon进程的地址，可以使用下面命令查看。\nroot@lunar-virtual-machine:~# echo $DBUS_SESSION_BUS_ADDRESSunix:path=/run/user/0/bus\n\nsession bus由用户登录时dbus-launch脚本自动启动，当然，你也可以自己在终端启动一个bus-daemon，方便调试：\nroot@lunar-virtual-machine:~# DBUS_VERBOSE=1 dbus-daemon --session --print-addressunix:abstract=/tmp/dbus-49gl7TnTcs,guid=864158a61bb94df92e3e1ac866936bd1\n\n然后将DBUS_SESSION_BUS_ADDRESS修改为：unix:abstract&#x3D;&#x2F;tmp&#x2F;dbus-49gl7TnTcs,guid&#x3D;864158a61bb94df92e3e1ac866936bd1，再启动应用程序即可连接到自己启动的dbus-daemon上。\nDBus深入理解先下载一个dbus实例：http://www.fmddlmyy.cn/down2/hello-dbus3-0.1.tar.gz\n四步构建并运行dbus服务器\n./autogen.sh./configuremake./example-service\n\n然后另起一个终端，执行d-feet命令，会弹出一个窗口：\n\n双击Methods下的Add方法，会弹出一个对话框，输入参数并确认后你就会看到相加的结果。这个具体的交互过程和。上面我们分析的流程是一样的。\n下面通过该窗口逐步了解DBUS的相关概念。\nBus Name： 应用和消息总线的连接标识符，有两类，包括：well-known name（熟知名）和unique name（唯一名）\nwell-known name形如：”org.fmddlmyy.Test“\nunique name形如：”:1.114“\nwell-known name可以被多个连接（非同时）所拥有，unique name在所有连接中是唯一的。一个连接可以同时拥有well-known name和unique name，well-known name，但是唯一名是必须的，但熟知名不是必须的。well-known name可以类比于网络中的域名，unique name可以类比于网络中的IP地址。\n当多个应用连接到消息总线，要求提供同一个公共名的服务。消息总线会把这些连接排在链表中，并选择一个连接提供公共名代表的服务。可以说这个提供服务的连接拥有了这个公共名。如果这个连接退出了，消息总线会从链表中选择下一个连接提供服务。\nNative Objects and Object Paths： Native Objects类似java中的java.lang.Object、QT中的QObject等等。在最底层的dbus中纯在形式只是一个字符串。\nObject Paths形如：”&#x2F;TestObject“\nMethods and Signals（Name）： Methods表示被具体调用的方法，而Signals就是信号。Method可以有返回值，Signals一定没有返回值，此外，Signals可以进行广播，应用程序可以向bus订阅要接收的信号的Interface，一旦注册，只要DBUS收到信号，就会根据的信号的接口，将信号广播给相应的应用程序。在最底层的dbus中纯在形式只是一个字符串。\nInterfaces（Name）： 相当于C++中的纯虚类。每个对象都有一个或者多个接口，一个接口就是多个方法和信号的集合。在最底层的dbus中纯在形式只是一个字符串。\n形如：org.fmddlmyy.Test.Basic\norg.freedesktop.DBus.Introspectable （由消息总线提供的标准接口\norg.freedesktop.DBus.Properties （由消息总线提供的标准接口\n在基于C实现的最底层的dbus中的对象、接口、方法、信号等概念可能很难去理解，我最开始也是比较困惑，但是如果你对dbus框架感兴趣的话，可以去了解一下dbus在面向对象的语言在的用法，比如dbus-c++，他里面的这些概念就非常直观。\n因为D-BUS的底层接口没有对象相关概念，所以它的Object、Methods、Signals、Interfaces都是以字符串的形式标识。在面向对象的语言对dbus进行封装后，我们可以通过编写XML来定义接口和方法，并且通过我们编写的XML来生成相应的接口代码。接口里面就会包括方法和信号的定义。用户只用去继承这些接口，然后去实现它里面的方法即可。当然用户还需要去根据这些接口去定义相应的对象。\n这里我故意漏掉了一个概念—Proxies： 代理对象用来表示其他的remote object。当触发了proxy对象的method时，将会在D-Bus上发送一个method_call的消息，并等待答复，根据答复返回。总线上的对象一般通过代理来访问。总线上的对象位于客户进程以外，而客户可以调用本地接口与对象通信，此时，本地接口充当了代理的角色。当触发了代理对象的方法时，将会在D-Bus上发送一个method_call的消息，并等待答复返回，就象使用一个本地对象一样。重点关注最后一句话！代理能够使用户去调用一个本地接口，该本地接口代替你向总线发消息，去调用另一个进程方法，并且接收返回值。简而言之，代理的概念还是要配合面向对象的语言对dbus的封装才好理解。\n比如不用代理：\nMessage message = new Message(&quot;/remote/object/path&quot;, &quot;MethodName&quot;, arg1, arg2); Connection connection = getBusConnection();           connection.send(message);           Message reply = connection.waitForReply(message);           if (reply.isError()) &#123;                         &#125;else &#123;              \tObject returnValue = reply.getReturnValue();           &#125; \n\n用代理就是：\nProxy proxy = new Proxy(getBusConnection(), &quot;/remote/object/path&quot;);           Object returnValue = proxy.MethodName(arg1, arg2);\n\ndbus规定了一个统一的接口：org.freedesktop.DBus.Introspectable，通过该接口的Introspect方法，可以递归遍历一个连接的对象树，包括server的接口和其方法、信号。另外因为Bus Daemon Process也属于进程，应用进程也可以请求Bus Daemon Process的接口和方法。Bus Daemon Process的Bus Name为：org.freedesktop.DBus，利用总线的org.freedesktop.DBus.Introspectable.Introspect方法我们可以查看消息总线对象支持的接口以及其方法、信号。此外我们还可以调用总线的\\对象的org.freedesktop.DBus.ListNames方法，来获取消息总线上已连接的所有连接名，包括所有公共名和唯一名。\n这些内容具体细节在https://www.e-learn.cn/topic/1808992有深入讲解，感兴趣的读者可以去看一看。\ndbus是支持提供server的应用程序按需启动，在client请求的server应用程序没有启动时，daemon bus首先会依据配置起一个server应用程序。通过总线的&#x2F;对象的org.freedesktop.DBus.ListActivatableNames方法，可以获取所有能够自启动的服务。\n要想让自己的server应用程序自启动，需要添加一个配置文件，如下：\nvim /usr/share/dbus-1/services/org.fmddlmyy.Test.service# [D-BUS Service]# Name=org.fmddlmyy.Test    # 定义Bus Name# Exec=/home/lvjie/work/dbus/hello-dbus3-0.1/src/example-service    # 提供可执行程序的路径\n\ndbus的方法参数、返回值类型可以是基本类型，也可以是复合类型，具体语法可以参考：https://pythonhosted.org/txdbus/dbus_overview.html。\n这里简单列举一下复合类型：\n数组：\nai - 32位整型数组\na(ii) - 元素类型为两个32位整型的结构体的数组\naai - 元素类型为32位整型的数组的数组\n字典：\na{ss} - key为string，value为string\na{is} - key为32位整数 ⇒ value为string\na{s(ii)} - key为string ⇒ value为包含2个32位整数的数组\na{sa{ss}} - key为string ⇒ value也是字典。\nDBus示例代码Server Application：不断处理来自Client Application的方法调用。\n#include &lt;stdio.h&gt;#include &lt;stdlib.h&gt;#include &lt;string.h&gt;#include &lt;dbus/dbus.h&gt;#include &lt;unistd.h&gt;void reply_to_method_call(DBusMessage *msg, DBusConnection *conn)&#123;\tDBusMessage *reply;\tDBusMessageIter arg;\tchar *param = NULL;\tdbus_bool_t stat = TRUE;\tdbus_uint32_t level = 2010;\tdbus_uint32_t serial = 0;\t\t//从msg中读取参数\tif(!dbus_message_iter_init(msg, &amp;arg))\t\tprintf(&quot;Message has noargs\\n&quot;);\telse if(dbus_message_iter_get_arg_type(&amp;arg) != DBUS_TYPE_STRING)\t\tprintf(&quot;Arg is notstring!\\n&quot;);\telse\t\tdbus_message_iter_get_basic(&amp;arg, &amp;param);\tif(param == NULL) return;\t\t\t//创建返回消息reply\treply = dbus_message_new_method_return(msg);\t//在返回消息中填入两个参数，和信号加入参数的方式是一样的。这次我们将加入两个参数。\tdbus_message_iter_init_append(reply, &amp;arg);\tif(!dbus_message_iter_append_basic(&amp;arg, DBUS_TYPE_BOOLEAN, &amp;stat))&#123;\t\tprintf(&quot;Out ofMemory!\\n&quot;);\t\texit(1);\t&#125;\tif(!dbus_message_iter_append_basic(&amp;arg, DBUS_TYPE_UINT32, &amp;level))&#123;\t\tprintf(&quot;Out ofMemory!\\n&quot;);\t\texit(1);\t&#125;\t//发送返回消息\tif(!dbus_connection_send(conn, reply, &amp;serial))&#123;\t\tprintf(&quot;Out of Memory\\n&quot;);\t\texit(1);\t&#125;\tdbus_connection_flush(conn);\tdbus_message_unref(reply);&#125;void listen_dbus()&#123;\tDBusMessage *msg;\tDBusMessageIter arg;\tDBusConnection *connection;\tDBusError err;\tint ret;\tchar *sigvalue;\t\tdbus_error_init(&amp;err);\t//创建于session D-Bus的连接\tconnection = dbus_bus_get(DBUS_BUS_SESSION, &amp;err);\tif(dbus_error_is_set(&amp;err))&#123;\t\tfprintf(stderr, &quot;ConnectionError %s\\n&quot;, err.message);\t\tdbus_error_free(&amp;err);\t&#125;\tif(connection == NULL)\t\treturn;\t//设置一个BUS name：test.wei.dest\tret = dbus_bus_request_name(connection, &quot;test.wei.dest&quot;, DBUS_NAME_FLAG_REPLACE_EXISTING, &amp;err);\tif(dbus_error_is_set(&amp;err))&#123;\t\tfprintf(stderr, &quot;Name Error%s\\n&quot;, err.message);\t\tdbus_error_free(&amp;err);\t&#125;\tif(ret != DBUS_REQUEST_NAME_REPLY_PRIMARY_OWNER)\t\treturn;\t\t//要求监听某个singal：来自接口test.signal.Type的信号\tdbus_bus_add_match(connection, &quot;type=&#x27;signal&#x27;, interface=&#x27;test.signal.Type&#x27;&quot;, &amp;err);\tdbus_connection_flush(connection);\tif(dbus_error_is_set(&amp;err))&#123;\t\tfprintf(stderr, &quot;Match Error%s\\n&quot;, err.message);\t\tdbus_error_free(&amp;err);\t&#125;\t\twhile(1)&#123;\t\tdbus_connection_read_write(connection, 0);\t\tmsg = dbus_connection_pop_message(connection);\t\t\tif(msg == NULL)&#123;\t\t\tsleep(1);\t\t\tcontinue;\t\t&#125;\t\t\tif(dbus_message_is_signal(msg, &quot;test.signal.Type&quot;, &quot;Test&quot;))&#123;\t\t\tif(!dbus_message_iter_init(msg, &amp;arg))\t\t\t\tfprintf(stderr, &quot;Message Has no Param&quot;);\t\t\telse if(dbus_message_iter_get_arg_type(&amp;arg) != DBUS_TYPE_STRING)\t\t\t\tfprintf(stderr, &quot;Param isnot string&quot;);\t\t\telse&#123;                dbus_message_iter_get_basic(&amp;arg, &amp;sigvalue);                fprintf(stdout, &quot;[method_call]Got Singal withvalue : %s\\n&quot;, sigvalue);            &#125;\t\t&#125;else if(dbus_message_is_method_call(msg, &quot;test.method.Type&quot;, &quot;Method&quot;))&#123;\t\t\t//我们这里面先比较了接口名字和方法名字，实际上应当现比较路径\t\t\tif(strcmp(dbus_message_get_path(msg), &quot;/test/method/Object&quot;) == 0)&#123;\t\t\t\treply_to_method_call(msg, connection);                fprintf(stdout, &quot;[method_call]Got method_call, reply to it!\\n&quot;);            &#125;\t\t&#125;\t\tdbus_message_unref(msg);\t&#125;&#125;int main(int argc, char **argv)&#123;\tlisten_dbus();\treturn 0;&#125;\n\nClient Application：发送Method Call消息或者信号。\n#include &lt;stdio.h&gt;#include &lt;stdlib.h&gt;#include &lt;string.h&gt;#include &lt;dbus/dbus.h&gt;#include &lt;unistd.h&gt;//建立与session D-Bus daemo的连接，并设定连接的名字，相关的代码已经多次使用过了DBusConnection* connect_dbus()&#123;\tDBusError err;\tDBusConnection *connection;\tint ret;\t\t//Step 1: connecting session bus\t\tdbus_error_init(&amp;err);\t\tconnection = dbus_bus_get(DBUS_BUS_SESSION, &amp;err);\tif(dbus_error_is_set(&amp;err))&#123;\t\tfprintf(stderr, &quot;ConnectionErr : %s\\n&quot;, err.message);\t\tdbus_error_free(&amp;err);\t&#125;\tif(connection == NULL)\t\treturn NULL;\t\t//step 2: 设置BUS name，也即连接的名字。\tret = dbus_bus_request_name(connection, &quot;test.wei.source&quot;, DBUS_NAME_FLAG_REPLACE_EXISTING, &amp;err);\tif(dbus_error_is_set(&amp;err))&#123;\t\tfprintf(stderr, &quot;Name Err :%s\\n&quot;, err.message);\t\tdbus_error_free(&amp;err);\t&#125;\tif(ret != DBUS_REQUEST_NAME_REPLY_PRIMARY_OWNER)\t\treturn NULL;\t\treturn connection;&#125;void send_a_method_call(DBusConnection *connection,char *param)&#123;\tDBusError err;\tDBusMessage *msg;\tDBusMessageIter arg;\tDBusPendingCall *pending;\tdbus_bool_t *stat;\tdbus_uint32_t *level;\t\tdbus_error_init(&amp;err);\t\t//针对目的地地址，请参考图，创建一个method call消息。Constructs a new message to invoke a method on a remote object.\tmsg = dbus_message_new_method_call(&quot;test.wei.dest&quot;, &quot;/test/method/Object&quot;, &quot;test.method.Type&quot;, &quot;Method&quot;);\tif(msg == NULL)&#123;\t\tfprintf(stderr, &quot;MessageNULL&quot;);\t\treturn;\t&#125;\t\t//为消息添加参数。Appendarguments\tdbus_message_iter_init_append(msg, &amp;arg);\tif(!dbus_message_iter_append_basic(&amp;arg, DBUS_TYPE_STRING, &amp;param))&#123;\t\tfprintf(stderr, &quot;Out of Memory!&quot;);\t\texit(1);\t&#125;\t\t//发送消息并获得reply的handle。Queues amessage to send, as withdbus_connection_send() , but also returns aDBusPendingCall used to receive a reply to the message.\tif(!dbus_connection_send_with_reply(connection, msg, &amp;pending, -1))&#123;\t\tfprintf(stderr, &quot;Out of Memory!&quot;);\t\texit(1);\t&#125;\t\tif(pending == NULL)&#123;\t\tfprintf(stderr, &quot;Pending CallNULL: connection is disconnected &quot;);\t\tdbus_message_unref(msg);\t\treturn;\t&#125;\t\tdbus_connection_flush(connection);\tdbus_message_unref(msg);\t\t//waiting a reply，在发送的时候，已经获取了methodreply的handle，类型为DBusPendingCall。\t// block until we recieve a reply， Block until the pendingcall is completed.\tdbus_pending_call_block(pending);\t//get the reply message，Gets thereply, or returns NULL if none has been received yet.\tmsg = dbus_pending_call_steal_reply(pending);\tif (msg == NULL) &#123;\t\tfprintf(stderr, &quot;ReplyNull\\n&quot;);\t\texit(1);\t&#125;\t// free the pendingmessage handle\tdbus_pending_call_unref(pending);\t// read the parameters\tif(!dbus_message_iter_init(msg, &amp;arg))\t\tfprintf(stderr, &quot;Message hasno arguments!\\n&quot;);\telse if (dbus_message_iter_get_arg_type(&amp;arg) != DBUS_TYPE_BOOLEAN)\t\tfprintf(stderr, &quot;Argument isnot boolean!\\n&quot;);\telse\t\tdbus_message_iter_get_basic(&amp;arg, &amp;stat);\t\tif (!dbus_message_iter_next(&amp;arg))\t\tfprintf(stderr, &quot;Message hastoo few arguments!\\n&quot;);\telse if (dbus_message_iter_get_arg_type(&amp;arg) != DBUS_TYPE_UINT32 )\t\tfprintf(stderr, &quot;Argument isnot int!\\n&quot;);\telse\t\tdbus_message_iter_get_basic(&amp;arg, &amp;level);\t\tprintf(&quot;Got Reply: %d,%d\\n&quot;, stat, level);\tdbus_message_unref(msg);&#125;int send_a_signal(DBusConnection *connection, char *sigvalue)&#123;\tDBusError err;\tDBusMessage *msg;\tDBusMessageIter arg;\tdbus_uint32_t  serial = 0;\tint ret;\t\t//步骤3:发送一个信号\t//根据图，我们给出这个信号的路径（即可以指向对象），接口，以及信号名，创建一个Message\tif((msg = dbus_message_new_signal(&quot;/test/signal/Object&quot;, &quot;test.signal.Type&quot;, &quot;Test&quot;))== NULL)&#123;\t\tfprintf(stderr, &quot;MessageNULL\\n&quot;);\t\treturn -1;\t&#125;\t//给这个信号（messge）具体的内容\tdbus_message_iter_init_append(msg, &amp;arg);\tif(!dbus_message_iter_append_basic(&amp;arg, DBUS_TYPE_STRING, &amp;sigvalue))&#123;\t\tfprintf(stderr, &quot;Out OfMemory!\\n&quot;);\t\treturn -1;\t&#125;\t\t//步骤4: 将信号从连接中发送\tif(!dbus_connection_send(connection, msg, &amp;serial))&#123;\t\tfprintf(stderr, &quot;Out of Memory!\\n&quot;);\t\treturn -1;\t&#125;\tdbus_connection_flush(connection);\tprintf(&quot;Signal Send\\n&quot;);\t\t//步骤5: 释放相关的分配的内存。\tdbus_message_unref(msg);\treturn 0;&#125;int main(int argc, char **argv)&#123;\tDBusConnection *connection;\tconnection = connect_dbus();\tif(connection == NULL)\t\treturn -1;\t    if (argc &gt; 1) &#123;        send_a_method_call(connection,&quot;Hello, D-Bus&quot;);    &#125; else &#123;        send_a_signal(connection, &quot;Hello,world!&quot;);    &#125;\treturn 0;&#125;\n\ngcc -Wall -o server server.c -ldbus-1 -I/usr/include/dbus-1.0/ -I/usr/include/glib-2.0/ -I/usr/lib/x86_64-linux-gnu/glib-2.0/include/ -I/usr/lib/x86_64-linux-gnu/dbus-1.0/include/gcc -Wall -o client client.c -ldbus-1 -I/usr/include/dbus-1.0/ -I/usr/include/glib-2.0/ -I/usr/lib/x86_64-linux-gnu/glib-2.0/include/ -I/usr/lib/x86_64-linux-gnu/dbus-1.0/include/\n\n结果如下：\n\n\n本章完结\n","tags":["DBus"]},{"title":"烧写uboot、linux镜像、根文件系统到开发板","url":"/2024/06/08/linux_driver/fusing/","content":"环境介绍本博客使用x6818开发板。\n公司：三星\nARM架构\nCortex-A53核\n型号：S5P6818\n特性：8核，最高主频2GHz\n\n\n烧写uboot使用网络烧写网络烧写上位机是Ubuntu虚拟机。\n\n先利用上位机，给TF卡烧写uboot，然后将TF卡插到下位机的TF0卡槽中。连接好电源口、网口、串口。电源口给开发板供电，网口用于传输可执行文件、系统镜像等，串口用于传输uboot命令、打印调试等。Ubuntu虚拟机配置成桥接模式，桥接到主机的以太网网卡上，用一根网线直接连接电脑网口和开发板网口。在下位机上电时，cpu会以TF0-&gt;TF1-&gt;MCC的顺序去读取uboot信息，并将uboot加载到内存同时cpu会跳到uboot，去执行其代码。因为我们将烧写了uboot的TF卡插到了TF0卡槽，所以cpu会直接读取TF0上的uboot。\n\nUbuntu安装tftp服务\n apt-get install tftpd-hpa \n\n如果需要的话可以通过tftp的配置文件修改共享目录的位置，如下：\n # 修改TFTP_DIRECTORY参数即可修改共享目录。vim /etc/default/tftpd-hpa\n\n为Ubuntu虚拟机配置一个静态ip，ip可以随便取，我取Ubuntuip为192.168.200.3、网关为192.168.200.2子网掩码是255.255.255.0。这里配置静态ip的教程参考Ubuntu配置静态ip。\n\n通过串口工具SecureCRT，向uboot发送命令，配置下位机uboot的ip地址、serverip、网关：\n # 打印uboot环境变量，注意重要的两个环境变量：# ipaddr=192.168.1.165      // 此乃开发板的IP地址# serverip=192.168.1.164    // 此乃上位机的IP地址# gatewayip=192.168.1.2     // 网关print \t# 设置本机的IP地址为192.168.200.4setenv ipaddr 192.168.200.4# 设置服务器的IP地址为192.168.200.3setenv serverip 192.168.200.3# 设置网关的IP地址为192.168.200.2setenv gatewayip 192.168.200.2# 保存saveenv# 再次确保环境变量是对滴!print# 下位机ping上位机# 如果出现is not alive:表示失败，继续ping一下，第一次总是不成功的# 如果出现is alive：表示成功，继续后续网络下载操作ping 192.168.200.3\n\n从上位机Ubuntu共享目录下载uboot：\n # 将上位机的ubootpak.bin文件下载到下位机的内存0x48000000处，tftp 0x48000000 ubootpak.bin\n\n将内存中的uboot数据烧写到mmc：\n # 烧写到mmc上，以0x200（512）作为起始地址，烧写0x78000这么多字节，这里是以字节为单位！update_mmc 2 2ndboot 0x48000000 0x200 0x78000\n\n拔掉TF0卡槽的TF卡，按下复位键，可以观察到串口工具SecureCRT有uboot的打印信息。\n\n\n至此uboot网络烧写介绍完毕！如果你希望宿主机可以利用vscode的remote插件连接上桥接在以太网卡的虚拟机，可以把宿主机的以太网卡手动配置一个静态ip，注意，ip的网络地址、子网掩码、网关地址一定要和虚拟机保持一致！如下：\n\n使用USB烧写这里演示Win作为上位机，USB烧写uboot，Ubuntu其实是一样的道理。\n\n同样，先利用上位机，给TF卡烧写uboot，然后将TF卡插到下位机的TF0卡槽中。连接好电源口、OTG USB口、串口。电源口给开发板供电，USB口用于传输可执行文件、系统镜像等，串口用于传输uboot命令、打印调试等。在下位机上电时，cpu会以TF0-&gt;TF1-&gt;MCC的顺序去读取uboot信息，并将uboot加载到内存同时cpu会跳到uboot，去执行其代码。因为我们将烧写了uboot的TF卡插到了TF0卡槽，所以cpu会直接读取TF0上的uboot。\n\n串口工具执行fastboot命令，此时命令行终端卡住不动，等待上位机fastboot客户端程序发送要下载的文件。\n\n上位机（windows）注意安装fastboot驱动。\n\n在windows自带的终端执行命令：\n # 对于ubuntu系统：# 先安装fastboot：sudo apt-get install fastboot# 再运行此烧写命令:sudo fastboot flash ubootpak ubootpak.binfastboot flash ubootpak ubootpak.bin\n\n拔掉TF0卡槽的TF卡，按下复位键，可以观察到串口工具SecureCRT有uboot的打印信息。\n\n\nUSB烧写uboot介绍完毕！\n烧写linux镜像以及根文件系统rootfs_ext4.img：根文件系统。\nuImage：linux内核的二进制可执行文件。\n两个文件都文件位于：porting_resource目录下\n开发板mmc的分区规划如下：\nEMMC分区规划如下：0-------512----------1M--------------65M---------------819M--------------剩余   保留      uboot\t        uImage\t        rootfs\t            大片       第一分区\t    第二分区        第三分区            暂时不分            mmcblk0boot0   \tmmcblk0p1    mmcblk0p2\t# linux内核给每个分区指定的名称\n\n使用网络烧写\n将文件uImage拷贝到上位机Ubuntu下的&#x2F;srv&#x2F;tftp共享目录，保证上下位机网络通畅\n\n通过串口，使用uboot自带的分区命令fdisk（以字节为单位），对下位机的mmc进行分区：fdisk 2 2 0x100000:0x4000000 0x4100000:0x2f200000\n 第一个参数：首先TF卡,SD卡,EMMC硬件特性一模一样，统称MMC。如果传递0：表示对SD0卡槽的TF进行分区。如果传递1：表示对SD1卡槽的TF进行分区。如果传递2：表示对EMMC进行分区。\n 第二个参数：表示要分两个分区，分别是uImage和rootfs所在的分区。注意：uboot所在的分区由linux内核自己来分。\n 最后两个参数：0x100000:0x4000000:指定第二分区uImage所在的分区起始地址和大小。0x4100000:0x2f200000：指定第三分区rootfs所在的分区起始地址和大小\n\n利用SecureCRT通过串口发送uboot命令控制下位机的uboot，使用tftp命令从上位机共享文件中拷贝uImage、rootfs_ext4.img，并将其烧录到mmc：\n # 以下命令都是通过串口发送给uboot执行# 利用tftp服务从上位机的共享目录中下载uImage到下位机内存0x480000000tftp 0x48000000 uImage  # 将uImage从内存的0x48000000写入到EMMC的0x800起始地址(以sector=512字节为单位)，烧写0x3000这么多块(6M)mmc write 0x48000000 0x800 0x3000 # 利用tftp服务从上位机的共享目录中下载rootfs_ext4.img到下位机内存0x480000000tftp 0x48000000 rootfs_ext4.img # 将rootfs从内存的0x48000000写入到EMMC的0x20800起始地址(以sector=512字节为单位)，烧写0x32000这么多块(100M)mmc write 0x48000000  0x20800  0x32000\n\n设置uboot的环境变量，让其启动后，加载内核到内存指定地址，并且为内核提供启动参数，指定根文件系统的位置：\n    # 以下命令都是通过串口发送给uboot执行   # 设置加载启动内核的环境变量，从参数中我们可以看到，uboot会将内核放在0x48000000处   setenv bootcmd mmc read  0x48000000 0x800 0x3000 \\; bootm 0x48000000   # 保存   saveenv   # 设置内核启动之后定位根文件系统的启动参数   # root=/dev/mmcblk0p2:告诉linux内核将来要挂接找的根文件系统rootfs在第三分区   # init=/linuxrc:linux内核一旦找到根文件系统rootfs，执行的第一个进程就是根目录下的linuxrc程序,注意：linuxrc会帮你启动第一号进程:/sbin/init   # console=ttySAC0,115200:指定将来linux内核打印输出的信息通过第一个串口来输出到上位机上，波特率115200# ttySAC0:第一个串口# ttySAC1:第二个串口\t\t...   # maxcpus=1:只启动一个CPU核，CPU0核   # lcd=wy070ml:指定LCD显示屏的型号   # tp=gslx680-linux:指定触摸屏的型号   setenv bootargs root=/dev/mmcblk0p2 init=/linuxrc console=ttySAC0,115200 maxcpus=1 lcd=wy070ml  tp=gslx680-linux   # 保存   saveenv\n使用uboot命令启动操作系统：\n # 以下命令都是通过串口发送给uboot执行# 直接运行bootcmd环境变量的命令启动下位机linux系统boot# 等待系统启动就OK了！\n\n使用USB烧写\n同网络烧写步骤2，进行emmc分区。\n\n串口工具执行fastboot命令，此时命令行终端卡住不动，等待上位机fastboot客户端程序发送要下载的文件。\n\nlinux镜像以及根文件系统的烧写使用fastboot命令进行USB烧写：\n # 使用fastboot，烧写到emmc的地址是在uboot的配置文件已经配置好的，不需要用户手动指定。# 如有需要，可以更改uboot的配置文件重新编译uboot、烧写uboot，可以使USB烧写地址改变。# 将uImage烧写到下位机EMMC的第二分区上，注意：uI(大写的i)mage。fastboot  flash boot uImage# 将rootfs_ext4.img烧写到EMMC第三分区上fastboot  flash system rootfs_ext4.img\n\n配置步骤同网络烧写步骤4、5。\n\n\n\n本章完结\n","tags":["驱动开发"]},{"title":"OSD实现原理","url":"/2024/11/04/mediaserver/OSD/","content":"前言本文简单记录一下我在学习IPCamera项目的的OSD的原理的过程。\n在学习OSD的过程当中，可能需要补充的基础知识：\n\nOSD是什么？\n\nBMP图像文件格式大致组成？\n\n图像调色（Palette）的原理？\n\nRGA常用接口的使用？\n\n\n涉及的项目的文件路径：\n\nosd服务：app&#x2F;mediaserver&#x2F;src&#x2F;utils&#x2F;osd。最关键的应该是osd_server.h&#x2F;cpp文件。阅读完这个文件的代码，基本就能了解osd大概的实现原理，然后其他的文件都是实现细节相关的。\n\n编码器的实现，包括如下文件：\n\n编解码器基类头文件：external&#x2F;rkmedia&#x2F;include&#x2F;easymedia&#x2F;encoder.h和codec.h\n\n编解码器基类源文件：external&#x2F;rkmedia&#x2F;src&#x2F;encoder.cc\n\nrkmpp编码器的封装：external&#x2F;rkmedia&#x2F;src&#x2F;rkmpp&#x2F;mpp_encoder和mpp_final_encoder\n\n\n\nOSD绘制：包括使用RGA“手动”绘制OSD和使用瑞芯微提供的底层mpp接口“自动”绘制。\n\n\nOSD是什么？首先需要了解OSD是什么？有什么意义？一句话概括：OSD就是给一段原生的实时视频或者录像的每一帧加上一点东西，包括但不限于：logo、时间日期，提示语等。\n在IPCamera中，OSD是如何实现的？在IPCamera中，OSD是作为一个服务来实现。实现于mediaserver层。我这里直接放上一段OSD实现最核心的代码：\n// 位于文件：app/mediaserver/src/utils/osd/osd_server.cppstatic void *ServerProcess(void *arg) &#123;  auto os = reinterpret_cast&lt;OSDServer *&gt;(arg);  char thread_name[40];  snprintf(thread_name, sizeof(thread_name), &quot;OSDServer[%d]&quot;, os-&gt;GetWidth());  prctl(PR_SET_NAME, thread_name);  LOG_DEBUG(&quot;osd ServerProcess in\\n&quot;);  int time_count = 0;  while (os-&gt;status() == kThreadRunning) &#123;    os-&gt;UpdateTimeDate();   // 更新日期    os-&gt;UpdateImage();      // 更新logo    os-&gt;UpdateMask();       // 屏蔽某一区域    os-&gt;UpdateBlink();      // 闪烁    if (time_count == 30) &#123;      os-&gt;UpdateText();     // 更新文本      time_count = 0;    &#125;    time_count++;    std::this_thread::sleep_for(std::chrono::milliseconds(os-&gt;GetDelayMs()));  &#125;  LOG_DEBUG(&quot;osd ServerProcess out\\n&quot;);  return nullptr;&#125;\n\n从函数命名就可以很清晰的看出，OSDServer中会起一个线程来运行ServerProcess函数，函数会间隔一定时间去更新最终画到视频帧上的OSD的内容。\n在IPCamera当中，每个OSD是和video_encoder_flow绑定的。并且OSD是被video_encoder_flow在编码前绘制到每一帧图像上的。\n仔细阅读源码的话，就会发现，因为时间是不断变化的，所以UpdateTimeDate函数能保证每次时间有变化时，都会调用easymedia::video_encoder_set_osd_region，去更新编码器绘制的osd的内容。相反的，因为logo和提示文字因为是几乎不变的，所以只会在系统最开始或者被变更时才会真正调用easymedia::video_encoder_set_osd_region去更新编码器的osd的内容。\n那么系统怎样监视一个OSD是否有更新呢？答案就是简单的通过一个整型数组。为1代表该种osd需要被更新到编码器中，为0就无需更新。这个数组对应osd_server头文件中的osds_db_data_change_。\n为什么是一个数组呢？没错，结合之前提到过的，一帧图像的OSD可能有好几种，每种OSD负责展示不同的信息。所以数组的存在的很有必要的。从osd_server的实现中能更清楚的理解这一点。\n谈到监视更新的变量就不得不提到OSDServer的两个核心成员：osds_db_data_、region_data_，这两成员都是数组，每个元素的类型定义如下：\n// osds_db_data_（长度为15的数组）typedef struct osd_data &#123;  int type;             // 图像还是文本还是边框  union &#123;    const char *image;    text_data_s text;    border_data_s border;  &#125;;  int width;            // osd宽  int height;           // osd高  uint8_t *buffer;      // osd内容的buffer  uint32_t size;        // buffer大小  int origin_x;         // x  int origin_y;         // y  int enable;           // 是否使能？&#125; osd_data_s;// region_data_（长度为8的数组）typedef struct &#123;  uint8_t *buffer; // Content: ID of palette  uint32_t pos_x;  uint32_t pos_y;  uint32_t width;  uint32_t height;  uint32_t inverse;  uint32_t region_id; // max = 8.  uint8_t enable;  REGION_TYPE region_type;  uint32_t cover_color; // for REGION_TYPE_COVER&#125; OsdRegionData;\n\n从OSDServer的构造函数中，我们可以看到在一个OSDServer对象创建时会进行一些比较重要的初始化操作，如下：\n\n\n设置调色板。\n\n初始化region_data_。\n\n查询数据库对osd的配置，并设置到osds_db_data_数组设置osds_db_data_change_相应元素为1，使后台线程能察觉到osd配置的改变，并同步给编码器。\n\n\nOSD更新的详细流程这里梳理一下OSD更新的详细流程：\n\n用户通过网页对OSD进行设置。\n\n下位机服务器收到并解析请求。将设置持久化到数据库。（mediaserver那边其实会收到dbus的signal）。\n\n服务器向mediaserver发送设置osd的dbus请求。\n\nmediaserver收到请求然后调用OSDServer::SetOsdRegion(region_id, map)函数进行osd的设置。（dbus请求的参数会被转换成map然后作为OSDServer::SetOsdRegion参数）。\n\n根据map设置osds_db_data_[region_id]。并且相应的osds_db_data_change_[region_id]被设置为1。\n\n后台线程检测到osds_db_data_change_[region_id]为1，根据osds_db_data_[region_id]的配置，生成图像（字体）点阵，将点阵设置到region_data_数组中。\n\n调用easymedia::video_encoder_set_osd_region函数，将osd配置到编码器中。\n\n\n如下图：\n\n图中红色字体表示Dbus发出的请求，黑色字体代表普通网络请求。\n上文一直在回避三个个比较重要的细节：什么是点阵？字体和图像是怎么绘制成点阵的？编码器是如何将osd绘制到视频的每一帧上的？下面两节就专门来解释这几个问题。\n点阵与调色点阵也是一个比较有意思的东西，这里推荐可以先去了解一下RGA里面怎么进行调色的。可以参考external&#x2F;linux-rga&#x2F;samples&#x2F;rgaColorPalette下的实例，这里面是使用rga相关的接口对图像进行调色的一个demo，源图像就是一个点阵（每个像素是一Byte），通过一个长度为256的数组（调色板，每个元素4Byte），将点阵映射成RGBA格式的图像（目的图像）。\n上面一段粗体其实就已经很好的解释了点阵的作用。总结一下：\n\n如果你想用点阵来显示一段文字（不管中文还是英文），直接将仿照文字的笔画，将你所需要的字节置为一即可。然后告诉mpp&#x2F;rga用什么调色板，将绘制好的位图传给它们，它们就能够将点阵钻换成文字显示到一帧帧画面上了。在OSDServer中，FontFactory类就是封装了ft2build库，在点阵中绘制文字。\n\n如果你想用点阵绘制一幅图片，（参考OSDServer的做法）使用bmp图像，bmp图像具体格式的学习参考链接，bmp里面使用位图数据来表示图像。OSDServer调用BMPReader类的LoadYuvaMapFromFile函数将RGB格式的位图数据通过find_color函数将三通道（rgb）的数据转换成yuva调色板的索引（一通道），这样拿到yuva的调色板后mpp&#x2F;osd就能将点阵转换成图像。\n\n\n对于文字的点阵生成参考mediaserver的osd下的font_factory的实现，对于图像的点阵生成参考mediaserver的osd下的bmp_reader的实现。原理其实都是差不多的，只不过文字只有一个颜色，所以点阵中点亮的值恒为1。而图像则是多彩的，所以索引值有多种。\n利用点阵+调色板的方式有两个很好的优势：\n\n点阵是确定的，但调色板是灵活的，用户可以根据自己的喜好，使用不同的调色板映射调色板。\n\n典型的点阵每个像素只有1Byte，占用空间小。\n\n\n最后，我认为点阵源码最重要的是color_table.h文件下的find_color函数。该函数功能是根据rgb三通道反向查找在调色板中最匹配的一个索引。\n代码如下：\nuint8_t inline find_color(const uint32_t *pal, uint32_t len, uint8_t r,                          uint8_t g, uint8_t b) &#123;    uint32_t i = 0;    uint8_t pixel = 0;    unsigned int smallest = 0;    unsigned int distance = 0;    int rd, gd, bd;    uint8_t rp, gp, bp;    smallest = ~0;    // LOG_DEBUG(&quot;find_color rgba_value %8x&quot;, (0xFF &lt;&lt; 24 | r &lt;&lt; 16 | g &lt;&lt;8 | b    // &lt;&lt;0));    for (i = 0; i &lt; len; ++i) &#123;        // rgb : 从低到高        bp = (pal[i] &amp; 0xff000000) &gt;&gt; 24;        gp = (pal[i] &amp; 0x00ff0000) &gt;&gt; 16;        rp = (pal[i] &amp; 0x0000ff00) &gt;&gt; 8;        rd = rp - r;        gd = gp - g;        bd = bp - b;        // 计算方差        distance = (rd * rd) + (gd * gd) + (bd * bd);        if (distance &lt; smallest) &#123;            pixel = i;            /* Perfect match! */            if (distance == 0)            break;            smallest = distance;        &#125;    &#125;    // LOG_DEBUG(&quot;find_color pixel %d pal[%d][%d] %8x&quot;, pixel, pixel/6, pixel%6,    // pal[pixel]);    return pixel;&#125;\n\n这里备忘一个名词，色深：色彩深度又叫色彩位数，即位图中要用多少个二进制位来表示每个点的颜色，是分辨率的一个重要指标。\n编码器IPCamera的OSD的实现是基于编码器的。而在编码器中，OSD有两种绘制方式：让MPP去绘制或者利用RGA进行OSD的绘制。因为RGA的绘制比较直观，而MPP的绘制需要深入到MPP的API中，目前这方面的基础知识尚缺，所以这里主要介绍一下RGA的绘制方式。\nOSDServer向编码器设置OSD的函数调用连如下：\n\neasymedia::video_encoder_set_osd_region()\n\nenc_flow-&gt;Control(VideoEncoder::kOSDDataChange, pbuff);\n\nenc-&gt;RequestChange(request, value);\n\n(request, value)会被挂到VideoEncoder的编码器设置链表上。\n\n\n++++++++++++++++++++++++++++++++++++++++++++++\n当有图片到达VideoEncoderFlow后，里面的回调函数会继续调用：\n\n进入函数：MPPEncoder::Process(src, dst, extra_dst);\n\n处理编码器的设置链表： while (HasChangeReq()) &#123;    auto change = PeekChange();    if (change.first &amp;&amp; !CheckConfigChange(change))    return -1;&#125;\n\n 对于RGA的OSD，MPPEncoder类中也会有8个类型为RgaOsdData的数组，OSDServer的设置最终通过上面的循环变更到这里。\n\nRgaOsdRegionProcess，该函数负责调用improcess将各个osd绘制到即将编码的图像帧上。\n\n\n\n为了方便梳理流程，最后贴一下MPPEncoder::Process(src, dst, extra_dst)中OSD相关的伪代码：\nint MPPEncoder::Process(const std::shared_ptr&lt;MediaBuffer&gt; &amp;input,                        std::shared_ptr&lt;MediaBuffer&gt; &amp;output,                        std::shared_ptr&lt;MediaBuffer&gt; extra_output) &#123;    // 处理编码器配置链表    while (HasChangeReq()) &#123;        auto change = PeekChange();        if (change.first &amp;&amp; !CheckConfigChange(change))            return -1;    &#125;    // ...    // 使用RGA绘制所有的OSD#ifdef RGA_OSD_ENABLE    if (rga_osd_cnt &gt; 0)    RgaOsdRegionProcess(hw_buffer);#endif    //...    return 0;&#125;\n\nRGA的绘制其实就是按一定比例使用图像blend（混合）的方式进行叠加的。RGA的作用呢包括：对图像缩放、裁剪、混合、调色、拷贝等。它是基于RGA硬件独立于CPU的去做这些操作的，所提供的接口以及用法可以参考external&#x2F;linux-rga。对于MPP对OSD的绘制，作者暂时还未能深入了解，感兴趣的读者可以自行深入研究。\n\n本章完结\n","tags":["音视频"]},{"title":"音视频基础分享","url":"/2024/10/19/mediaserver/Start/","content":"adb push src dest\nfile 可执行文件     获取可执行文件架构\nmodetest命令可以查看图层信息\nRKMedia的各个组件及其交互首先上图：\n\n\n考虑到公司业务主要是相机，所以，主要去关注图像数据流，对于音频数据流直接忽略。\n图像数据流向：\n\nCamera Sensor将光信号转换成电信号（Raw数据） -&gt;\n\nISP（ image signal peocess）初步对raw数据进行一些处理，此时数据格式变成了（yuv420的变种）NV12格式 -&gt;\n\nNV12格式数据下面的流向大体有三种：MPP、RGA、DRM。\n\n首先是MPP，在rkmedia中，该模块主要对传送过来的图像数据做编码操作，比如将数据编码成H264、H265格式。这些格式能极大的压缩图像数据的大小。具体H264里面的结构，文章后面有详细介绍。OSD对于框图中MPP上面的OSD模块，可以为MPP中每一帧图像添加一些额外的标记吧。OSD模块和MPP是 相互绑定 的，至少我目前在代码中看到的是这样。\n\n其次是RGA，该模块独立存在，并且可以对输入的NV12图像进行裁剪、缩放、旋转，然后将处理都图像数据发送给MPP模块。\n\n最后是DRM模块，该模块就是一个图像显示框架，可以直接将输入的数据显示到屏幕上。\n\n在经过MPP模块编码后的图像，最后还会一个叫Muxer的模块，该模块会将编码后的图像数据和音频数据进行打包，打包成MP4或者flx等格式，最后推流给服务器。\n\n\nraw原始图像格式有四种包括：BGGR、RGGB、GBRG、GRBG。也称Bayer raw。\nVI基于v4l2接口实现\nRKMedia接收者和发送者绑定限制：\n\n摄像头的工作方式摄像头有三种工作方式：\ntypedef enum &#123;    RK_AIQ_WORKING_MODE_NORMAL,                 // 又称单帧模式    RK_AIQ_WORKING_MODE_ISP_HDR2    = 0x10,     // 两帧和成一帧    RK_AIQ_WORKING_MODE_ISP_HDR3    = 0x20,     // 三帧合成一帧//    RK_AIQ_WORKING_MODE_SENSOR_HDR = 10, // sensor built-in hdr mode&#125; rk_aiq_working_mode_t;\n\nhdr解释为高动态范围，能显示的亮度范围更大，三帧合一中的三帧对应：欠曝光帧，正常曝光帧，过度曝光帧。\n一般使用常规单帧\n关于MIPI接口标准也是很复杂，本文先在应用层简单了解一下它的使用，具体细节，后面需要再去深究。这里还是贴一张它的框图：\n\n上面的CCI就是相机控制接口，使用I2C进行通信。下面CSI就是相机串行接口，主要是传输数据用的。\n接下来根据ISP的初始化代码来进一步了解ISP：\n代码如下：\n// RK_S32 SAMPLE_COMM_ISP_Init(RK_S32 CamId, rk_aiq_working_mode_t WDRMode,//                             RK_BOOL MultiCam, const char *iq_file_dir) rk_aiq_working_mode_t hdr_mode = RK_AIQ_WORKING_MODE_NORMAL;int fps = 30;SAMPLE_COMM_ISP_Init(s32CamId, hdr_mode, bMultictx, pIqfilesPath);SAMPLE_COMM_ISP_Run(s32CamId);// 设置isp处理帧率非视频显示帧率SAMPLE_COMM_ISP_SetFrameRate(s32CamId, fps);\n\nCamId：代表使用哪个mipi csi，0代表mipi csi0，1代表mipi csi1，如果只有一个摄像头就可以为0，不管摄像头插在哪个接口（默认id就是0），如果有两个摄像头，就必须有camid的区分。\nWDRMode：就是上面提到的三种工作模式。\nMultiCam：false代表开启一个摄像头的isp，true代表开启多个摄像头的isp，一个摄像头设置为false即可。\niq_file_dir：并且ISP的工作会依赖iq配置文件，所以函数SAMPLE_COMM_ISP_Init中iq_file_dir就是iq文件所在目录。\n此外，当多个摄像头使用一个isp会分时复用，\n注意这里提一下ISPP输出码流格式限制：\n\nRGA模块的使用接下来深入认识一下RGA模块的使用：\nRGA_ATTR_S stRgaAttr;memset(&amp;stRgaAttr, 0, sizeof(stRgaAttr));stRgaAttr.bEnBufPool = RK_TRUE;             // 使用缓冲池stRgaAttr.u16BufPoolCnt = 3;                // 缓冲池的数量stRgaAttr.u16Rotaion = 90;                  // 旋转90度stRgaAttr.stImgIn.u32X = 0;                 // 在输入图像的横坐标为x处取样stRgaAttr.stImgIn.u32Y = 0;                 // 在输入图像的纵坐标为y处取样stRgaAttr.stImgIn.imgType = IMAGE_TYPE_NV12;// 输入图像格式stRgaAttr.stImgIn.u32Width = video_width;   // 输入图像宽stRgaAttr.stImgIn.u32Height = video_height; // 输入图像高stRgaAttr.stImgIn.u32HorStride = video_width; // 水平跨距stRgaAttr.stImgIn.u32VirStride = video_height;  // 垂直跨距stRgaAttr.stImgOut.u32X = 0;                    // 相对stImgIn.u32XstRgaAttr.stImgOut.u32Y = 0;                    // 相对stImgIn.u32YstRgaAttr.stImgOut.imgType = IMAGE_TYPE_RGB888; // 输出图像格式stRgaAttr.stImgOut.u32Width = disp_width;       // 输出宽stRgaAttr.stImgOut.u32Height = disp_height;     // 输出高stRgaAttr.stImgOut.u32HorStride = disp_width;   // 含义同输入stRgaAttr.stImgOut.u32VirStride = disp_height;  // 含义同输入ret = RK_MPI_RGA_CreateChn(0, &amp;stRgaAttr);\n\n前后还其实涉及VI模块、以及VI和RGA模块的绑定，考虑到篇幅太大，这里就掠过了，主要看RGA模块的配置。\n总结一下：stImgIn可以利用stImgIn.u32X、stImgIn.u32Y进行裁剪，stImgOut可以利用stImgOut.u32Width、stImgOut.u32Height进行缩放。\nRGA模块对图像格式的支持也是有限的，参考如下：\n\nmodetest命令可查看各个图层支持的格式\nVOP对  VO_PLANE_PRIMARY &#x3D; 0,  VO_PLANE_OVERLAY,两层有裁剪功能。并且VO_PLANE_OVERLAY还支持缩放\n  VO_PLANE_OVERLAY -&gt; win0  VO_PLANE_PRIMARY -&gt; win2  VO_PLANE_CURSOR -&gt; 背景层（rv1126不支持）\n杂项I帧（IDR帧，Instantaneous Decoding Refresh） ：帧内编码帧是一种自带全部信息的独立帧，无需参考其它图像便可独立进行解码，视频序列中的第一个帧始终都是I帧。 属于帧内压缩，压缩质量好，压缩比例小。IDR帧一定是I帧，I帧不一定是IDR帧。\nP帧 ：前向参考，采用帧内、帧间压缩。\nB帧 ：前后参考，采用帧内、帧间压缩，实时视频传输会关闭B帧。未来无法预测，需要先将B帧缓冲，等待后面的帧解码后，再从B帧往后开始播放。\n压缩比：B&gt;P&gt;I\nGOP ：H264中GOP就是一个图像序列，即从I帧开始到最后一个连续的非I帧为止的一组帧，一般而言同一组GOP的帧之间相似度很高。\nGOP又分两种：\n\nopen GOP：不同GOP之间的P、B帧可以跨GOP进行参考\n\nclose GOP：不同GOP之间的P、B帧不能跨GOP参考，也即不能参考IDR帧之前的帧。\n\n\nRKKmedia的GOP有三种模式：如下：\n\n再细分一帧图像可以划分成一个或者多个slice，slice称为片或者条带。\nslice：I条带、P条带、B条带。为支持不同编码流之间的切换，还定义了SI条带、SP条带。\n设置条带的目的就是为了并行编码，使编码片之间相互独立进行编码，能够限制误码的扩散和传播。\n\n宏块就是视频压缩的基本单位、H264固定16*16。\nH265：编码单元的名称叫：CTU（树形编码单元），大小是64*64像素，每个CTU包含3个CTB（树形编码快），每个CTU包含若干CU（编码单元），CU还能进行更灵活的细分。\n8*8小宏块压缩比小，但图像质量好\n16*16大宏块压缩比大，但图像质量查，一些些细节存在失真\n\n\n\nYUV参考：https://zhuanlan.zhihu.com/p/113122344\n首先了解一下RGB，RGB以三原色（红绿蓝）来表示一张图片，以一张1280 * 720 大小的图片为例，因为每个像素都有三个字节来控制颜色，所以占用 1280 * 720 * 3 &#x2F; 1024 &#x2F; 1024 &#x3D; 2.63 MB 存储空间\n而YUV 颜色编码采用的是 明亮度 和 色度 来指定像素的颜色。\n色度 又定义了颜色的两个方面：色调和饱和度\n也即yuv中每个像素也可以使用三个字节来控制其颜色。\n但是： 对于 YUV 图像来说，并不是每个像素点都需要包含了 Y、U、V 三个分量，根据不同的采样格式，可以每个 Y 分量都对应自己的 UV 分量，也可以几个 Y 分量共用 UV 分量。Y 和 UV 分量是可以分离的，如果没有 UV 分量一样可以显示完整的图像，只不过是 黑白的。\nyuv和rgb可以相互转化，一般来说，传输会使用yuv（节省带宽），显示的时候会使用rgb。公式如下：\n\n\n接下来就是yuv图像的采样，yuv图像的采样就是基于上面rgb转换成yuv后的数据进行采样，YUV 图像的主流采样方式有如下三种：\n\nYUV 4:4:4 采样\n\nYUV 4:2:2 采样\n\nYUV 4:2:0 采样\n\n\nYUV 4:4:4 采样YUV 4:4:4 采样是完整的将rgb-&gt;yuv，不存在分量的丢失\n形如：\nrgb-&gt;yuv：假如图像像素为：[Y0 U0 V0]、[Y1 U1 V1]、[Y2 U2 V2]、[Y3 U3 V3]对yuv进行采样：采样后的yuv码流：Y0 U0 V0 Y1 U1 V1 Y2 U2 V2 Y3 U3 V3对采样后的yuv码流进行网络传输：...将采样后的yuv码流-&gt;“完整”的yuv：最后映射出的像素点依旧为 [Y0 U0 V0]、[Y1 U1 V1]、[Y2 U2 V2]、[Y3 U3 V3] \n\n可以看出来，其图像大小和rgb一样，没有达到节省带宽的目的。\nYUV 4:4:4 也是采样进行的第一步，先将rgb码流转换成完整的yuv码流，再根据需求，对vu分量按比例进行采样。（注意：y分量不可漏采！）\nYUV 4:2:2 采样表示uv分量占y分量的一半，形如：\nrgb-&gt;yuv：假如图像像素为：[Y0 U0 V0]、[Y1 U1 V1]、[Y2 U2 V2]、[Y3 U3 V3]对yuv进行采样：采样后的yuv码流为：Y0 U0 Y1 V1 Y2 U2 Y3 V3 对采样后的yuv码流进行网络传输：...将采样后的yuv码流-&gt;“完整”的yuv：最后映射出的像素点为 [Y0 U0 V1]、[Y1 U0 V1]、[Y2 U2 V3]、[Y3 U2 V3]\n\n采样规则： 每采样过一个像素点，都会采样其 Y 分量，而 U、V 分量就会 间隔一列 采样。也即uv分量是YUV 4:4:4的1&#x2F;2\n可以看到再最后将采样后的yuv”还原“成”完整“的yuv后，第一个像素和第二个像素共用一个uv分量。第三像素和第四像素也一样。并且采样后的yuv码流大小为：（1280 * 720 * 1 + （1280 * 720） &#x2F; 2 + （1280 * 720） &#x2F; 2）&#x2F; 1024 &#x2F; 1024 &#x3D; 1.7578125 MB 可以看到压缩了近三分之一的带宽。\nYUV 4:2:0 采样YUV 4:2:0采样会比较难理解，举个例子，如下：\nrgb-&gt;yuv： 假设图像像素为： [Y0 U0 V0]、[Y1 U1 V1]、 [Y2 U2 V2]、 [Y3 U3 V3][Y5 U5 V5]、[Y6 U6 V6]、 [Y7 U7 V7] 、[Y8 U8 V8] 对yuv进行采样：采样后的yuv码流为：Y0 U0 Y1 Y2 U2 Y3 Y5 V5 Y6 Y7 V7 Y8对采样后的yuv码流进行网络传输：... 将采样后的yuv码流-&gt;“完整”的yuv：最后映射出的像素点为：[Y0 U0 V5]、[Y1 U0 V5]、[Y2 U2 V7]、[Y3 U2 V7][Y5 U0 V5]、[Y6 U0 V5]、[Y7 U2 V7]、[Y8 U2 V7]\n\n采样规则： 每采样过一个像素点，都会采样其 Y 分量，而 U、V 分量就会交替 间隔一行 采样，并且一行采样中，隔一列 采样。也即uv分量是YUV 4:2:2 采样的1&#x2F;2\n最后在将采样的yuv”还原“成”完整“的yuv后，每四方格像素会共用一个uv分量。并且采样后的yuv码流大小为： （1280 * 720 * 1 + （（1280  &#x2F; 2） * （720 &#x2F; 2）） + （（1280  &#x2F; 2） * （720 &#x2F; 2）））&#x2F; 1024 &#x2F; 1024 &#x3D; 1.318359375 MB 将原来的rgb码流大小压缩了1&#x2F;2倍！\nYUV 4:2:2 变种YUV 4:2:2 采样又有很多变种，比如：YUYV 格式、UYVY 格式、YUV 422P 格式。\nYUYV（打包格式）形如：\nrgb-&gt;yuv：假如图像像素为：[Y0 U0 V0]、[Y1 U1 V1]、[Y2 U2 V2]、[Y3 U3 V3]对yuv进行采样：采样后的yuv码流为：Y0 U0 Y1 V0 Y2 U1 Y3 V1 对采样后的yuv码流进行网络传输：...将采样后的yuv码流-&gt;“完整”的yuv：最后映射出的像素点为 [Y0 U0 V0]、[Y1 U0 V0]、[Y2 U1 V1]、[Y3 U1 V1]\n\n采样规则： 每采样过一个像素点，都会采样其 Y 分量，而 U、V 分量逐帧依次采样。\nUYVY（打包格式） 和YUYV同理，仅仅存储的顺序不同。\nYUV 422P（平面格式） ：先存储所有Y分量，再存储U，最后V。\nYUV 4:2:0 变种主要包括两大类：YUV 420P 和 YUV 420SP，YUV 420P对应YU12、YV12，YUV 420SP对应NV12、NV21。YUV 420P 和 YUV 420SP都是基于平面模式存储。这里就不展开讨论。\n\n\n\nH264可以参考网站： https://www.zzsin.com/article/avc_0_start.html\nH264没有音频、没有时间戳，处理图像本身，啥都没有。\nH264又称avc，由一个个nalu组成。\n有两种标准：AnnexB、avcC\nAnnexB，nalu之间使用起始码分隔（一般是001或者0001（每位代表一字节）），并且类型为sps、pps的nalu当作普通nalu处理。当然，如果原数据中本身存在001字节序列，为使其正常传输，在发送数据前，会将原数据中的所有的”已定义字符“（001&#x2F;0001）转义一下，这也是防竞争字节的概念。规则如下：\n0 0 0 =&gt; 0 0 3 00 0 1 =&gt; 0 0 3 10 0 2 =&gt; 0 0 3 20 0 3 =&gt; 0 0 3 3\n\navcC，在每个nalu前面有一个固定长度的字节，这些字节会描述紧跟着的nalu有效载荷数据的长度。并且在一路采用 avcC 打包的 H.264 流之中，我们首先看到的将是一段被称之为 extradata 的数据，这段数据定义了这个 H.264 流的基本属性数据，当然，也包含了 SPS 和 PPS 数据。和AnnexB不同，avcC会将SPS和PPS放到extradata中（也可以把extradata看作协议头）。extradata还会定义，固定长度的字节的固定长度。extradata格式如下：\n\n\n\n长度(bits)\n名称\n备注\n\n\n\n8\nversion\n总是等于 0x01\n\n\n8\navc profile\n所存放第一个 SPS 的第一个字节\n\n\n8\navc compatibility\n所存放第一个 SPS 的第二个字节\n\n\n8\navc level\n所存放第一个 SPS 的第三个字节\n\n\n6\nreserved\n保留字段\n\n\n2\nNALULengthSizeMinusOne\nNALU Body Length 数据的长度减去 1\n\n\n3\nreserved\n保留字段\n\n\n5\nnumber of SPS NALUs\n有几个 SPS，一般情况下这里是 1\n\n\n\nfor(int i&#x3D;0; i&lt;number of SPS NALUs; i++){\n~\n\n\n16\nSPS size\nSPS 的长度\n\n\n变长\nSPS NALU data\nSPS NALU 的数据\n\n\n\n}\n~\n\n\n8\nnumber of PPS NALUs\n有几个 PPS，一般情况下这里是 1\n\n\n\nfor(int i&#x3D;0; i&lt;number of PPS NALUs; i++){\n~\n\n\n16\nPPS size\nPPS 的长度\n\n\n变长\nPPS NALU data\nPPS NALU 的数据\n\n\n\n}\n~\n\n\nnalu的第一个字节依次存放：禁止位、该nalu的重要性、nalu的类型。格式如下：\n\n\n\nnal_unit( NumBytesInNALunit )\nC\nDescriptor\n\n\n\nforbidden_zero_bit\nAll\nf(1)\n\n\nnal_ref_idc\nAll\nu(2)\n\n\nnal_unit_type\nAll\nu(5)\n\n\n…\n…\n…\n\n\n典型的nalu类型有：\nnalu的类型为sps（Sequence Paramater Set）的level表示视频质量（5.1）\nnalu的类型为sps（Sequence Paramater Set）的profile表示码流压缩档次（100）\nnalu的类型为sei：补充增强信息单元。存放用户自定义的信息（可有可无\n其余的参考下图：\n\nsps的profile值可以参考如下：\n\nnalu后续字节就是图像的有效载荷。\n\n\n\n杂项H264中，码率控制模式（RCMode）包括：CBR、VBR、AVBR\n\nCBR：恒定码率，码率变化平稳，但是当有运动画面时，图像质量会变差只考虑带宽、不考虑图像质量，可以使用这种模式\n\nVBR：可变码率，码率会依据图像质量的变化而变化，在意图像质量可以使用该模式。\n\nAVBR：自适应可变码率，会自动检测当前编码的图像画面是运动的还是静止的，如果是运动的，会提高码率，否则会降低码率，同时考虑带宽和图像质量就使用该模式。结合了CBR和VBR的优点。\n\n\nRKMedia中有专门的成员来配置选择哪种模式。\nROI：region of interest：感兴趣的区域。\nSVC和AVC都属于H264但是属于不同标准\n\n本章完结\n","tags":["音视频"]},{"title":"深入浅出StorageManager","url":"/2025/07/05/mediaserver/StorageManager/","content":"前言首先说明一下StorageManager在IPC中起到的作用：从英文命名可以大致猜出它的作用——存储管理，主要功能就是维护存储的目录结构以及数据库。\n进一步的：这些目录里面会放什么？数据库里面会放什么？\n下面一一回答这些问题：\n\n目录主要会存放手动&#x2F;自动录制的视频以及抓拍的照片。\n\n数据库主要记录录制视频、照片的文件名。在网页如下页面中的视频&#x2F;照片列表的数据来源就是这个数据库。\n\n\n如下表数据就来自StorageManager独自维护的数据库当中：\n\n\n\n这里强调一下，StorageManager和自动存储功能需要区分开，自动存储并不由StorageManager实现，而是在mediaserver中由SchedulesManager来实现，该模块会在以后的博客进行讲解，本文主要集中在StorageManager。\n其实在看StorageManager之前我是带着一些疑问的：\n在IPC网页上进行手动&#x2F;自动视频录制时，产生的视频文件放在了哪里？视频的文件名肯定是保存在数据库当中的。所以首要的目标就是找到存储视频文件名的数据库。经过深入阅读rkmedia下的muxer_flow，发现视频文件的创建和写入就是在这个类中进行，但奇怪的是我搜遍了mediaserver，没一处是向数据库中写入录制视频的文件名的。（PS：我当时并不知道存储视频文件名的数据库是在StorageManager当中。）\n苦苦寻找很久，最后准备转变思路看看网页在删除录制的视频文件时会和哪个数据库产生关联？结果前端在点击删除视频文件时，后端会做如下操作：\n// ipcweb-backend/src/storage_api.cppint media_file_delete(std::string delete_type, nlohmann::json file_list) &#123;    int path_id = file_path_get(delete_type);    if (path_id &lt; 0) &#123;        return 0;    &#125;    char *str = storage_manager_get_media_path();    nlohmann::json scan_path = nlohmann::json::parse(str).at(&quot;sScanPath&quot;);    std::string path = scan_path.at(path_id).at(&quot;sMediaPath&quot;);    for (auto &amp;x: nlohmann::json::iterator_wrapper(file_list)) &#123;        std::string delete_name = x.value();        if (delete_name.find(&quot;/&quot;) == std::string::npos) &#123;            std::string delete_path = path + &quot;/&quot; + delete_name;            int rst = unlink((char *)delete_path.c_str());        &#125; else &#123;            minilog_debug(&quot;media_file_delete: unlaw name is %s&quot;, (char *)delete_name.c_str());        &#125;    &#125;    return 1;&#125;\n\n什么？删除文件直接unlink？？？这也太暴力了，并且这块也没有删除数据库的操作啊，网页上显示数据库确实 （通过某种方式）检测 到文件被unlink了并且自动删除数据库中对应的文件名。这种操作也太奇怪了！此时我注意到了：“sScanPath”、“sMediaPath”等字眼。所以经过一番跟踪，了解真相原来在StorageManager当中。并且在muxer_flow创建一个视频文件时，也是 通过某种方式检测 到文件被create并且自动将其添加到数据库当中。\n所以接下来让我们一步一步去解析。\nStorageManager的实现首先贴上StorageManager核心初始化代码，篇幅有限，这里省略和dbserver重合的初始化代码：\nstatic void *main_init(void *arg) &#123;    LOG_INFO(&quot;storage_manager init\\n&quot;);    msg_init();    remove(db_file);    rkdb_init(db_file);    db_monitor_init();    manage_init();          // 消息处理线程在此处启动    uevent_monitor_init();    LOG_INFO(&quot;storage_manager init finish\\n&quot;);&#125;int main( int argc , char ** argv) &#123;    pthread_create(&amp;thread_id, NULL, (void*)main_init, NULL);&#125;\n\n消息机制 - msg_init()msg_init会初始化一个消息队列，它的实现很简单，主要在msg_process.c文件，对StorageManager的任何操作都是先将其封装成一条带类型的消息，然后将其放到消息队列当中，等待消息处理线程的消费。没错，这就是经典的生产者消费者问题的简单实现。\nStorageManager核心 - db_monitor_init()如小标题所言，db_monitor_init所做的初始化正是StorageManager的核心所在，它会构建出存储目录的结构，并且每次开机都新建一个数据库，将所有的视频文件和图片名重新添加到数据库当中。并通过一种类似eopll的消息机制来监视视频文件的变化。\ndb_monitor_init所做的初始化如下：\nvoid db_monitor_init(void)&#123;    disable_loop();#ifdef AUTO_ADJUST_MEDIAPATH    if (access(&quot;/dev/block/by-name/media&quot;, F_OK)) &#123;        LOG_INFO(&quot;/dev/block/by-name/media folder does not exist, using /userdata/ as emmc path\\n&quot;);        while (set_emmc_path(&quot;/userdata&quot;) != 0) &#123;            LOG_INFO(&quot;set_emmc_path, wait dbserver.\\n&quot;);            usleep(50000);        &#125;    &#125; else &#123;        LOG_INFO(&quot;/dev/block/by-name/media folder exist\\n&quot;);    &#125;#endif    while (get_storage_config() != 0) &#123;        LOG_INFO(&quot;dbserver_get_storage_config, wait dbserver.\\n&quot;);        usleep(50000);    &#125;    while (get_meida_folder() != 0) &#123;        LOG_INFO(&quot;dbserver_get_meida_folder, wait dbserver.\\n&quot;);        usleep(50000);    &#125;    while (get_disk_path() != 0) &#123;        LOG_INFO(&quot;dbserver_get_disk_path, wait dbserver.\\n&quot;);        usleep(50000);    &#125;    dbus_monitor_signal_registered(DBSERVER_STORAGE_INTERFACE, DS_SIGNAL_DATACHANGED, &amp;signal_storage_datachanged);&#125;\n\n因为main中会主动开启dbus的loop所以，这里调用了一下disable_loop防止dbus_monitor_signal_registered函数在注册信号时自动又开启一个loop。\n从上面的代码可以看到有四个while循环，我们下面把注意主要放在最后三个while循环上。\n配置默认使用的挂载点 - get_storage_config代码如下：\nstatic int get_storage_config(void) &#123;    char *json_str = dbserver_get_storage_config();    if (json_str) &#123;        add_db_storage_config(json_str);        g_free(json_str);        return 0;    &#125;    return -1;&#125;\n\n该函数首先会向dbserver读取默认挂载点的配置，如下：\n\n经过阅读源码得知，该MountPath就是emmc的挂载目录。\n然后调用add_db_storage_config函数向消息队列中添加一条配置当前存储路径的消息，存储路径正是使用的数据库当中的默认配置：&#x2F;userdata&#x2F;media。正是这个原因，在系统最开始的时候，打开网页，我们录制的视频默认都会放在emmc的挂载目录下。\n消息处理函数如下：\nstatic void add_storage_config(char *json_str)&#123;    json_object *j_ret = json_tokener_parse(json_str);    json_object *j_array = json_object_object_get(j_ret, &quot;jData&quot;);    int len = json_object_array_length(j_array);    for (int i = 0; i &lt; len; i++) &#123;        json_object *j_obj = json_object_array_get_idx(j_array, i);        char *path = (char *)json_object_get_string(json_object_object_get(j_obj, &quot;sMountPath&quot;));        if (storage_config.mountpath)            g_free(storage_config.mountpath);        storage_config.mountpath = g_strdup_printf(&quot;%s&quot;, path);        storage_config.id = (int)json_object_get_int(json_object_object_get(j_obj, &quot;id&quot;));        storage_config.freesize = (int)json_object_get_int(json_object_object_get(j_obj, &quot;iFreeSize&quot;));        storage_config.freesizenotice = (int)json_object_get_int(json_object_object_get(j_obj, &quot;iFreeSizeNotice&quot;));    &#125;    json_object_put(j_ret);    signal_update_media_path();&#125;\n\n函数实现非常简单，就是解析数据库里的数据，然后填充到storage_config变量上。\n其中storage_config全局变量会记录当前存储系统使用的存储介质的挂载点。它的结构体定义如下：\nstruct StorageConfig &#123;    int id;    int freesize;    int freesizenotice;    char *mountpath;    // 挂载点&#125;;\n\n构造db_media_folder_list目录链表 - get_meida_folder代码如下：\nstatic int get_meida_folder(void) &#123;    char *json_str = dbserver_get_storage_media_folder();    if (json_str) &#123;        add_db_media_folder(json_str);        g_free(json_str);        return 0;    &#125;    return -1;&#125;\n\n该函数会先查询dbserver的StorageMediaFolder表，在我们的项目中，该表有如下数据：\n\n这里解释一下表中各个字段的意义：\n\n\n\nid\nsMediaFolder\nsThumbFolder\nsFormat\nicamld\niType\niDuty\niMaxNUI\n\n\n\n目录ID\n目录名\n缓存文件目录名\n目录当中文件所使用的格式\n目录属于（当前）拼接码流还是融合码流（0：拼接，1：融合）\n目录存放文件类型（0：video，1：photo，其他（PS目前还不太清楚））\n目录中文件总大小超过介质的多少（百分比）开始回滚（删除最早视频）\n目录中文件总的数量超过该值即开始回滚\n\n\n拿到所有的entry后，会让每个entry作为一个节点，构造成一个链表：\nstatic void add_media_folder(char *json_str)&#123;    json_object *j_ret = json_tokener_parse(json_str);    json_object *j_array = json_object_object_get(j_ret, &quot;jData&quot;);    int len = json_object_array_length(j_array);    for (int i = 0; i &lt; len; i++) &#123;        struct DbMediaFolder *mediafolder = malloc(sizeof(struct DbMediaFolder));        if (db_media_folder_list) &#123;            struct DbMediaFolder *tmp = db_media_folder_list;            while (tmp-&gt;next) &#123;                tmp = tmp-&gt;next;            &#125;            tmp-&gt;next = mediafolder;        &#125; else &#123;            db_media_folder_list = mediafolder;        &#125;        json_object *j_obj = json_object_array_get_idx(j_array, i);        mediafolder-&gt;media_folder = g_strdup((char *)json_object_get_string(json_object_object_get(j_obj, &quot;sMediaFolder&quot;)));        /* 省略... */        mediafolder-&gt;maxnum = (int)json_object_get_int(json_object_object_get(j_obj, &quot;iMaxNum&quot;));        mediafolder-&gt;next = NULL;    &#125;    json_object_put(j_ret);&#125;\n\n这段尾插构造链表的代码也不过多啰嗦了，最终形成如下结构：\ndb_media_folder_list -&gt; (DbMediaFolder&#123;.media_folder=video0，.thumb_folder=video0/.thumb, ...&#125;)                                    |                                    V                        (DbMediaFolder&#123;.media_folder=photo0，.thumb_folder=photo0/.thumb, ...&#125;)                                    |                                    V                        (DbMediaFolder&#123;.media_folder=video1，.thumb_folder=video1/.thumb, ...&#125;)                                    |                                    V                        (DbMediaFolder&#123;.media_folder=photo1，.thumb_folder=photo1/.thumb, ...&#125;)                                    |                                    V                        (DbMediaFolder&#123;.media_folder=black_list, ...&#125;)                                    |                                    V                        (DbMediaFolder&#123;.media_folder=snapshot, ...&#125;)                                    |                                    V                        (DbMediaFolder&#123;.media_folder=white_list, ...&#125;)                                    |                                    V                                nullptr\n\n构造存储路径结构链表 - get_disk_path代码如下：\nstatic int get_disk_path(void) &#123;    char *json_str = dbserver_get_storage_disk_path(NULL);    if (json_str) &#123;        add_db_disk_path(json_str);        g_free(json_str);        return 0;    &#125;    return -1;&#125;\n\n先是查询dbserver的StorageDiskPath表，在项目中，该表的数据如下：\n\n该表各个字段的含义如下：\n\n\n\nid\nsPath\nsName\niMount\n\n\n\n就是ID嘛\n存储介质挂载点\n存储介质的名字\n是否已经被挂载\n\n\n拿到所有的entry后，也是封装成一条消息，消息的处理函数的逻辑是这样的：结合上一节的db_media_folder_list，分别为每个介质的挂载点构造一条目录路径链表。（PS，如果你学习过数据库笛卡尔积的概念，可能这样解释会更加清晰一点：这里的处理函数其实就是在将StorageDiskPath表 和 上一节构造的db_media_folder_list做笛卡尔积）\nstatic void add_disk_path(char *json_str)&#123;    json_object *j_ret = json_tokener_parse(json_str);    json_object *j_array = json_object_object_get(j_ret, &quot;jData&quot;);    int len = json_object_array_length(j_array);    for (int i = 0; i &lt; len; i++) &#123;        json_object *j_obj = json_object_array_get_idx(j_array, i);        char *path = (char *)json_object_get_string(json_object_object_get(j_obj, &quot;sPath&quot;));        struct FileStatus *status = g_hash_table_lookup(db_file_hash, path);        if (status == NULL) &#123;            status = malloc(sizeof(struct FileStatus));            memset(status, 0, sizeof(struct FileStatus));            pthread_mutex_init(&amp;(status-&gt;mutex), NULL);            pthread_cond_init(&amp;(status-&gt;cond), NULL);            g_hash_table_replace(db_file_hash, g_strdup(path), (gpointer)status);        &#125;        status-&gt;mountpath = g_strdup(path);        status-&gt;id = (int)json_object_get_int(json_object_object_get(j_obj, &quot;id&quot;));        status-&gt;name = g_strdup((char *)json_object_get_string(json_object_object_get(j_obj, &quot;sName&quot;)));        status-&gt;mount = (int)json_object_get_int(json_object_object_get(j_obj, &quot;iMount&quot;));        struct DbMediaFolder *media_folder = db_media_folder_list;        struct ScanPath *scanpath_last = NULL;        while (media_folder) &#123;            struct ScanPath *scanpath = (struct ScanPath *)calloc(1, sizeof(struct ScanPath));            if (scanpath_last == NULL) &#123;                status-&gt;scanpath = scanpath;            &#125; else &#123;                scanpath_last-&gt;next = scanpath;            &#125;            scanpath_last = scanpath;            scanpath-&gt;media_path = g_strdup_printf(&quot;%s/%s&quot;, status-&gt;mountpath, media_folder-&gt;media_folder);            if (media_folder-&gt;thumb_folder &amp;&amp; (!g_str_equal(media_folder-&gt;thumb_folder, &quot;&quot;)))                scanpath-&gt;thumb_path = g_strdup_printf(&quot;%s/%s&quot;, status-&gt;mountpath, media_folder-&gt;thumb_folder);            else                scanpath-&gt;thumb_path = NULL;            scanpath-&gt;db_media_folder = media_folder;            media_folder = media_folder-&gt;next;        &#125;        file_scan_run(status-&gt;mountpath);    &#125;    json_object_put(j_ret);&#125;\n\n看到自己贴了这么一大段代码其实挺愧疚的，其实如果你认真阅读下来，一点也不复杂。本来想删减一些不重要的东西，但是又感觉都挺重要的。每次写博客，写着写着，不知不觉就开始无情的贴代码，导致通篇代码比例占比特别高，挺烦躁的。为平衡一下代码和文字的比例，提高博文质量，这里画了一下最终会形成的一种结构：\n\n好了，storage manager维护的整体目录结构已经在我们的脑海中有了一个清晰的轮廓。\n回到文章开头的几个问题：管理录像文件名的数据库在啥时候创建的？当flow_muxer创建一个录像文件时，或者用户删除数据库时，storage manager这么得知这些事件的产生的？storage manager又是怎么知道要删除的录像文件名的？\n注意上面代码for循环中最后执行的file_scan_run函数，我可以明确的告诉你，所有的秘密都在scan_run函数当中：\nstatic void file_scan_run(char *path)&#123;    struct FileStatus *status;    status = g_hash_table_lookup(db_file_hash, path);    if (status != NULL &amp;&amp; status-&gt;run == 0) &#123;        char *dev = NULL;        char *type = NULL;        char *attributes = NULL;        if (checkdev(path, &amp;dev, &amp;type, &amp;attributes) == 0) &#123;    // 如果挂载点存在            /* ... */            status-&gt;dev = dev;            status-&gt;type = type;            status-&gt;attributes = attributes;            status-&gt;run = 1;            status-&gt;mount_status = 1;            LOG_DEBUG(&quot;befpre pthread: path: %s\\n&quot;, path);            pthread_create(&amp;status-&gt;scan_tid, NULL, file_scan_thread, status);        &#125;    &#125;&#125;\n\n继续跟file_scan_thread！（PS：以狂热而且热切并且迫不及待的语气）\nstatic void *file_scan_thread(void *arg)&#123;    char *db;    char *col_para = &quot;id INTEGER PRIMARY KEY AUTOINCREMENT,&quot; \\                     &quot;iPATHID INT DEFAULT 0,&quot; \\                     &quot;sNAME TEXT,&quot; \\                     &quot;sSIZE TEXT,&quot; \\                     &quot;iTIME INT&quot;;    struct FileStatus *status;    prctl(PR_SET_NAME, &quot;file_scan_thread&quot;, 0, 0, 0);    status = (struct FileStatus *)arg;    // 将挂载目录转换成数据库表名。    db = pathtodb(status-&gt;mountpath);    // 创建并清空数据库    g_free(rkdb_create(db, col_para));    g_free(rkdb_delete(db, NULL));    if (status-&gt;db)        g_free(status-&gt;db);    status-&gt;db = db;            // 表明记录到FileStatus中    // 初始化操作    checkdisksize(status-&gt;mountpath, &amp;status-&gt;total_size, &amp;status-&gt;free_size);    signal_update_disks_status(status-&gt;mountpath);    LOG_INFO(&quot;%s: mount path:%s\\n&quot;, __FUNCTION__, status-&gt;mountpath);    // 扫描挂载点当中所有文件，并将所有文件加入到数据库当中    scanfile(status);    if (status-&gt;run) &#123;        // 创建文件增加、删除事件监听线程，监视挂载点文件的修改，并同步到数据库当中        pthread_create(&amp;status-&gt;scan_monitor_tid, NULL, file_monitor_thread, status);        // 创建回滚线程，当挂载点文件数量或大小达到设定值就回滚删除最老的文件        pthread_create(&amp;status-&gt;delete_file_tid, NULL, delete_file_thread, status);    &#125;    /* ... */&#125;\n\n主要就是干了四件事：\n\n根据挂载点的名字创建一张表。\n\n进入挂载点目录，扫描里面的文件，将其同步到数据库表当中。\n\n创建文件增加、删除事件监听线程，监视挂载点文件的修改，并同步到数据库当中。\n\n创建回滚线程，当挂载点文件数量或大小达到设定值就回滚删除最老的文件。\n\n\n终于真相大白，下面只需要深入细节看看它们的实现。这里就只简单聊一下3、4的做法。\n对于第2件事，它的实现非常简单，就调用了linux几个操作目录的接口来扫描所有的文件，然后将文件添加到数据库。仅此而已。感兴趣的读者可以自行查看源码。\n对于第3监事，也即file_monitor_thread线程回调函数的实现如下：\nstatic void *file_monitor_thread(void *arg)&#123;    int fd;    int len;    int nread;    char buf[BUFSIZ];    struct inotify_event *event;    int i;    struct FileStatus *status;    status = (struct FileStatus *)arg;    // 创建notify套接字，然后添加需要监听的目录及事件    fd = inotify_init();    struct ScanPath *scanpath = status-&gt;scanpath;    while (scanpath) &#123;        scanpath-&gt;wd = inotify_add_watch(fd, scanpath-&gt;media_path, IN_CREATE | IN_MOVED_TO | IN_DELETE | IN_MOVED_FROM | IN_CLOSE_WRITE);        scanpath = scanpath-&gt;next;    &#125;    buf[sizeof(buf) - 1] = 0;    while ((len = read(fd, buf, sizeof(buf) - 1)) &gt; 0) &#123;        nread = 0;        while (len &gt; 0) &#123;            event = (struct inotify_event *)&amp;buf[nread];            for (i = 0; i &lt; EVENT_NUM; i++) &#123;                if ((event-&gt;mask &gt;&gt; i) &amp; 1) &#123;                    if (event-&gt;len &gt; 0) &#123;                        scanpath = status-&gt;scanpath;                        while (scanpath) &#123;                            if (event-&gt;wd == scanpath-&gt;wd) &#123;                                if (g_str_equal(event_str[i], &quot;IN_CREATE&quot;) || g_str_equal(event_str[i], &quot;IN_MOVED_TO&quot;)) &#123;                                    // 新建文件，同步到数据库                                    // ...                                 &#125; else if (g_str_equal(event_str[i], &quot;IN_DELETE&quot;) || g_str_equal(event_str[i], &quot;IN_MOVED_FROM&quot;)) &#123;                                    // 文件删除，同步到数据库                                    // ...                                &#125; else if (g_str_equal(event_str[i], &quot;IN_CLOSE_WRITE&quot;)) &#123;                                    // 写完更新数据库当中文件的大小                                    // ...                                 &#125;                                break;                            &#125;                            scanpath = scanpath-&gt;next;                        &#125;                    &#125;                &#125;            &#125;            nread = nread + sizeof(struct inotify_event) + event-&gt;len;            len = len - sizeof(struct inotify_event) - event-&gt;len;        &#125;    &#125;    pthread_detach(pthread_self());    pthread_exit(NULL);&#125;\n\n\ninotify是一种强大的、细粒度的、异步的文件系统事件监控机制，linux内核从2.6.13起，加入了inotify支持，通过Inotify可以监控文件系统添加、删除、移动、修改等各种事件，利用这个内核接口，第三方软件就可以监控文件系统下文件的各种变化情况。Inotify可用于检测单个文件，也可以检测整个目录。当检测的对象是一个目录的时候，目录本身和目录里的内容都会成为检测的对象。可以使用select、poll、epoll这些接口来监听，当有事件发生是，inotify文件描述符会可读。\n这里不得不感叹一下linux的强大，提供了这么优雅的接口。虽然相对整个storage manager是异步，但是直接开一个单独的线程去同步监听处理还是很暴力的。考虑到我们ipc项目存储相关的业务不是特别复杂，所以这里的用法还是可以接受的。\n最后就是回滚删除线程：\nstatic void *delete_file_thread(void *arg)&#123;\t  struct FileStatus *status = (struct FileStatus *)arg;    while (status-&gt;run) &#123;        int i;        long total = 0;        checkdisksize(status-&gt;mountpath, &amp;status-&gt;total_size, &amp;status-&gt;free_size);        total = status-&gt;free_size;        struct ScanPath *scanpath = status-&gt;scanpath;        size_t media_size = 0;        while (scanpath) &#123;            while ((scanpath-&gt;db_media_folder-&gt;maxnum &gt;= 0) &amp;&amp; (scanpath-&gt;num &gt; scanpath-&gt;db_media_folder-&gt;maxnum)) &#123;                struct OldFile *oldfile = get_old_file(status, scanpath);                // 如果文件数量超过限制，删除最老的文件                // ...            &#125;            scanpath = scanpath-&gt;next;        &#125;        if ((status-&gt;free_size &lt; (storage_config.freesize * 1024)) &amp;&amp; (total &gt; 0)) &#123;            scanpath = status-&gt;scanpath;            while (scanpath) &#123;                float duty = (float)scanpath-&gt;totalsize * 100 / total;                if (duty &gt; scanpath-&gt;db_media_folder-&gt;duty &amp;&amp; scanpath-&gt;db_media_folder-&gt;duty &gt;= 0) &#123;                    // 如果视频/图像文件大小占比超过设定值，删除最老的文件                    // ...                &#125;                scanpath = scanpath-&gt;next;            &#125;        &#125; else &#123;            usleep(2000000);        &#125;    &#125;&#125;\n\n所以从整体来看，storage manager会为每个（被checkdev探测到已存在的）挂载点都创建一个单独的目录事件监听线程和删除线程。\n一旦在挂载点的某个目录下新增&#x2F;删除某个文件，都会被file_monitor_thread线程检测到，此时file_monitor_thread线程就会将操作同步到数据库。当（视频&#x2F;图片）文件超过一定数量&#x2F;大小，都会被delete_file_thread线程注意到，此时delete_file_thread线程就会执行回滚操作，删除最老的文件。产生一种新文件覆盖老文件的现象。\n至此，storage manager核心功能、以及开头的问题都已解释清楚。\n\n本章完结\n","tags":["音视频"]},{"title":"MIT6.5840（原MIT6.824）Lab2总结（Raft）","url":"/2024/03/09/mit6.5840/raft/","content":"资源分享：\n官网地址：http://nil.csail.mit.edu/6.5840/2023/\nRaft论文地址：http://nil.csail.mit.edu/6.5840/2023/papers/raft-extended.pdf\n官方学生指导（又称官方避坑指导）：https://thesquareplanet.com/blog/students-guide-to-raft/\n总结：\n简单来说，Raft算法是：可以让一条数据备份到多台机器上，而在外部看来，好像只有一台机器的样子。 ，实验做完到现在，也过去了很久了，在这里只能模模糊糊还原当时遇到的一些印象比较深的BUG，千言万语，还是亲身体验一下这些坑，印象才会深刻。\n\n算法整体流程概述这里只对算法整体流程做一个总结。如果想从代码上实现一个Raft，请移步到Raft论文的Figure 2，许多非常精妙的细节还需按照论文中的描述一步一步去实现，要不然BUG真的满天飞。学生指导也是这么强调。\n首先引用论文中的Figure 4：\n\n算法涉及两个定时器： 选举定时器、心跳定时器。\n\n初始化状态： 所有节点处于Follower状态，选举定时器开启，心跳定时器关闭。\n\n作为Leader： 进行日志复制，将一条日志通过RPC，发送给其他节点，从而保持一致性。当然如果某一个节点落后太多，过早的日志被压缩了，Leader会给节点发送快照。拥有心跳定时器，超时就向所有其他节点发送心跳，给自己续命，防止其他节点发起选举，心跳也是一种特殊的日志。\n\n作为Follower： 接收Leader发来的日志和快照。处理其他节点投票的请求。拥有选举定时器，超时就转为Candidate，开始选举。\n\n作为Candidate： 给所有其他节点发送RequestVote RPC拉票。期间，如果获得大多数节点的投票就成为Leader。拥有选举定时器，超时就转为Candidate，重新选举。\n\n\n所有节点都会存在的逻辑： 意识到有新Leader已经诞生就变成Follower状态，当然之前如果是Leader状态，转变成Follower前需要开启选举定时器并关闭心跳定时器。 不断的向应用层Apply日志，直到首个未被提交的日志为止。\n细节描述 &amp; 踩坑记录我的Raft结构体大概长这个样子：\ntype Raft struct &#123;\tmu        sync.Mutex          // 全局锁\tpeers     []*labrpc.ClientEnd // 其他节点的rpc对象\tpersister *Persister          // 本端必要数据持久化的对象\tme        int                 // 本端在peers的索引\tdead      int32               // set by Kill()\t// Your data here (2A, 2B, 2C).\t// Look at the paper&#x27;s Figure 2 for a description of what\t// state a Raft server must maintain.\t// table map[uint]map[uint]uint\telectTimer     *time.Timer  \t// 选举超时定时器（Fllower和Candidate有效\theartbeatTimer *time.Timer  \t// 心跳超时定时器（发送心跳\tapplyCh        chan ApplyMsg    // raft层和应用层通信的chan\twakeupApply    chan interface&#123;&#125; // 唤醒后台协程去Apply日志\twakeupSnapshot chan ApplyMsg\t// 唤醒后台协程去Apply快照\tstate          uint //当前服务器扮演的角色\t// Persistent state on all servers:\tcurrentTerm      int32  \t\t// 当前节点的时期\tvotedFor         int32  \t\t// 当前节点将票投给了谁？\tlogs             []LogEntry \t// 产生的日志\tlastIncludeIndex int32  \t\t// 最后一条被压缩的日志的索引\tlastIncludeTerm  int32  \t\t// 最后一条被压缩的日志的时期\t// Volatile state on all servers:\tcommitIndex int32   \t\t\t// 提交成功的日志索引\tlastApplied int32   \t\t\t// 被上层应用成功的日志的索引\t// Volatile state on leaders:\tnextIndex  []int32  \t\t\t// Leader下一次应该从哪里发日志给对端\tmatchIndex []int32  \t\t\t// Leader对端当前和本段哪里匹配\tsnapshotCount int32 \t\t\t// 有几个协程正在安装快照，确保快照安装的原子性&#125;\n\n其中每个成员的作用，已经注明。\n1. 关于加锁-解锁-又加锁带来的问题这个BUG我并没有遇到，一开始写代码的时候就考虑到了这种情况，也算是迸现了一点点码感吧。只不过确实很容易踩坑，所以在这里记录一下。常听到的一把大锁保平安的缘由，就在这。在一个代码块中，如果间断性加解锁，新手很容易出现一些奇奇怪怪的线程安全问题。举个例子，有如下逻辑：\nfunc (data *Data) Task() &#123;\tdata.mu.Lock()\t// 临界区1\t// 对data做一些条件判断\tif !data.has&#123;\t\treturn\t&#125;\t// ...\tdata.mu.Unlock()\tDoLongTimeWork()\tdata.mu.Lock()\t// 临界区2\t// 因为data.hash == true 所以做一些后续处理。&#125;\n\n这段间断加锁的代码看似没什么逻辑问题，但是在多线程（协程）情况下，临界区1做的条件判断，对于临界区2是无效的，临界区2做的处理不能依赖临界区1的判断。因为临界区1到临界区2是有一段未持锁的区间的。在该区间，可能发生切换，导致有其他协程对data的成员发生了更改（data.has被置为false），从而导致之前临界区2做出错误的处理。要修复这个BUG就需要在临界区2上锁后，再进行一次判断（“冗余”判断），如下：\nfunc (data *Data) Task() &#123;\tdata.mu.Lock()\t// 临界区1\t// 对data做一些条件判断\tif !data.has&#123;\t\treturn\t&#125;\t// ...\tdata.mu.Unlock()\tDoLongTimeWork()\tdata.mu.Lock()\t// 临界区2\tif !data.has &#123;\t\treturn\t&#125;\t// 因为data.hash == true 所以做一些后续处理。&#125;\n\n这是编写多线程程序非常容易出现的一个问题。在MIT6.824的Raft实现中很多地方的需求是必须使用阶段性加锁的逻辑，比如Leader进行日志复制时，需要解锁调用RPC。RPC调用完成后，再加锁，需要检查reqArgs.Term是否和节点当前的Term一致，不一致需要直接返回，因为节点只能处理同一Term发送的RPC请求。 从直觉上，也应该这么做。学生指导好像也提到过这个问题。\n于此同时，我做实验碰到的一个相关的低级BUG是，调用RPC后没有接它的返回值去判断RPC调用成功了没有！！！ 这点一定要注意。\n2. 关于index等于0的日志的含义的抽象。在实现lab2的 Part 2D: log compaction (hard)前，需要保证index为0的日志是Term为0的“守护”日志，logs数组初始化长度为1。这样做的原因是防止有节点落后Leader太多，所有日志都和Leader不匹配，从而回退到了index为1的日志，此时我们定义的index为0的日志作用就生效了，因为每一个节点的index为0的日志其index和Term一定是一样的，保证了日志复制在index为1时绝对的成功性。\n在实现lab2的 Part 2D: log compaction (hard)后，由于我的实现是利用了logs[0]的日志。而在系统刚启动时，对于index为0的日志我将其抽象成了lastIncludeIndex为0、lastIncludeTerm也为0的默认已经被压缩过的日志，当Leader需要向其他节点复制index为1的日志时，也保证了其绝对的成功性。\n3. 选举定时器的重置时机的讲究做实验前，本人就没有好好的去阅读官方的避坑指南，所以在这里就栽了跟头。导致了莫名奇妙的活锁。\n错误复现： 最开始的实现是，除了选举时、成功收到日志时会重置选举定时器外，一旦节点意识到有新Leader产生，我都会将选举定时器重置。\n正确实现： 查阅官方避坑指南后，重置定时器有三个时期：\n\n从当前Leader那里收到AppendEntries RPC（本端Term和arg中的Term一定要一致！）。\n\n开始一次选举\n\n被请求投票时，同意将票投给对方。\n\n\n此外还要注意，保证定时器类型的正确性。当转换成Leader时，要开启心跳定时器 &amp;&amp; 关闭选举定时器。当转换成Follower或者Candidate时，要关闭心跳定时器 &amp;&amp; 开启选举定时器！在Candidate选举失败时，注意找时机回到Follower状态！\n4. 别把心跳不当日志，当然，正常的日志也能当成是一次心跳！关于心跳定时器的重置时期：每次发送日志都可以重置一下心跳定时器。\n当对某个节点已经达成一致，没有可复制的日志时，实现中还是要发一条Entries为0的日志。我的实现中，利用go语言的for实现了一种do while的结构，如下：\nfor&#123;\t// 做一些工作\t// ...\tif 条件不满足 &#123;\t\tbreak\t&#125;&#125;\n\n5. 日志回退加速的优化Raft论文中，认为日志回退加速的优化是没有必要的，因为在实际中，逐步回退完全够用。但是MIT6.824要求实现这一优化。\n根据官方避坑指导，应该这样做：\n在AppendEntries RPC的reply中加上两个字段：conflictIndex、conflictTerm 。\n对于AppendEntries RPC的接收方\n\n如果prevLogIndex 不在logs的表示的范围内，就将conflictIndex置为最后一条日志的index + 1，并且conflictTerm为non。\n\n如果prevLogIndex 在logs的表示的范围内，但是prevLogTerm对不上，conflictTerm置为本端索引为prevLogIndex的日志的Term，conflictIndex置为Term为conflictTerm的第一个日志的索引。（当然要保证conflictIndex &gt; rf.lastIncludeIndex）\n\n\n对于AppendEntries RPC的发送方\n\n如果接收方的logs中有可能找到Term为conflictTerm的日志，将相应的next置为最后一个Term为conflictTerm的日志的index + 1\n\n否则，说明既然当前作为Leader的我没有该Term，你Follower就别保留和该Term的日志了，直接将相应的next置为conflictIndex即可。\n\n\n官方避坑指南说，可以只实现conflictIndex，我为了偷懒，就是只实现了conflictIndex，最后也能稳定通过测试。\n6. 快照Apply的原子性这个BUG是我在做lab4时发现的。因为应用层偶尔会出现，日志回退导致出现，except index is n, but is n - 10，的情况，经过痛苦的查看日志。最后发现Follower处理InstallSnapshot RPC的逻辑是：\n\nrf.mu.Lock()\n\n根据快照修改raft层的成员数据\n\nrf.mu.Unlock()\n\n通知后台向应用层Apply快照。\n\n\n这里1、3步骤不是连续的，导致在应用层安装快照前，Raft层有其他协程修改了1中相关的数据成员，就造成了不一致。解决办法是：Raft中增加一个快照计数器，在0到2之间对计数器增1。在其他可能修改1中相关数据成员的地方，在修改前，判断计数器是否为0，不为零就放弃更改。\n7. 对Figure 8的深入理解关于Figure 8要表达的东西，在这篇文章中讲解的非常清楚了：https://zhuanlan.zhihu.com/p/369989974\n论文Figure 2中的右下角中：\n\nIf there exists an N such that N &gt; commitIndex, a majority of matchIndex[i] ≥ N, and log[N].term &#x3D;&#x3D; currentTerm: set commitIndex &#x3D; N (§5.3, §5.4)\n\n加粗的部分的判断非常精妙！也非常必要！这里的意思要求Leader不能直接提交以前任期的日志，只能通过当前任期的日志来间接提交以前任期的日志\n这里简单总结一下：\n\n只有拥有最新的日志的Candidate才能当选Leader。\n\nLeader不能提交以前任期的日志，只能间接提交，否者根据Figure 8的情况，会出现日志回滚覆盖，导致同一index的日志，重复提交了两次的危险情况。不让提交以前任期的日志能保证即使覆盖了以前的被复制到了大多数节点的日志也没有关系，因为没有提交过。\n\n需要在Leader当选时，发送一条no-op 日志（区别于心跳的空日志，这里的日志会被追加到logs中，但上层执行该日志时，不会做任何操作），这个操作保证了让Raft能够迅速间接提交以前的日志。etcd 中有实现这个。\n\n\n8. 死锁避免这里列举一个Raft常见的死锁，虽然课程官方有提到：Raft层在向上层通过applyCh提交日志或快照时，不要占着Raft的锁，因为上层在处理日志时，也会请求Raft的锁。当applyCh满时，会导致Raft层占锁阻塞等待上层去处理日志，而上层处理日志又需要Raft的这把锁，导致日志一直无法被处理，从而造成死锁。\n\n本章完结\n","tags":["存储"]},{"title":"muduo源码阅读笔记（4、异步日志）","url":"/2024/01/13/muduo/AsyncLogging/","content":"Muduo源码笔记系列：\nmuduo源码阅读笔记（0、下载编译muduo）\nmuduo源码阅读笔记（1、同步日志）\nmuduo源码阅读笔记（2、对C语言原生的线程安全以及同步的API的封装）\nmuduo源码阅读笔记（3、线程和线程池的封装）\nmuduo源码阅读笔记（4、异步日志）\nmuduo源码阅读笔记（5、Channel和Poller）\nmuduo源码阅读笔记（6、ExevntLoop和Thread）\nmuduo源码阅读笔记（7、EventLoopThreadPool）\nmuduo源码阅读笔记（8、定时器TimerQueue）\nmuduo源码阅读笔记（9、TcpServer）\nmuduo源码阅读笔记（10、TcpConnection）\n前言\nMuduo的异步日志支持：异步日志输出，日志回滚。本文重点在异步日志输出，主要集中在AsyncLogging.cc文件的实现，至于日志回滚，属于文件IO管理范畴，不会细讲，这部分的代码主要集中在LogFile.cc、FileUtil.cc文件，代码量也不大，感兴趣的读者可以自行深入阅读。\n正文Muduo异步日志的实现是典型的多对一的，多生产者，单消费者模型。简单来说就是程序中的多个线程（前台线程）产生日志，然后由一个日志同步线程（后台线程）消化日志，将日志同步给磁盘。\n术语纠正\n阅读过Muduo源码的朋友都应该知道，Muduo实现的异步日志是双缓冲的，但是我阅读过很多Muduo有关异步日志的博客，有的人说Muduo里面双缓冲指的是currentBuffer_和nextBuffer_两块缓存，也有人说，Muduo的双缓冲是指前台的缓存和后台线程的缓存。\n查阅资料得知：\n定义： 双缓冲技术是一种通过使用两个缓冲区（buffers）来实现某种功能的技术。通常，这两个缓冲区会交替使用，一个用于写入数据，另一个用于读取数据，或者在某种操作完成后进行交换。\n在不同的领域中，双缓冲技术有不同的应用，以下是一些常见的应用场景：\n\n图形学： 在图形学中，双缓冲技术通常用于解决图像闪烁的问题。一个缓冲区用于显示当前图像，而另一个缓冲区则用于在后台绘制下一帧图像。当绘制完成后，两个缓冲区进行交换，确保只显示完整的图像，从而避免了闪烁。\n\n\n计算机图形渲染： 在图形渲染中，双缓冲技术可以用于提高性能。例如，使用一个缓冲区进行渲染，同时在另一个缓冲区进行后台计算或处理，然后交换缓冲区。这样可以实现更平滑的图形渲染效果。\n\n日志系统： 在日志系统中，双缓冲技术可以用于异步日志的实现。一个缓冲区用于应用程序写入日志，而另一个缓冲区用于异步写入磁盘。当一个缓冲区满了之后，可以交换缓冲区，实现高效的异步日志记录。\n\n网络传输： 在网络传输中，双缓冲技术可以用于提高数据传输的效率。一个缓冲区用于发送数据，而另一个缓冲区用于填充新的数据。这样，在一个缓冲区发送的同时，可以在另一个缓冲区准备下一批数据。\n\n\n综上，Muduo异步日志的双缓冲就是指的前台的缓存和后台线程的缓存。\n实现提供的接口：\nclass AsyncLogging : noncopyable&#123;public:    AsyncLogging(const string&amp; basename,                off_t rollSize,                int flushInterval = 3);    ~AsyncLogging()&#123;        if (running_)&#123;            stop();        &#125;    &#125;    void append(const char* logline, int len);    void start()&#123;        running_ = true;        thread_.start();    // 启动线程        latch_.wait();  // 等待启动    &#125;    void stop() NO_THREAD_SAFETY_ANALYSIS&#123;        running_ = false;        cond_.notify();        thread_.join();    &#125;private:    void threadFunc();    typedef muduo::detail::FixedBuffer&lt;muduo::detail::kLargeBuffer&gt; Buffer;   // 4M的缓存    typedef std::vector&lt;std::unique_ptr&lt;Buffer&gt;&gt; BufferVector;  // 使用独占智能指针管理    typedef BufferVector::value_type BufferPtr;    const int flushInterval_; // 将日志同步到磁盘上的间隔    std::atomic&lt;bool&gt; running_; // 后台同步线程正在运行？    const string basename_; // 文件前缀名    const off_t rollSize_;  // 日志文件回滚大小    muduo::Thread thread_;  // 后台日志同步线程    muduo::CountDownLatch latch_; // 确保同步回调函数跑起来了    muduo::MutexLock mutex_;    muduo::Condition cond_ GUARDED_BY(mutex_);  // 等待有至少一个Buffer满了    BufferPtr currentBuffer_ GUARDED_BY(mutex_);  // 接收日志的buff    BufferPtr nextBuffer_ GUARDED_BY(mutex_); // 替补的buff，为减少分配内存的开销    BufferVector buffers_ GUARDED_BY(mutex_); // 满载日志的buff队列&#125;;\n\n异步日志架构图：\n简单画了一下Muduo异步日志的架构图，忽略了currentBuffer_、nextBuffer_等细节。如图所示。\n\n实现的伪代码：\n/** 构造函数就省略了，主要做的事：* 1. 将AsyncLogging::threadFunc作为后台线程的回调。* 2. 构造currentBuffer_和nextBuffer_。*/// ...void AsyncLogging::append(const char* logline, int len)&#123;    muduo::MutexLockGuard lock(mutex_); // 持锁    if (currentBuffer_-&gt;avail() &gt; len)&#123;  //当前缓存没满        currentBuffer_-&gt;append(logline, len); // 先写到前台缓存    &#125;else&#123; //满了        buffers_.push_back(std::move(currentBuffer_));  // 放到缓存队列        if (nextBuffer_)&#123;  // 优先考虑替补缓存，没有分配内存的开销，性能更高。            currentBuffer_ = std::move(nextBuffer_);        &#125;else &#123; // 最坏的情况，替补缓存也用光了，只能去申请内存。            currentBuffer_.reset(new Buffer); // Rarely happens        &#125;        currentBuffer_-&gt;append(logline, len);        cond_.notify(); // 提醒一下后台线程，前台缓存队列非空。    &#125;&#125;void AsyncLogging::threadFunc()&#123;    assert(running_ == true);    latch_.countDown();    LogFile output(basename_, rollSize_, false);  // 日志文件管理类，实现了日志回滚。    BufferPtr newBuffer1(new Buffer); // 提前缓存两块buff减少内存开销    BufferPtr newBuffer2(new Buffer);    newBuffer1-&gt;bzero();    newBuffer2-&gt;bzero();    BufferVector buffersToWrite;    buffersToWrite.reserve(16);    while (running_)&#123;        // 后台线程的buff在向前台buff置换日志前，保证后台线程的buff是被清空的。        assert(newBuffer1 &amp;&amp; newBuffer1-&gt;length() == 0);        assert(newBuffer2 &amp;&amp; newBuffer2-&gt;length() == 0);        assert(buffersToWrite.empty());        &#123;            muduo::MutexLockGuard lock(mutex_);            if (buffers_.empty())&#123;  // unusual usage!                /*                * 1. 因前台日志队列非空而被唤醒。（AsyncLogging::append）                * 2. 超时。                */                cond_.waitForSeconds(flushInterval_);            &#125;            // 不管currentBuffer_中是否有日志，都归入前台的日志队列。可以保证所有日志都归入到了前台的日志队列。            buffers_.push_back(std::move(currentBuffer_));             currentBuffer_ = std::move(newBuffer1); // 补充currentBuffer_            buffersToWrite.swap(buffers_);  // 将前台的日志队列，置换到后台的日志队列，置换后前台的日志队列被清空            if (!nextBuffer_)&#123; // 前台的nextBuffer_也被用了。                // nextBuffer_也需要补充                nextBuffer_ = std::move(newBuffer2);            &#125;        &#125;        assert(!buffersToWrite.empty());  // 至少为1        if (buffersToWrite.size() &gt; 25)&#123; // 避免日志洪流            char buf[256];            snprintf(buf, sizeof buf, &quot;Dropped log messages at %s, %zd larger buffers\\n&quot;,                    Timestamp::now().toFormattedString().c_str(),                    buffersToWrite.size()-2);            fputs(buf, stderr);            output.append(buf, static_cast&lt;int&gt;(strlen(buf)));            buffersToWrite.erase(buffersToWrite.begin()+2, buffersToWrite.end());        &#125;        for (const auto&amp; buffer : buffersToWrite)&#123; // 日志落盘            // FIXME: use unbuffered stdio FILE ? or use ::writev ?            output.append(buffer-&gt;data(), buffer-&gt;length());        &#125;        if (buffersToWrite.size() &gt; 2)&#123; // 保留两块buff，填充newBuffer1、newBuffer2            // drop non-bzero-ed buffers, avoid trashing            buffersToWrite.resize(2);        &#125;        if (!newBuffer1)&#123;            assert(!buffersToWrite.empty());            newBuffer1 = std::move(buffersToWrite.back());            buffersToWrite.pop_back();            newBuffer1-&gt;reset();        &#125;        if (!newBuffer2)&#123;            assert(!buffersToWrite.empty());            newBuffer2 = std::move(buffersToWrite.back());            buffersToWrite.pop_back();            newBuffer2-&gt;reset();        &#125;        buffersToWrite.clear(); // 清空后台日志队列。        output.flush(); // 文件同步。    &#125;    output.flush();&#125;\n\n套路总结\n这里是一份经典的降低锁的粒度的套路模板，也是Muduo异步日志高效的核心：\n// 临界区外面处理临界区的数据// 定义临界区的数据结构struct CriticalData &#123;    // 数据成员...&#125;;void processCriticalData(const CriticalData&amp; data) &#123;    // 在临界区外面处理数据    // 可以进行计算、拷贝、异步处理等操作&#125;// 在临界区内使用局部变量进行处理void criticalSection(const CriticalData&amp; sharedData) &#123;    CriticalData localCopy    // 进入临界区    // ...    // 拷贝数据到局部变量    localCopy.swap(sharedData);    // 离开临界区    // ...    // 在临界区外面处理局部变量    processCriticalData(localCopy);&#125;\n\n细节明细：疑问：\nMuduo的异步日志为什么日志过多就删除一些日志？\n解答：\n在Muduo库的异步日志系统中，删除一些日志的策略可能是为了防止日志积累过多导致系统资源消耗过大，以及为了保持日志的存储大小在可控范围内。具体的删除策略可能会根据应用程序的需求和性能考虑而定，通常包括以下一些原因：\n\n资源限制： 大量的日志可能会占用大量磁盘空间，特别是在长时间运行的系统中。删除一些日志可以确保系统的磁盘空间不被过多消耗，避免磁盘空间不足的问题。\n\n性能考虑： 当日志量非常大时，写入和处理大量日志会对系统性能产生影响。删除一些日志可以降低写入和处理的负担，确保系统的性能得到维持。\n\n避免日志洪流： 在某些情况下，产生大量的日志可能并不是问题的根本原因，而是问题的表征。删除一些日志可以帮助集中关注真正的问题，而不被大量的无关日志所干扰。\n\n\n疑问：\nMuduo异步日志中nextBuffer_是不是冗余了？currentBuffer_满了的话，不是可以再new一个Buffer吗？为什么要额外提供一个nextBuffer_？\n解答：\n 预先分配并循环使用多个缓冲区（包括 nextBuffer_）可以减少内存分配的频率。如果只使用一个 currentBuffer_，每次都需要在 currentBuffer_ 满了之后重新分配一个新的缓冲区，这可能导致频繁的内存分配和释放操作，影响性能。\n 综上提供nextBuffer_主要是为了减少内存分配次数\n\n本章完结\n","tags":["高性能服务器框架"]},{"title":"muduo源码阅读笔记（5、Channel和Poller）","url":"/2024/01/14/muduo/ChannelAndPoller/","content":"Muduo源码笔记系列：\nmuduo源码阅读笔记（0、下载编译muduo）\nmuduo源码阅读笔记（1、同步日志）\nmuduo源码阅读笔记（2、对C语言原生的线程安全以及同步的API的封装）\nmuduo源码阅读笔记（3、线程和线程池的封装）\nmuduo源码阅读笔记（4、异步日志）\nmuduo源码阅读笔记（5、Channel和Poller）\nmuduo源码阅读笔记（6、ExevntLoop和Thread）\nmuduo源码阅读笔记（7、EventLoopThreadPool）\nmuduo源码阅读笔记（8、定时器TimerQueue）\nmuduo源码阅读笔记（9、TcpServer）\nmuduo源码阅读笔记（10、TcpConnection）\n前言\n简单讲，Channel就是对文件描述符（fd）的封装，进行事件管理，将fd和对其操作的回调封装在一起，方便，在fd上有IO事件到来时，利用相应的回调来处理IO事件；Poller就是对Linux下各种IO多路复用进行抽象，提供一个统一的接口，该类是一个虚基类。路径.&#x2F;net&#x2F;poller中的源码，就是对Poller的实现，包括：EPollPoller、PollPoller。\n这部分源代码很朴实易懂，代码量也不大，建议读者，亲自看看源码。\nChannel的实现提供的接口：\n\nclass Channel : noncopyable&#123;public:    typedef std::function&lt;void()&gt; EventCallback;    typedef std::function&lt;void(Timestamp)&gt; ReadEventCallback;    Channel(EventLoop* loop, int fd);    ~Channel();    void handleEvent(Timestamp receiveTime);    void setReadCallback(ReadEventCallback cb)    &#123; readCallback_ = std::move(cb); &#125;    void setWriteCallback(EventCallback cb)    &#123; writeCallback_ = std::move(cb); &#125;    void setCloseCallback(EventCallback cb)    &#123; closeCallback_ = std::move(cb); &#125;    void setErrorCallback(EventCallback cb)    &#123; errorCallback_ = std::move(cb); &#125;    /// Tie this channel to the owner object managed by shared_ptr,    /// prevent the owner object being destroyed in handleEvent.    void tie(const std::shared_ptr&lt;void&gt;&amp;);    int fd() const &#123; return fd_; &#125;    int events() const &#123; return events_; &#125;    void set_revents(int revt) &#123; revents_ = revt; &#125; // used by pollers    // int revents() const &#123; return revents_; &#125;    bool isNoneEvent() const &#123; return events_ == kNoneEvent; &#125;    void enableReading() &#123; events_ |= kReadEvent; update(); &#125;    void disableReading() &#123; events_ &amp;= ~kReadEvent; update(); &#125;    void enableWriting() &#123; events_ |= kWriteEvent; update(); &#125;    void disableWriting() &#123; events_ &amp;= ~kWriteEvent; update(); &#125;    void disableAll() &#123; events_ = kNoneEvent; update(); &#125;    bool isWriting() const &#123; return events_ &amp; kWriteEvent; &#125;    bool isReading() const &#123; return events_ &amp; kReadEvent; &#125;    // for Poller    int index() &#123; return index_; &#125;    void set_index(int idx) &#123; index_ = idx; &#125;    // for debug    string reventsToString() const;    string eventsToString() const;    void doNotLogHup() &#123; logHup_ = false; &#125;    EventLoop* ownerLoop() &#123; return loop_; &#125;    void remove();private:    static string eventsToString(int fd, int ev);    void update();    void handleEventWithGuard(Timestamp receiveTime);    static const int kNoneEvent;  // 0    static const int kReadEvent;  // POLLIN | POLLPRI    static const int kWriteEvent; // POLLOUT    EventLoop* loop_; // channel被那个EventLoop监听    const int  fd_; // fd    int        events_;    int        revents_; // it&#x27;s the received event types of epoll or poll // 触发的事件    int        index_; // used by Poller.    bool       logHup_;    std::weak_ptr&lt;void&gt; tie_;   // 主要解决循环引用的问题    bool tied_;    bool eventHandling_;  // 正在handleEventWithGuard中处理事件    bool addedToLoop_;  // 被添加到EventLoop了没？    ReadEventCallback readCallback_;  // 读回调    EventCallback writeCallback_; // 写回调    EventCallback closeCallback_; // 连接断开回调    EventCallback errorCallback_; // 错误处理回调&#125;;\n\n实现的伪代码：\nChannel::Channel(EventLoop* loop, int fd__)  : loop_(loop),    fd_(fd__),    events_(0),    revents_(0),    index_(-1),    logHup_(true),    tied_(false),    eventHandling_(false),    addedToLoop_(false)&#123;&#125;Channel::~Channel()&#123;    assert(!eventHandling_);    assert(!addedToLoop_);    if (loop_-&gt;isInLoopThread())&#123;        // 该channel即将销毁，不能被EventLoop监听        assert(!loop_-&gt;hasChannel(this));    &#125;&#125;void Channel::tie(const std::shared_ptr&lt;void&gt;&amp; obj)&#123;    tie_ = obj;   // 循环依赖，做生命周期的绑定    tied_ = true;&#125;void Channel::update()&#123;    // 在EventLoop中进行fd的事件更新    addedToLoop_ = true;    loop_-&gt;updateChannel(this);&#125;void Channel::remove()&#123;    // 取消EventLoop对channel的监听    assert(isNoneEvent());    addedToLoop_ = false;    loop_-&gt;removeChannel(this);&#125;void Channel::handleEvent(Timestamp receiveTime)&#123;    std::shared_ptr&lt;void&gt; guard;    if (tied_)&#123;        guard = tie_.lock();        if (guard)&#123; // 保证依赖对象没有被释放            handleEventWithGuard(receiveTime);        &#125;    &#125;else&#123;        handleEventWithGuard(receiveTime);    &#125;&#125;void Channel::handleEventWithGuard(Timestamp receiveTime)&#123;    eventHandling_ = true;    if ((revents_ &amp; POLLHUP) &amp;&amp; !(revents_ &amp; POLLIN))&#123;        // 连接断开，并且fd上没有可读数据（默认水平触发）        // 调用关闭回调        if (closeCallback_) closeCallback_();    &#125;    if (revents_ &amp; (POLLERR | POLLNVAL))&#123;        // 错误处理        if (errorCallback_) errorCallback_();    &#125;    if (revents_ &amp; (POLLIN | POLLPRI | POLLRDHUP))&#123;        // 可读        if (readCallback_) readCallback_(receiveTime);    &#125;    if (revents_ &amp; POLLOUT)&#123;        // 可写        if (writeCallback_) writeCallback_();    &#125;    eventHandling_ = false;&#125;\n\n注意\n智能指针不是万能的，并不能解决cpp所有内存泄露的问题！！！\n这里多提一句，关于循环引用这种头疼的问题，几个类可能很容易看出来，但是如果项目太大，涉及的类太多，就很容出现循环引用导致内存泄漏的问题！而且难以察觉。所以，在类的设计上，要特别注意这个坑。\n自引用：\nclass Node;class Node &#123;public:    std::shared_ptr&lt;Node&gt; next;    Node() &#123;        std::cout &lt;&lt; &quot;Node constructed&quot; &lt;&lt; std::endl;    &#125;    ~Node() &#123;        std::cout &lt;&lt; &quot;Node destructed&quot; &lt;&lt; std::endl;    &#125;&#125;;int main() &#123;    std::shared_ptr&lt;Node&gt; node1 = std::make_shared&lt;Node&gt;();    node1-&gt;next = node1;    return 0;&#125;\n\n结果如下：\n[root@localhost muduo]# g++ -Wall -std=c++11 -o test.bin test.cc [root@localhost muduo]# [root@localhost muduo]# [root@localhost muduo]# ./test.bin Node constructed[root@localhost muduo]#\n\n多个类间的循环引用是同理的，一般是靠weak_ptr解决，单个类的自引用的化话，目前无解。\n细节明细：疑问：\nMuduo中Channel的成员变量std::weak_ptr&lt;void&gt; tie_有什么意义？\n解答：\n使用std::weak_ptr的目的是为了避免循环引用（circular reference），因为TcpConnection对象通常也会持有一个指向Channel的指针。通过使用std::weak_ptr，可以避免引发循环引用导致对象无法正确释放的问题。\n疑问：\nfd上何时返回POLLHUP又何时返回POLLRDHUP？（POLLIN&#x2F;POLLOUT呢？）\n解答：\n有写过一个deamo（有时间再写这方面的博客）专门测试这些事件触发条件，实验表明，POLLHUP事件一般是系统默认添加的事件，在连接关闭时会触发，而POLLIN&#x2F;POLLOUT&#x2F;POLLRDHUP等，需要用户手动添加才会触发。\n\n\n\n事件\n水平触发\n边沿触发\n\n\n\nPOLLIN\n接收缓存有数据就一直触发\n接收缓存有新数据来就触发\n\n\nPOLLOUT\n发送缓存未满就一致触发\n发送数据就触发\n\n\nPOLLRDHUP\n对端写关闭，本端就触发，同时触发POLLIN\n同水平\n\n\nPOLLHUP\n连接关闭，同时触发POLLIN&#x2F;POLLOUT事件（用户添加过什么事件就触发什么事件）\n同水平\n\n\nPoller的实现简单提一下，在Muduo中，Poller是对原生的linux下，C语言的IO多路复用接口进行了封装，毕竟面向对象用起来更舒服。Poller会在EventLoop中使用。具体，怎么使用，在下一章节，讲到EventLoop时，才会有所领悟，这里可以所见即所得，知道Poller就是封装IO多路复用的即可。\nPoller提供的接口：\nclass Poller : noncopyable&#123;public:    typedef std::vector&lt;Channel*&gt; ChannelList;    Poller(EventLoop* loop);    virtual ~Poller();    /// Polls the I/O events.    /// Must be called in the loop thread.    virtual Timestamp poll(int timeoutMs, ChannelList* activeChannels) = 0;    /// Changes the interested I/O events.    /// Must be called in the loop thread.    virtual void updateChannel(Channel* channel) = 0;    /// Remove the channel, when it destructs.    /// Must be called in the loop thread.    virtual void removeChannel(Channel* channel) = 0;    virtual bool hasChannel(Channel* channel) const;    static Poller* newDefaultPoller(EventLoop* loop);    void assertInLoopThread() const&#123;        ownerLoop_-&gt;assertInLoopThread();    &#125;protected:    typedef std::map&lt;int, Channel*&gt; ChannelMap;     ChannelMap channels_;// fd到channel的映射    private:    EventLoop* ownerLoop_;  // 所属的EventLoop&#125;;\n\nPoller实现的伪代码：\nPoller::Poller(EventLoop* loop)  : ownerLoop_(loop)&#123;&#125;Poller::~Poller() = default;bool Poller::hasChannel(Channel* channel) const&#123;    assertInLoopThread();    ChannelMap::const_iterator it = channels_.find(channel-&gt;fd());    return it != channels_.end() &amp;&amp; it-&gt;second == channel;&#125;\n\n因为Muduo继承Poller实现了EPollPoller、PollPoller，考虑到篇幅有限，这里讲一下PollPoller的实现吧。\nPollPoller的实现在muduo网络库中，PollPoller 是对 poll() 系统调用的封装，用于实现事件循环（EventLoop）中的事件分发。PollPoller 负责将 Channel管理的文件描述符注册到 poll() 中，监听各个文件描述符的事件。\n以下是对 PollPoller 的简要介绍：\n\n文件位置： PollPoller 类的实现通常位于 PollPoller.cc 文件中。\n\n继承关系： PollPoller 类继承自 Poller 类，而 Poller 类是对事件轮询机制的抽象。\n\n主要方法： 重要的方法包括 poll 和 fillActiveChannels。\n\npoll 方法负责调用 poll() 系统调用，等待事件发生。\nfillActiveChannels 方法用于将 poll() 返回的就绪事件填充到 activeChannels 中。\n\n\n事件分发： PollPoller 通过 EventLoop 实现了事件的分发。当有事件发生时，PollPoller 会通知 EventLoop，而后 EventLoop 会调用相应的 Channel 的handleEvent进行事件处理。\n\n\n提供的接口\nclass PollPoller : public Poller&#123;public:    PollPoller(EventLoop* loop);    ~PollPoller() override;    Timestamp poll(int timeoutMs, ChannelList* activeChannels) override;    void updateChannel(Channel* channel) override;    void removeChannel(Channel* channel) override;private:    void fillActiveChannels(int numEvents,                            ChannelList* activeChannels) const;    /*    * struct pollfd定义如下：    *    struct pollfd &#123;    *        int   fd;     // 监听的文件描述符        *       short events;   // 要监听的事件    *        short revents; // 监听到得到事件    *    &#125;;    */    typedef std::vector&lt;struct pollfd&gt; PollFdList;    PollFdList pollfds_;&#125;;\n\n实现的伪代码\nPollPoller::PollPoller(EventLoop* loop)  : Poller(loop)&#123;&#125;PollPoller::~PollPoller() = default;// 提供给EventLoop的接口，也是EventLoop 等待事件的程序点。Timestamp PollPoller::poll(int timeoutMs, ChannelList* activeChannels)&#123;    // 等待事件到来，或者超时    // XXX pollfds_ shouldn&#x27;t change    int numEvents = ::poll(&amp;*pollfds_.begin(), pollfds_.size(), timeoutMs);    int savedErrno = errno;    Timestamp now(Timestamp::now());    if (numEvents &gt; 0)&#123;        fillActiveChannels(numEvents, activeChannels);    &#125;else if (numEvents == 0)&#123;        LOG_TRACE &lt;&lt; &quot; nothing happened&quot;;    &#125;else&#123;        if (savedErrno != EINTR)&#123;   // 中断            errno = savedErrno;            LOG_SYSERR &lt;&lt; &quot;PollPoller::poll()&quot;;        &#125;    &#125;    return now;&#125;void PollPoller::fillActiveChannels(int numEvents,                                    ChannelList* activeChannels) const&#123;    // 将所有发生事件的fd对应的channel，收集到activeChannels，供EventLoop处理    for (PollFdList::const_iterator pfd = pollfds_.begin();        pfd != pollfds_.end() &amp;&amp; numEvents &gt; 0; ++pfd)&#123;        if (pfd-&gt;revents &gt; 0)&#123;            --numEvents;            ChannelMap::const_iterator ch = channels_.find(pfd-&gt;fd);            assert(ch != channels_.end());   //一定要存在            Channel* channel = ch-&gt;second;            channel-&gt;set_revents(pfd-&gt;revents);            // pfd-&gt;revents = 0;    // poll会自动清零            activeChannels-&gt;push_back(channel);        &#125;    &#125;&#125;void PollPoller::updateChannel(Channel* channel)&#123;    Poller::assertInLoopThread();    if (channel-&gt;index() &lt; 0)&#123;        // 新的channel        // a new one, add to pollfds_        assert(channels_.find(channel-&gt;fd()) == channels_.end());        struct pollfd pfd;        pfd.fd = channel-&gt;fd();        pfd.events = static_cast&lt;short&gt;(channel-&gt;events());        pfd.revents = 0;        pollfds_.push_back(pfd);        int idx = static_cast&lt;int&gt;(pollfds_.size())-1;        channel-&gt;set_index(idx);        channels_[pfd.fd] = channel;    &#125;else&#123;        // update existing one        assert(channels_.find(channel-&gt;fd()) != channels_.end());        int idx = channel-&gt;index();        // idx 有效        assert(0 &lt;= idx &amp;&amp; idx &lt; static_cast&lt;int&gt;(pollfds_.size()));        struct pollfd&amp; pfd = pollfds_[idx];        // fd对的上，或者，因为之前不需要监听，被置为-(fd + 1)保证为负，这样，poll不会监听fd为负的channel。        assert(pfd.fd == channel-&gt;fd() || pfd.fd == -channel-&gt;fd()-1);        pfd.fd = channel-&gt;fd();        pfd.events = static_cast&lt;short&gt;(channel-&gt;events());        pfd.revents = 0;        if (channel-&gt;isNoneEvent())&#123;            // 事件为空，置为负。            // ignore this pollfd            pfd.fd = -channel-&gt;fd()-1;        &#125;    &#125;&#125;void PollPoller::removeChannel(Channel* channel)&#123;    Poller::assertInLoopThread();    assert(channels_.find(channel-&gt;fd()) != channels_.end());    assert(channels_[channel-&gt;fd()] == channel);    assert(channel-&gt;isNoneEvent());    int idx = channel-&gt;index();    assert(0 &lt;= idx &amp;&amp; idx &lt; static_cast&lt;int&gt;(pollfds_.size()));    const struct pollfd&amp; pfd = pollfds_[idx]; (void)pfd;    assert(pfd.fd == -channel-&gt;fd()-1 &amp;&amp; pfd.events == channel-&gt;events());    size_t n = channels_.erase(channel-&gt;fd());    assert(n == 1); (void)n;    /*    * 因为poll是基于数组做轮询，考虑到数组的删除代价很大，所以Muduo在这里    * 做了一个优化：如果删除的fd正好是数组尾部，直接pop_back即可，否则，    * 把要删除的fd和数组最后一个元素做交换，并通过父类的channels_设置原来    * 最后一个fd对应的cahnnel的index。最后，删除数组最后一个需要删除的fd即    * 可。    */    if (implicit_cast&lt;size_t&gt;(idx) == pollfds_.size()-1)&#123;        pollfds_.pop_back();    &#125;else&#123;        int channelAtEnd = pollfds_.back().fd;        iter_swap(pollfds_.begin()+idx, pollfds_.end()-1);        if (channelAtEnd &lt; 0)&#123;            channelAtEnd = -channelAtEnd-1;        &#125;        channels_[channelAtEnd]-&gt;set_index(idx);        pollfds_.pop_back();    &#125;&#125;\n\n细节明细疑问\nMuduo为什么要额外为fd的事件封装一个Channel?\n解答\nChannel和Poller的封装其实有考量Muduo的跨平台，为fd的事件多抽象一层channel，在使用不同平台的IO多路复用接口时，只需编写不同平台的Poller代码，然后触发事件后，统一将事件交由channel由上层统一处理，达到了解耦合的效果。Redis网络部分也做了类似的处理。\n\n本章完结\n","tags":["高性能服务器框架"]},{"title":"muduo源码阅读笔记（6、EvevntLoop和Thread）","url":"/2024/01/15/muduo/EvevntLoopAndThread/","content":"Muduo源码笔记系列：\nmuduo源码阅读笔记（0、下载编译muduo）\nmuduo源码阅读笔记（1、同步日志）\nmuduo源码阅读笔记（2、对C语言原生的线程安全以及同步的API的封装）\nmuduo源码阅读笔记（3、线程和线程池的封装）\nmuduo源码阅读笔记（4、异步日志）\nmuduo源码阅读笔记（5、Channel和Poller）\nmuduo源码阅读笔记（6、ExevntLoop和Thread）\nmuduo源码阅读笔记（7、EventLoopThreadPool）\nmuduo源码阅读笔记（8、定时器TimerQueue）\nmuduo源码阅读笔记（9、TcpServer）\nmuduo源码阅读笔记（10、TcpConnection）\n前言\n终于到了Muduo网络库最最核心的部分，这里还是建议大家亲自看看源码。源码很好读，博客最多起到辅助作用。因为EventLoop和Thread是绑定的，所以，可能这两部分放在一起更适合。\n了解ExevntLoop和Thread后，对One Loop Per Thread思想，就有了一个大体的轮廓，这种设计思想，真的很高效，因为，每个线程都有自己的资源，比如epoll、IO事件处理，定时器、任务队列等。每个线程内部资源都是自我维护的（自己的事情自己做）， 除了对线程的任务队列进行操作时有一段极小的临界区需要加锁外，不涉及任何锁的竞争。这里为每个线程设置自己的任务队列的思想特别关键，正是利用每个线程只处理自己任务队列里面的回调任务，实现了将并行任务串行化的效果。 将原本的并发（涉及线程安全，需要加锁）操作，封装成任务（无需加锁）回调，添加到各自的任务队列中，交给线程自己处理。实现了线程的隔离、无锁化编程，巧妙的利用单线程天生串行执行的优势。活该Muduo高性能、高并发。\nEventLoop的实现提供的接口：\n\nclass EventLoop : noncopyable&#123;public:    typedef std::function&lt;void()&gt; Functor;    EventLoop();    ~EventLoop();  // force out-line dtor, for std::unique_ptr members.    ///    /// Loops forever.    ///    /// Must be called in the same thread as creation of the object.    ///    void loop();    /// Quits loop.    ///    /// This is not 100% thread safe, if you call through a raw pointer,    /// better to call through shared_ptr&lt;EventLoop&gt; for 100% safety.    void quit();    /// Runs callback immediately in the loop thread.    /// It wakes up the loop, and run the cb.    /// If in the same loop thread, cb is run within the function.    /// Safe to call from other threads.    void runInLoop(Functor cb);    /// Queues callback in the loop thread.    /// Runs after finish pooling.    /// Safe to call from other threads.    void queueInLoop(Functor cb);    size_t queueSize() const;    // timers    // ...    // internal usage    void wakeup();    void updateChannel(Channel* channel);    void removeChannel(Channel* channel);    // pid_t threadId() const &#123; return threadId_; &#125;    void assertInLoopThread()    bool isInLoopThread() const &#123; return threadId_ == CurrentThread::tid(); &#125;    // 获取当前线程的EventLoop    static EventLoop* getEventLoopOfCurrentThread();private:    void abortNotInLoopThread();    void handleRead();  // waked up    void doPendingFunctors();    typedef std::vector&lt;Channel*&gt; ChannelList;    bool looping_; /* atomic */ // 在loop()中？    std::atomic&lt;bool&gt; quit_;  // EventLoop是否退出    bool eventHandling_; /* atomic */ // 正在处理事件？    bool callingPendingFunctors_; /* atomic */  // 正在处理任务回调？    int64_t iteration_;   // loop()循环的次数    const pid_t threadId_;  // EventLoop和哪个线程绑定？    Timestamp pollReturnTime_;  // epoll_wait返回时的时间戳    std::unique_ptr&lt;Poller&gt; poller_;  // epoll/poll    std::unique_ptr&lt;TimerQueue&gt; timerQueue_;  // 定时器    int wakeupFd_;  // tickleFd，用于手动唤醒epoll_wait，以便即使处理任务    // unlike in TimerQueue, which is an internal class,    // we don&#x27;t expose Channel to client.    std::unique_ptr&lt;Channel&gt; wakeupChannel_;  // 管理tickleFd的channel    boost::any context_;  // 待定    // scratch variables    ChannelList activeChannels_;   // poller_收集到的fd（channel）事件    Channel* currentActiveChannel_; // loop()正在处理的channel    mutable MutexLock mutex_; // 只对pendingFunctors_提供保护    std::vector&lt;Functor&gt; pendingFunctors_ GUARDED_BY(mutex_); // 任务队列&#125;;\n\nMuduo在EventLoop中，使用了Linux系统中EventFd，作为wakeupChannel_的成员。这里主要为了将线程即时唤醒处理回调任务。如果你阅读过sylar的源码应该在这里会有所感知，EventLoop::wakeup()函数其实作用和sylar中的IOManager::tickle()类似。\n此外，为了和Muduo的EventLoop适配，Muduo定时器的实现也是利用Linux上提供的TimeFd，在TimerQueue构造函数中，也会为该fd构造一个Channel，并将该Channel注册到EventLoop的Poller中，这样极大的方便了定时器的管理与维护。\n简单画了一下EventLoop中核心函数EventLoop::loop()的执行流程图：\n\n实现的伪代码：\n// 线程全局变量，for将线程和EventLoop绑定__thread EventLoop* t_loopInThisThread = 0;EventLoop::EventLoop()  : // ...    threadId_(CurrentThread::tid()),    poller_(Poller::newDefaultPoller(this)),    // epoll    timerQueue_(new TimerQueue(this)),      // Muduo定时器采用的是timerfd的接口，TimerQueue对象内部也构造了对应的channel，在构造时，会向EventLopp的Poller中注册该fd。    wakeupFd_(createEventfd()),   // 创建一个非阻塞eventfd，作为tickle    wakeupChannel_(new Channel(this, wakeupFd_))&#123; // 将eventfd封装成channel    LOG_DEBUG &lt;&lt; &quot;EventLoop created &quot; &lt;&lt; this &lt;&lt; &quot; in thread &quot; &lt;&lt; threadId_;    if (t_loopInThisThread)&#123;        LOG_FATAL &lt;&lt; &quot;Another EventLoop &quot; &lt;&lt; t_loopInThisThread                &lt;&lt; &quot; exists in this thread &quot; &lt;&lt; threadId_;    &#125;else&#123;        t_loopInThisThread = this;  // 设置线程局部变量    &#125;    // 为eventfd设置读回调    wakeupChannel_-&gt;setReadCallback(        std::bind(&amp;EventLoop::handleRead, this));    // we are always reading the wakeupfd    wakeupChannel_-&gt;enableReading();&#125;EventLoop::~EventLoop()&#123;    // 善后    LOG_DEBUG &lt;&lt; &quot;EventLoop &quot; &lt;&lt; this &lt;&lt; &quot; of thread &quot; &lt;&lt; threadId_            &lt;&lt; &quot; destructs in thread &quot; &lt;&lt; CurrentThread::tid();    wakeupChannel_-&gt;disableAll();    wakeupChannel_-&gt;remove();    ::close(wakeupFd_);    t_loopInThisThread = NULL;&#125;void EventLoop::loop()&#123;    assert(!looping_);    assertInLoopThread(); // 在自己的线程中？    looping_ = true;    quit_ = false;  // FIXME: what if someone calls quit() before loop() ?    LOG_TRACE &lt;&lt; &quot;EventLoop &quot; &lt;&lt; this &lt;&lt; &quot; start looping&quot;;    while (!quit_)&#123;        activeChannels_.clear();        pollReturnTime_ = poller_-&gt;poll(kPollTimeMs, &amp;activeChannels_); // epoll_wait        ++iteration_;        if (Logger::logLevel() &lt;= Logger::TRACE)&#123;            printActiveChannels();        &#125;        // TODO sort channel by priority        eventHandling_ = true;  // 开始处理每一个fd被触发的事件        for (Channel* channel : activeChannels_)&#123;            currentActiveChannel_ = channel;            currentActiveChannel_-&gt;handleEvent(pollReturnTime_);  // 利用channel处理事件        &#125;        currentActiveChannel_ = NULL;        eventHandling_ = false; // 所有fd的事件被处理完        doPendingFunctors();  // 处理任务回调    &#125;    LOG_TRACE &lt;&lt; &quot;EventLoop &quot; &lt;&lt; this &lt;&lt; &quot; stop looping&quot;;    looping_ = false;&#125;void EventLoop::quit()&#123;    quit_ = true;    // 有线程安全问题    // There is a chance that loop() just executes while(!quit_) and exits,    // then EventLoop destructs, then we are accessing an invalid object.    // Can be fixed using mutex_ in both places.    if (!isInLoopThread())&#123;  // 其他线程调用quit，不确定线程是否是Active的，需要调用wakeup();        wakeup();    &#125; // 在处理事件或者任务回调时，内部自己调用了quit，说名此时线程是Active的，不用去唤醒&#125;void EventLoop::runInLoop(Functor cb)&#123;    if (isInLoopThread()) &#123;// 线程自己调用runInLoop，本来就是Active，直接顺手处理        cb();    &#125;else&#123;  // 其他线程调用runInLoop，需要加锁        queueInLoop(std::move(cb));    &#125;&#125;void EventLoop::queueInLoop(Functor cb)&#123;    &#123;        MutexLockGuard lock(mutex_);        pendingFunctors_.push_back(std::move(cb));  // 任务放入任务队列    &#125;    if (!isInLoopThread() || callingPendingFunctors_)&#123;         // 其他线程调用queueInLoop || 线程在处理任务队列中的任务时调用了queueInLoop        //以便让线程不要阻塞在epoll_wait上从而及时处理任务回调。        wakeup();    &#125;&#125;void EventLoop::updateChannel(Channel* channel)&#123;    assert(channel-&gt;ownerLoop() == this);    assertInLoopThread();    poller_-&gt;updateChannel(channel);&#125;void EventLoop::removeChannel(Channel* channel)&#123;    assert(channel-&gt;ownerLoop() == this);    assertInLoopThread();    if (eventHandling_)&#123;        // channel只有自己能remove自己，其他的channel禁止删除并非自己的channel        assert(currentActiveChannel_ == channel ||            std::find(activeChannels_.begin(), activeChannels_.end(), channel) == activeChannels_.end());    &#125;    poller_-&gt;removeChannel(channel);&#125;// 唤醒线程void EventLoop::wakeup()&#123;    uint64_t one = 1;    // tickle一下    ssize_t n = sockets::write(wakeupFd_, &amp;one, sizeof one);    if (n != sizeof one)&#123;        LOG_ERROR &lt;&lt; &quot;EventLoop::wakeup() writes &quot; &lt;&lt; n &lt;&lt; &quot; bytes instead of 8&quot;;    &#125;&#125;void EventLoop::handleRead()&#123;    uint64_t one = 1;    // tickle 回调处理    ssize_t n = sockets::read(wakeupFd_, &amp;one, sizeof one);    if (n != sizeof one)&#123;        LOG_ERROR &lt;&lt; &quot;EventLoop::handleRead() reads &quot; &lt;&lt; n &lt;&lt; &quot; bytes instead of 8&quot;;    &#125;&#125;void EventLoop::doPendingFunctors()&#123;    std::vector&lt;Functor&gt; functors;    callingPendingFunctors_ = true;    // 写时置换，同异步日志，即减少持锁时间，又减少死锁可能。    &#123;        MutexLockGuard lock(mutex_);        functors.swap(pendingFunctors_);    &#125;    for (const Functor&amp; functor : functors)&#123;        functor();    &#125;    callingPendingFunctors_ = false;&#125;\n\n细节明细：在EventLoop类的成员变量的定义顺序中，poller_的定义位于timerQueue_之上，这个定义的顺序很关键，首先在EventLoop在构造时，先按成员变量的定义顺序构造成员变量，再会构造EventLoop本身。在析构的时候，会先析构自身，再会去按成员变量的定义顺序的倒序，去析构成员变量。考虑到timerQueue_的析构是依赖poller_的，Muduo的定义顺序（先析构timerQueue_，再析构poller_），正好规避了这个问题。\nEventLoopThread的实现简单讲，EventLoop作用就是让EventLoop::loop()跑在线程上。\nEventLoopThread的实现代码是自解释的，代码量很少，也很容易理解。\n提供的接口：\nclass EventLoopThread : noncopyable&#123;public:    typedef std::function&lt;void(EventLoop*)&gt; ThreadInitCallback;    EventLoopThread(const ThreadInitCallback&amp; cb = ThreadInitCallback(),                    const string&amp; name = string());    ~EventLoopThread();    EventLoop* startLoop();private:    void threadFunc();    EventLoop* loop_ GUARDED_BY(mutex_);  // 依赖EventLoop    bool exiting_;  // 线程是否living    Thread thread_; // 线程实体    MutexLock mutex_;    Condition cond_ GUARDED_BY(mutex_);    ThreadInitCallback callback_;&#125;;\n\n有了EventLoop和EventLoopThread后，结合muduo源码阅读笔记（3、线程和线程池的封装）一个EventLoopThread线程启动流程如下：\n\nEventLoopThread::startLoop() -&gt;\n\nThread::start()  -&gt; \n\npthread_create(…, &amp;detail::startThread,…)    -&gt; \n\nstartThread(void* obj)   -&gt;\n\nThreadData::runInThread()    -&gt; \n\nThread::func_()  -&gt;\n\nEventLoopThread::threadFunc()    -&gt;\n\nEventLoop::loop()\n\n\n实现的伪代码：\nEventLoopThread::EventLoopThread(const ThreadInitCallback&amp; cb,                                 const string&amp; name)  : loop_(NULL),    exiting_(false),    thread_(std::bind(&amp;EventLoopThread::threadFunc, this), name),    mutex_(),    cond_(mutex_),    callback_(cb)&#123;&#125;EventLoopThread::~EventLoopThread()&#123;    exiting_ = true;    if (loop_ != NULL)&#123; // not 100% race-free, eg. threadFunc could be running callback_.        // still a tiny chance to call destructed object, if threadFunc exits just now.        // but when EventLoopThread destructs, usually programming is exiting anyway.        loop_-&gt;quit();        thread_.join();    &#125;&#125;EventLoop* EventLoopThread::startLoop()&#123;    assert(!thread_.started());    thread_.start();    EventLoop* loop = NULL;    &#123;        MutexLockGuard lock(mutex_);        while (loop_ == NULL)&#123;  // 解决条件变量惊群效应/虚唤醒            cond_.wait();        &#125;        loop = loop_;    &#125;    return loop;&#125;void EventLoopThread::threadFunc()&#123;    EventLoop loop;    if (callback_)&#123;        callback_(&amp;loop);    &#125;    &#123;        MutexLockGuard lock(mutex_);        loop_ = &amp;loop;        cond_.notify();    &#125;    loop.loop();    //assert(exiting_);    MutexLockGuard lock(mutex_);    loop_ = NULL;&#125;\n\n总结在Muduo网络库的设计中，EventLoop 统一使用文件描述符（file descriptor）的方式来处理事件，主要是基于以下一些好处和设计原则：\n\n一致性： 使用文件描述符作为事件的抽象，使得对于不同类型的事件（包括套接字、定时器等）的处理方式一致。这种一致性简化了 EventLoop 内部的设计和实现，使得对于事件的处理更加通用。\n\n多路复用： 文件描述符是多路复用（Multiplexing）机制的核心。通过将多个文件描述符注册到同一个 EventLoop 中，可以使用诸如 select、poll、epoll 等多路复用技术，实现同时监听多个事件并进行有效的事件分发。\n\n高效性： 文件描述符的处理在操作系统层面已经高度优化，使用多路复用机制可以高效地管理和调度大量的事件。这对于实现高性能的网络库尤为重要。\n\n\n如果读者有阅读其他网络库源码就会不可思议的发现，Muduo的One Loop Per Thread设计思想太精妙了，这种设计几乎不存在锁的竞争！！！\n\n本章完结\n","tags":["高性能服务器框架"]},{"title":"muduo源码阅读笔记（7、EventLoopThreadPool）","url":"/2024/01/16/muduo/EventLoopThreadPool/","content":"Muduo源码笔记系列：\nmuduo源码阅读笔记（0、下载编译muduo）\nmuduo源码阅读笔记（1、同步日志）\nmuduo源码阅读笔记（2、对C语言原生的线程安全以及同步的API的封装）\nmuduo源码阅读笔记（3、线程和线程池的封装）\nmuduo源码阅读笔记（4、异步日志）\nmuduo源码阅读笔记（5、Channel和Poller）\nmuduo源码阅读笔记（6、ExevntLoop和Thread）\nmuduo源码阅读笔记（7、EventLoopThreadPool）\nmuduo源码阅读笔记（8、定时器TimerQueue）\nmuduo源码阅读笔记（9、TcpServer）\nmuduo源码阅读笔记（10、TcpConnection）\n前言\n与base文件夹下的通用线程池相比，EventLoopThreadPool更加专门化，专为为EventLoopThread而生，专为EventLoop而生，专为One Loop Per Thread而生，专为网络事件驱动而生，专为Muduo而生！\n实现提供的接口：\n\nclass EventLoopThreadPool : noncopyable&#123;public:    typedef std::function&lt;void(EventLoop*)&gt; ThreadInitCallback;    EventLoopThreadPool(EventLoop* baseLoop, const string&amp; nameArg);    ~EventLoopThreadPool();    void setThreadNum(int numThreads) &#123; numThreads_ = numThreads; &#125;    void start(const ThreadInitCallback&amp; cb = ThreadInitCallback());    // valid after calling start()    /// round-robin    EventLoop* getNextLoop();    /// with the same hash code, it will always return the same EventLoop    EventLoop* getLoopForHash(size_t hashCode);    std::vector&lt;EventLoop*&gt; getAllLoops();    bool started() const    &#123; return started_; &#125;    const string&amp; name() const    &#123; return name_; &#125;private:    EventLoop* baseLoop_; // 启动EventLoopThreadPool的EventLoop    string name_; // 线程池名    bool started_;  // 启动了？    int numThreads_;  // EventLoopThread线程的数量    int next_;    // 使用round-robin算法做线程的负载均衡，调度到了哪一个线程？    std::vector&lt;std::unique_ptr&lt;EventLoopThread&gt;&gt; threads_; // 线程池本体    std::vector&lt;EventLoop*&gt; loops_; // 每个线程对应的EventLoop&#125;;\n\n结合muduo源码阅读笔记（6、ExevntLoop和Thread）简单画了一下EventLoopThreadPool的架构图：\n\n实现的伪代码：\nEventLoopThreadPool::EventLoopThreadPool(EventLoop* baseLoop, const string&amp; nameArg)  : baseLoop_(baseLoop),    name_(nameArg),    started_(false),    numThreads_(0),    next_(0)&#123;&#125;EventLoopThreadPool::~EventLoopThreadPool()&#123;    // Don&#x27;t delete loop, it&#x27;s stack variable&#125;void EventLoopThreadPool::start(const ThreadInitCallback&amp; cb)&#123;    assert(!started_);    baseLoop_-&gt;assertInLoopThread();    started_ = true;    for (int i = 0; i &lt; numThreads_; ++i)&#123;        char buf[name_.size() + 32];        snprintf(buf, sizeof buf, &quot;%s%d&quot;, name_.c_str(), i);        EventLoopThread* t = new EventLoopThread(cb, buf);        threads_.push_back(std::unique_ptr&lt;EventLoopThread&gt;(t));        loops_.push_back(t-&gt;startLoop()); // 启动线程    &#125;    if (numThreads_ == 0 &amp;&amp; cb)&#123;        cb(baseLoop_);    &#125;&#125;EventLoop* EventLoopThreadPool::getNextLoop()&#123; // round-robin算法做负载均衡    baseLoop_-&gt;assertInLoopThread();    assert(started_);    EventLoop* loop = baseLoop_;  // 线程数为0，就让baseLoop返回    if (!loops_.empty())&#123;        // round-robin        loop = loops_[next_];        ++next_;        if (implicit_cast&lt;size_t&gt;(next_) &gt;= loops_.size())&#123;            next_ = 0;        &#125;    &#125;    return loop;&#125;EventLoop* EventLoopThreadPool::getLoopForHash(size_t hashCode)&#123; // hash散列做负载均衡    baseLoop_-&gt;assertInLoopThread();    EventLoop* loop = baseLoop_;// 线程数为0，就让baseLoop返回    if (!loops_.empty())&#123;        loop = loops_[hashCode % loops_.size()];    &#125;    return loop;&#125;std::vector&lt;EventLoop*&gt; EventLoopThreadPool::getAllLoops()&#123;    baseLoop_-&gt;assertInLoopThread();    assert(started_);    if (loops_.empty())&#123;        return std::vector&lt;EventLoop*&gt;(1, baseLoop_);    &#125;else&#123;        return loops_;    &#125;&#125;\n\n细节明细疑问：\n关于EventLoopThreadPool::getNextLoop()、EventLoopThreadPool::getLoopForHash的作用？\n解答：\n小到线程之间，大到服务器集群之间，都需要保证负载均衡，以免大量的连接集中在某一个线程或者某一台机器，导致压力过大，而使连接任务无法有效处理。\n疑问：\nMuduo为什么大量使用unique_ptr智能指针，而不是使用sahred_ptr智能指针？\n以下是一些可能的原因：\n\n所有权的清晰性： std::unique_ptr表示独占所有权，这意味着每个指针拥有对其指向对象的唯一所有权。这种所有权模型有助于明确代码中哪个部分负责释放资源。\n\n线程安全性： Muduo是一个面向多线程的网络库，而std::shared_ptr的引用计数是原子操作，可能在高并发环境下带来额外的竞争，从而影响性能。相比之下，std::unique_ptr的独占所有权模型更适合并发环境。\n\n性能开销： std::shared_ptr通常会维护一个引用计数，用于跟踪共享对象的所有权信息。这样的引用计数可能引入额外的性能开销，特别是在高并发的网络编程场景下，性能是一个关键因素。\n\n避免循环引用： 使用std::shared_ptr可能导致循环引用的问题，特别是在涉及到复杂的对象关系时。这可能导致资源无法被释放，从而引发内存泄漏。\n\n\n\n本章完结\n","tags":["高性能服务器框架"]},{"title":"muduo源码阅读笔记（0、下载编译muduo）","url":"/2024/01/10/muduo/Start/","content":"Muduo源码笔记系列：\nmuduo源码阅读笔记（0、下载编译muduo）\nmuduo源码阅读笔记（1、同步日志）\nmuduo源码阅读笔记（2、对C语言原生的线程安全以及同步的API的封装）\nmuduo源码阅读笔记（3、线程和线程池的封装）\nmuduo源码阅读笔记（4、异步日志）\nmuduo源码阅读笔记（5、Channel和Poller）\nmuduo源码阅读笔记（6、ExevntLoop和Thread）\nmuduo源码阅读笔记（7、EventLoopThreadPool）\nmuduo源码阅读笔记（8、定时器TimerQueue）\nmuduo源码阅读笔记（9、TcpServer）\nmuduo源码阅读笔记（10、TcpConnection）\nmuduo源码阅读笔记（11、TcpClient）\n环境搭建以及下载安装\ngit clone https://github.com/chenshuo/muduo.git #源码下载# 安装依赖项yum install cmake   # cmake安装yum install boost-devel # boost库# 创建build目录cd muduomkdir buildcd build# 在build目录生成makefile文件cmake ..# 编译make -j4# 安装（可忽略）make install\n\n编译错误的解决我编译时唯一遇到的错误如下：\n/root/workspace/muduo/muduo/base/TimeZone.cc:171:36: error: conversion to ‘int’ from ‘long unsigned int’ may alter its value [-Werror=conversion]   const int time_size = v1 ? sizeof(int32_t) : sizeof(int64_t);                                    ^/root/workspace/muduo/muduo/base/TimeZone.cc:171:54: error: conversion to ‘int’ from ‘long unsigned int’ may alter its value [-Werror=conversion]   const int time_size = v1 ? sizeof(int32_t) : sizeof(int64_t);                                                      ^cc1plus: all warnings being treated as errorsmake[2]: *** [muduo/base/CMakeFiles/muduo_base.dir/TimeZone.cc.o] Error 1make[2]: *** Waiting for unfinished jobs....make[1]: *** [muduo/base/CMakeFiles/muduo_base.dir/all] Error 2make: *** [all] Error 2\n\n查阅资料得知：\n\n该错误是由于编译时启用了 -Werror=conversion 选项，该选项会将警告视为错误。在这里，编译器提示可能由于从 long unsigned int 到 int 的转换而导致值的变化。\n\n在项目的根目录的CMakeLists.txt文件中，将-Werror选项注释即可，此时警告不会被视为错误。\n\n本章完结\n","tags":["高性能服务器框架"]},{"title":"muduo源码阅读笔记（1、同步日志）","url":"/2024/01/10/muduo/SynLogging/","content":"Muduo源码笔记系列：\nmuduo源码阅读笔记（0、下载编译muduo）\nmuduo源码阅读笔记（1、同步日志）\nmuduo源码阅读笔记（2、对C语言原生的线程安全以及同步的API的封装）\nmuduo源码阅读笔记（3、线程和线程池的封装）\nmuduo源码阅读笔记（4、异步日志）\nmuduo源码阅读笔记（5、Channel和Poller）\nmuduo源码阅读笔记（6、ExevntLoop和Thread）\nmuduo源码阅读笔记（7、EventLoopThreadPool）\nmuduo源码阅读笔记（8、定时器TimerQueue）\nmuduo源码阅读笔记（9、TcpServer）\nmuduo源码阅读笔记（10、TcpConnection）\n前言\nMuduo的日志设计的非常简单，日志的格式是固定的，一条日志包括：[日志头，日志体，日志尾]。实际上，参考工业级日志的使用来看，日志还应该能支持格式变更，即用户可以自定义日志的格式，选择自己关心的日志条目进行输出，或者在日志中添加一些额外的字符来修饰日志。考虑到Muduo的核心是网络库，而不是日志库，这些点就不过多深入讨论。\n日志消息体输出到Impl::stream_（简化日志的使用方式（宏定义 + 临时对象的编程技巧）调用匿名对象的stream()成员函数，会返回一个类型为LogStream的引用也即Impl::stream_对象本身，muduo对LogStream类进行了详细的&gt;&gt;操作符重载，这部分代码简单易读，就不详细赘述了，这样就能将字符串类型&#x2F;数值类型的数据使用&gt;&gt;操作符输出到Impl::stream_上（类似std::cout的使用）\n日志消息体的输出：\n\n//// CAUTION: do not write://// if (good)//   LOG_INFO &lt;&lt; &quot;Good news&quot;;// else//   LOG_WARN &lt;&lt; &quot;Bad news&quot;;//// this expends to//// if (good)//   if (logging_INFO)//     logInfoStream &lt;&lt; &quot;Good news&quot;;//   else//     logWarnStream &lt;&lt; &quot;Bad news&quot;;//#define LOG_TRACE if (muduo::Logger::logLevel() &lt;= muduo::Logger::TRACE) \\  muduo::Logger(__FILE__, __LINE__, muduo::Logger::TRACE, __func__).stream()#define LOG_DEBUG if (muduo::Logger::logLevel() &lt;= muduo::Logger::DEBUG) \\  muduo::Logger(__FILE__, __LINE__, muduo::Logger::DEBUG, __func__).stream()#define LOG_INFO if (muduo::Logger::logLevel() &lt;= muduo::Logger::INFO) \\  muduo::Logger(__FILE__, __LINE__).stream()#define LOG_WARN muduo::Logger(__FILE__, __LINE__, muduo::Logger::WARN).stream()#define LOG_ERROR muduo::Logger(__FILE__, __LINE__, muduo::Logger::ERROR).stream()#define LOG_FATAL muduo::Logger(__FILE__, __LINE__, muduo::Logger::FATAL).stream()#define LOG_SYSERR muduo::Logger(__FILE__, __LINE__, false).stream()#define LOG_SYSFATAL muduo::Logger(__FILE__, __LINE__, true).stream()\n\n综合Logger以及LogStream的实现可知，在程序运行期间，通过上面的宏使用muduo的日志时，创建的Logger临时对象会在 栈上开辟一段很大的空间（一般是detail::kSmallBuffer（4000byte）） 缓存日志\n日志消息头输出到Impl::stream_结合上面的宏定义来讲，在muduo中，当临时的Logger对象构造时，在其构造函数中，首先会自动输出一条日志的基本头部信息，比如对宏定义传来的时间戳进行格式化输出，输出线程所在的tid以及日志级别，如果是一条错误报告的log（此时errno非0），还会输出错误码的字符串信息。\nLogger::Impl::Impl(LogLevel level, int savedErrno, const SourceFile&amp; file, int line)  : time_(Timestamp::now()),    stream_(),    level_(level),    line_(line),    basename_(file)&#123;  formatTime(); // 输出格式化的时间戳（这部分代码可简略的看一下，了解作用即可，无需细看。  CurrentThread::tid(); // 缓存tid  stream_ &lt;&lt; T(CurrentThread::tidString(), CurrentThread::tidStringLength()); // 输出字符串形式的tid  stream_ &lt;&lt; T(LogLevelName[level], 6); //输出字符串形式的日志级别  if (savedErrno != 0)  //需要输出错误就输出错误  &#123;    stream_ &lt;&lt; strerror_tl(savedErrno) &lt;&lt; &quot; (errno=&quot; &lt;&lt; savedErrno &lt;&lt; &quot;) &quot;;  &#125;&#125;// ...Logger::Logger(SourceFile file, int line)  : impl_(INFO, 0, file, line)&#123;&#125;// ...\n\n日志尾部输出到Impl::stream_ &amp;&amp; Impl::stream_对象直接输出到日志输出地（LogAppender）（利用临时对象行生命周期的特点，在析构中，同步（默认）输出日志。这里的LogAppender可能代表磁盘上的文件、控制台std::cout、数据库等。\nMuduo的日志中g_output其实是类型是函数指针的全局变量，这里通过函数指针实现了C语言的多态，Muduo默认的g_output是直接将Impl::stream_拼接的日志输出到控制台，即输出是同步的。当然，如果用户参考g_output的定义，实现了自己的输出函数，可以通过Logger::setOutput()接口，提供自定义函数的地址作为参数，将自定义函数安装到g_output上。后面Muduo实现的异步日志就是这么干的。\nvoid defaultOutput(const char* msg, int len)&#123;  // 同步输出到终端  size_t n = fwrite(msg, 1, len, stdout);  //FIXME check n  (void)n;&#125;void defaultFlush()&#123;  fflush(stdout);&#125;/** 函数指针，实现多态。以及输出的解耦。* typedef void (*OutputFunc)(const char* msg, int len);* typedef void (*FlushFunc)();*/Logger::OutputFunc g_output = defaultOutput;Logger::FlushFunc g_flush = defaultFlush;// ...void Logger::Impl::finish()&#123;  stream_ &lt;&lt; &quot; - &quot; &lt;&lt; basename_ &lt;&lt; &#x27;:&#x27; &lt;&lt; line_ &lt;&lt; &#x27;\\n&#x27;;    //将文件名以及日志所在行号（临时对象的构造会传入这两个信息）作为日志尾输出到Impl::stream_&#125;// ...Logger::~Logger()&#123;  impl_.finish();  const LogStream::Buffer&amp; buf(stream().buffer());  // 获取Impl::stream_  g_output(buf.data(), buf.length());   // 将Impl::stream_输出到日志输出地（stdout/file/database）  if (impl_.level_ == FATAL)  &#123;    g_flush();    abort();  &#125;&#125;\n\n日志效果\n\n\nLogHeader\nLogBody\nLogTail\n\n\n\nTime ThreadID LogLevel\nLogMessage\n- FileName:LineNumber\n\n\n20240109 03:21:56.970321Z  3094 INFO  Hello - Logging_test.cc:6920240109 03:21:56.970363Z  3094 WARN  World - Logging_test.cc:7020240109 03:21:56.970367Z  3094 ERROR Error - Logging_test.cc:71\n\n细节明细在 Muduo 中，为了实现日志的功能，使用了一个内部的 Impl 类来处理日志的具体实现细节。这样的设计有几个优点：\n\n封装性：将日志的具体实现封装在 Impl 类中，使得日志系统的使用者无需关心内部的具体实现细节。这样可以减少用户对日志系统内部的依赖，提高系统的封装性和可维护性。\n\n灵活性：Impl 类的存在使得 Muduo 可以更加灵活地修改、扩展或者替换日志系统的具体实现，而不会对外部接口产生影响。如果未来需要更换日志库、修改日志输出格式等，只需修改 Impl 类的实现而不必修改用户代码。\n\n解耦：通过引入 Impl 类，日志系统的实现与接口之间形成了一种解耦。这种解耦有助于降低模块之间的依赖性，提高代码的灵活性和可维护性。\n\n信息隐藏：Impl 类将具体的实现细节隐藏在类的私有部分，只暴露必要的接口给外部。这有助于控制用户对日志系统内部的访问权限，同时防止滥用或错误的使用。\n\n\nmuduo还统一了日志级别的字符串长度，固定为6，不足的补空格，这样，也提升了一点点性能，毕竟积少成多。同时利用模板，在编译期确定字符串长度的操作，可以参考SourceFile类的数组引用构造的实现。\ntemplate&lt;int N&gt;SourceFile(const char (&amp;arr)[N])  // 数组引用，编译期就能确定字符串长度。    : data_(arr),    size_(N-1)&#123;    const char* slash = strrchr(data_, &#x27;/&#x27;); // builtin function    if (slash)    &#123;    data_ = slash + 1;    size_ -= static_cast&lt;int&gt;(data_ - arr);    &#125;&#125;\n\n\n本章完结\n","tags":["高性能服务器框架"]},{"title":"muduo源码阅读笔记（10、TcpConnection）","url":"/2024/01/19/muduo/TcpConnection/","content":"Muduo源码笔记系列：\nmuduo源码阅读笔记（0、下载编译muduo）\nmuduo源码阅读笔记（1、同步日志）\nmuduo源码阅读笔记（2、对C语言原生的线程安全以及同步的API的封装）\nmuduo源码阅读笔记（3、线程和线程池的封装）\nmuduo源码阅读笔记（4、异步日志）\nmuduo源码阅读笔记（5、Channel和Poller）\nmuduo源码阅读笔记（6、ExevntLoop和Thread）\nmuduo源码阅读笔记（7、EventLoopThreadPool）\nmuduo源码阅读笔记（8、定时器TimerQueue）\nmuduo源码阅读笔记（9、TcpServer）\nmuduo源码阅读笔记（10、TcpConnection）\n前言\n本章涉及两个新模块：TcpConnection、Buffer。本文重点集中在TcpConnection上，对于Buffer会进行简单的描述。\nBufferMuduo的Buffer类实际上就是基于vector&lt;char&gt;实现了一个缓存区，在vector的基础上，自己封装了扩容和缩容的接口。每个TcpConnection都会自带两个Buffer，一个读缓存区和一个写缓存区。\n这里只列出TcpConnection用到的接口的实现。\n提供的接口：\n\nclass Buffer : public muduo::copyable&#123;public:    static const size_t kCheapPrepend = 8;  // 为prepend预留    static const size_t kInitialSize = 1024;    // 默认大小    explicit Buffer(size_t initialSize = kInitialSize)    : buffer_(kCheapPrepend + initialSize),        readerIndex_(kCheapPrepend),        writerIndex_(kCheapPrepend)&#123;        assert(readableBytes() == 0);        assert(writableBytes() == initialSize);        assert(prependableBytes() == kCheapPrepend);    &#125;    size_t readableBytes() const    &#123; return writerIndex_ - readerIndex_; &#125;    size_t writableBytes() const    &#123; return buffer_.size() - writerIndex_; &#125;    size_t prependableBytes() const    &#123; return readerIndex_; &#125;    const char* peek() const    &#123; return begin() + readerIndex_; &#125;    //各种读写操作省略    // ...        void append(const char* /*restrict*/ data, size_t len)&#123;        ensureWritableBytes(len);        std::copy(data, data+len, beginWrite());        hasWritten(len);    &#125;    void ensureWritableBytes(size_t len)&#123;        if (writableBytes() &lt; len)&#123;            makeSpace(len);        &#125;        assert(writableBytes() &gt;= len);    &#125;    void shrink(size_t reserve)&#123;    // 缩容        // FIXME: use vector::shrink_to_fit() in C++ 11 if possible.        Buffer other;        other.ensureWritableBytes(readableBytes()+reserve);        other.append(toStringPiece());        swap(other);    &#125;    /// Read data directly into buffer.    ///    /// It may implement with readv(2)    /// @return result of read(2), @c errno is saved    ssize_t readFd(int fd, int* savedErrno);private:    char* begin()    &#123; return &amp;*buffer_.begin(); &#125;    void makeSpace(size_t len)&#123;        if (writableBytes() + prependableBytes() &lt; len + kCheapPrepend)&#123; // 扩容。            // FIXME: move readable data            buffer_.resize(writerIndex_+len);        &#125;else&#123; // 原地腾空间            // move readable data to the front, make space inside buffer            assert(kCheapPrepend &lt; readerIndex_);            size_t readable = readableBytes();            std::copy(begin()+readerIndex_,                    begin()+writerIndex_,                    begin()+kCheapPrepend);            readerIndex_ = kCheapPrepend;            writerIndex_ = readerIndex_ + readable;            assert(readable == readableBytes());        &#125;    &#125;private:  std::vector&lt;char&gt; buffer_;  size_t readerIndex_;  // 读到哪里  size_t writerIndex_;  // 写到哪里&#125;;\n\nBuffer的结构如下：\n/// A buffer class modeled after org.jboss.netty.buffer.ChannelBuffer////// @code/// +-------------------+------------------+------------------+/// | prependable bytes |  readable bytes  |  writable bytes  |/// |                   |     (CONTENT)    |                  |/// +-------------------+------------------+------------------+/// |                   |                  |                  |/// 0      &lt;=      readerIndex   &lt;=   writerIndex    &lt;=     size/// @endcode\n\n实现的伪代码：\n/** 将sockfd上的数据读到buffer上。*/ssize_t Buffer::readFd(int fd, int* savedErrno)&#123;    // saved an ioctl()/FIONREAD call to tell how much to read    char extrabuf[65536];    struct iovec vec[2];    const size_t writable = writableBytes();    vec[0].iov_base = begin()+writerIndex_;    vec[0].iov_len = writable;    vec[1].iov_base = extrabuf;    vec[1].iov_len = sizeof extrabuf;    // when there is enough space in this buffer, don&#x27;t read into extrabuf.    // when extrabuf is used, we read 128k-1 bytes at most.    // buffer够大，只用buffer，否者buffer和extrabuf一起用    const int iovcnt = (writable &lt; sizeof extrabuf) ? 2 : 1;    const ssize_t n = sockets::readv(fd, vec, iovcnt);    if (n &lt; 0)&#123;        *savedErrno = errno;    &#125;else if (implicit_cast&lt;size_t&gt;(n) &lt;= writable)&#123;        writerIndex_ += n;    &#125;else&#123;        writerIndex_ = buffer_.size();        append(extrabuf, n - writable); // 将extrabuf的数据append到buffer中    &#125;    return n;&#125;\n\nTcpConnection仔细阅读源码，结合前面的TimeQueue和Acceptor，TcpConnection的整体结构其实和这两个类差不多。内部都是维护了专门的fd的channel，实现了各种事件处理回调。只不过TcpConnection管理的是数据读写套接字，涉及的事件比较多，回调处理部分也稍稍复杂点。\nTcpConnection对象的构造：\n根据传进来的sockfd、loop，为sockfd构造一个channel，并为channel设置事件的回调处理函数，最后将sockfd设置为SO_KEEPALIVE。（TcpConnection::state_初始化为kConnecting）\n代码如下：\nTcpConnection::TcpConnection(EventLoop* loop,                             const string&amp; nameArg,                             int sockfd,                             const InetAddress&amp; localAddr,                             const InetAddress&amp; peerAddr)  : loop_(CHECK_NOTNULL(loop)),    name_(nameArg),    state_(kConnecting),    reading_(true),    socket_(new Socket(sockfd)),    channel_(new Channel(loop, sockfd)),    localAddr_(localAddr),    peerAddr_(peerAddr),    highWaterMark_(64*1024*1024)&#123;    channel_-&gt;setReadCallback(  // 读回调        std::bind(&amp;TcpConnection::handleRead, this, _1));    channel_-&gt;setWriteCallback( // 写回调        std::bind(&amp;TcpConnection::handleWrite, this));    channel_-&gt;setCloseCallback( // sockfd关闭回调        std::bind(&amp;TcpConnection::handleClose, this));    channel_-&gt;setErrorCallback( // 错误处理回调        std::bind(&amp;TcpConnection::handleError, this));    LOG_DEBUG &lt;&lt; &quot;TcpConnection::ctor[&quot; &lt;&lt;  name_ &lt;&lt; &quot;] at &quot; &lt;&lt; this            &lt;&lt; &quot; fd=&quot; &lt;&lt; sockfd;    socket_-&gt;setKeepAlive(true);    // 长连接&#125;\n\n连接的建立：\n接着muduo源码阅读笔记（9、TcpServer）。\n\n在绑定的ioloop中执行TcpConnection::connectEstablished()，进行连接的初始化，过程如下：\n\n将TcpConnection::state_设置成kConnected。\n\n将channel_的生命周期和TcpConnection绑定，以免TcpConnection被销毁后，channel的回调继续错误的被执行。\n\n向ioloop的Poller中注册channel_并使能读事件。\n\n调用TcpConnection::connectionCallback_回调。\n\n连接建立完毕。\n\n\n\n\n代码如下：\nvoid TcpConnection::connectEstablished()&#123;    loop_-&gt;assertInLoopThread();    assert(state_ == kConnecting);    setState(kConnected);    channel_-&gt;tie(shared_from_this());    channel_-&gt;enableReading();  // 使能读事件    connectionCallback_(shared_from_this());&#125;\n\n接收数据：\n全权由读回调接收：\n\n将数据读到TcpConnection::inputBuffer_，返回值n（读到字节数）\n\n\nn &gt; 0，调用TcpConnection::messageCallback_处理数据\nn &#x3D;&#x3D; 0，说明连接关闭，调用TcpConnection::handleClose()回调。\nn &lt; 0，出错，调用TcpConnection::handleError处理。\n\n\n\n代码如下：\nvoid TcpConnection::handleRead(Timestamp receiveTime)&#123;    loop_-&gt;assertInLoopThread();    int savedErrno = 0;    ssize_t n = inputBuffer_.readFd(channel_-&gt;fd(), &amp;savedErrno);    if (n &gt; 0)&#123;        messageCallback_(shared_from_this(), &amp;inputBuffer_, receiveTime);    &#125;else if (n == 0)&#123;        handleClose();    &#125;else&#123;        errno = savedErrno;        LOG_SYSERR &lt;&lt; &quot;TcpConnection::handleRead&quot;;        handleError();    &#125;&#125;\n\n发送数据：\n主动发送：\n\n用户调用TcpConnection::send\n\n如果正好在ioloop内，直接调用TcpConnection::sendInLoop()，否则，向ioloop的任务队列中添加TcpConnection::sendInLoop()异步回调。\n\n执行TcpConnection::sendInLoop()\n\n如果连接状态为kDisconnected，说明连接断开，直接返回。\n\n先直接调用::write，能写多少是多少，触发errno &#x3D;&#x3D; EPIPE || errno &#x3D;&#x3D; ECONNRESET错误就直接返回。\n\n如果写完了，异步调用一下writeCompleteCallback_回调。否者，说明底层的发送缓存满了，剩余的数据追加到outputBuffer_，并使能channel_的写事件，异步通知写outputBuffer_。当然，如果outputBuffer_积累的数据太多，达到阈值，就异步调用一下highWaterMarkCallback_。\n\n\n\n\n\n\nsendInLoop代码如下：\nvoid TcpConnection::sendInLoop(const void* data, size_t len)&#123;    loop_-&gt;assertInLoopThread();    ssize_t nwrote = 0;    size_t remaining = len; // 还剩多少没发    bool faultError = false;    if (state_ == kDisconnected)&#123; // 连接断开        LOG_WARN &lt;&lt; &quot;disconnected, give up writing&quot;;        return;    &#125;    // if no thing in output queue, try writing directly    if (!channel_-&gt;isWriting() &amp;&amp; outputBuffer_.readableBytes() == 0)&#123; // Poller没有监听conn fd的写事件 &amp;&amp; TcpConnection::outputBuffer_缓存没有数据等待发送（完全空闲）。        // 尽最大努力写一次，能写多少是多少        // 如果数据没写完，说明TCP发送缓存满，就需要向Poller注册写事件，来通知异步写，将剩余的数据写完。        nwrote = sockets::write(channel_-&gt;fd(), data, len);         if (nwrote &gt;= 0)&#123;            remaining = len - nwrote;            if (remaining == 0 &amp;&amp; writeCompleteCallback_)&#123;                loop_-&gt;queueInLoop(std::bind(writeCompleteCallback_, shared_from_this()));            &#125;        &#125;else&#123; // nwrote &lt; 0            nwrote = 0;            if (errno != EWOULDBLOCK)&#123;                LOG_SYSERR &lt;&lt; &quot;TcpConnection::sendInLoop&quot;;                if (errno == EPIPE || errno == ECONNRESET) &#123;// FIXME: any others?// 本端sock写关闭，但是还向sock里面写，会触发EPIPE || 连接关闭                    faultError = true;                &#125;            &#125;        &#125;    &#125;    assert(remaining &lt;= len);    if (!faultError &amp;&amp; remaining &gt; 0)&#123;  // TCP写缓存满，还有代写数据，只能异步写。        size_t oldLen = outputBuffer_.readableBytes();        if (oldLen + remaining &gt;= highWaterMark_            &amp;&amp; oldLen &lt; highWaterMark_            &amp;&amp; highWaterMarkCallback_)&#123;            loop_-&gt;queueInLoop(std::bind(highWaterMarkCallback_, shared_from_this(), oldLen + remaining));        &#125;        outputBuffer_.append(static_cast&lt;const char*&gt;(data)+nwrote, remaining);        if (!channel_-&gt;isWriting())&#123;            channel_-&gt;enableWriting();        &#125;    &#125;&#125;\n\n异步发送：\n因为发送缓存区满了，所以不得不由Poller异步通知来发送数据\n\n发送缓存未满，Poller触发可写事件，调用TcpConnection::handleWrite()\n\n保证channel_-&gt;isWriting() &#x3D;&#x3D; true，否则什么也不做输出日志后返回。\n\n调用::write()发送outputBuffer_数据。\n\n如果outputBuffer_数据发送完了，取消cahnnel_的写事件，并异步调用一下writeCompleteCallback_ &amp;&amp; 如果连接状态是kDisconnecting，执行shutdownInLoop()。关闭本端写。\n\n\n\n\nhandleWrite()代码如下：\nvoid TcpConnection::handleWrite()&#123;    loop_-&gt;assertInLoopThread();    if (channel_-&gt;isWriting())&#123;        ssize_t n = sockets::write(channel_-&gt;fd(),                                outputBuffer_.peek(),                                outputBuffer_.readableBytes());        if (n &gt; 0)&#123;            outputBuffer_.retrieve(n);            if (outputBuffer_.readableBytes() == 0)&#123;                channel_-&gt;disableWriting();                if (writeCompleteCallback_)&#123;                    loop_-&gt;queueInLoop(std::bind(writeCompleteCallback_, shared_from_this()));                &#125;                if (state_ == kDisconnecting)&#123;                    shutdownInLoop();                &#125;            &#125;        &#125;else&#123;            LOG_SYSERR &lt;&lt; &quot;TcpConnection::handleWrite&quot;;        &#125;    &#125;else&#123;        LOG_TRACE &lt;&lt; &quot;Connection fd = &quot; &lt;&lt; channel_-&gt;fd()                &lt;&lt; &quot; is down, no more writing&quot;;    &#125;&#125;\n\n关闭连接：\n主动关闭：\n\n主动调用TcpConnection::forceClose。\n\n将连接状态设置成kDisconnecting \n\n异步回调TcpConnection::forceCloseInLoop()\n\n调用handleClose()\n\n将连接状态设置成kDisconnected\n\n取消channel_所有事件\n\n调用connectionCallback_\n\n调用closeCallback_（即将TcpServer上的连接信息删除掉）\n\n异步回调TcpConnection::connectDestroyed\n\n将channel从Poller中移除。\n\n\n\n\n\n\n\n相关代码如下：\nvoid TcpConnection::forceClose()&#123;    // FIXME: use compare and swap    if (state_ == kConnected || state_ == kDisconnecting)&#123;        setState(kDisconnecting);        loop_-&gt;queueInLoop(std::bind(&amp;TcpConnection::forceCloseInLoop, shared_from_this()));    &#125;&#125;void TcpConnection::forceCloseInLoop()&#123;    loop_-&gt;assertInLoopThread();    if (state_ == kConnected || state_ == kDisconnecting)&#123;        // as if we received 0 byte in handleRead();        handleClose();    &#125;&#125;void TcpConnection::handleClose()&#123;    loop_-&gt;assertInLoopThread();    LOG_TRACE &lt;&lt; &quot;fd = &quot; &lt;&lt; channel_-&gt;fd() &lt;&lt; &quot; state = &quot; &lt;&lt; stateToString();    assert(state_ == kConnected || state_ == kDisconnecting);    // we don&#x27;t close fd, leave it to dtor, so we can find leaks easily.    setState(kDisconnected);    channel_-&gt;disableAll();    TcpConnectionPtr guardThis(shared_from_this());    connectionCallback_(guardThis);   // connectionCallback_见TcpServer    // must be the last line    closeCallback_(guardThis);    // closeCallback_见TcpServer&#125;void TcpConnection::connectDestroyed()&#123;    loop_-&gt;assertInLoopThread();    if (state_ == kConnected)&#123;        setState(kDisconnected);        channel_-&gt;disableAll();        connectionCallback_(shared_from_this());    &#125;    channel_-&gt;remove();&#125;\n\n被动关闭：\n\nTcpConnection::handleRead：因为read返回0，代表连接已经被关闭。会被动调用handleClose。\n\nChannel::handleEventWithGuard：channel_触发POLLHUP事件，连接被关闭，会被动调用handleClose。\n\n\n细节明细疑问：\n为什么 muduo 要设计一个 shutdown() 半关闭TCP连接？\n解答：\n用 shutdown 而不用 close 的效果是，如果对方已经发送了数据，这些数据还“在路上”，那么 muduo 不会漏收这些数据。换句话说，muduo 在 TCP 这一层面解决了“当你打算关闭网络连接的时候，如何得知对方有没有发了一些数据而你还没有收到？”这一问题。当然，这个问题也可以在上面的协议层解决，双方商量好不再互发数据，就可以直接断开连接。\n完整的流程是：我们发完了数据，于是 shutdownWrite，发送 TCP FIN 分节，对方会读到 0 字节，然后对方通常会关闭连接，这样 muduo 会读到 0 字节，然后 muduo 关闭连接。\n原文链接：https://blog.csdn.net/Solstice/article/details/6208634\n小结本章涉及的回调有些复杂，有遗漏的，后面会补充。至此，Muduo服务端源码分析，基本完成。真心建议各位读者能反复去阅读Muduo的源码。\n后续可能会计划出一下sylar的源码笔记。然后看有没有时间整理一下LevelDB的源码笔记（可能会鸽，因为马上要春招了，并没有多少时间去写博客）但找到工作之后，也会坚持写的。而且存储方面我也就了解点LevelDB，没有其他存储引擎的底子，没有对比理解的可能也不是很深。\n\n本章完结\n","tags":["高性能服务器框架"]},{"title":"muduo源码阅读笔记（11、TcpClient）","url":"/2024/01/27/muduo/TcpClient/","content":"Muduo源码笔记系列：\nmuduo源码阅读笔记（0、下载编译muduo）\nmuduo源码阅读笔记（1、同步日志）\nmuduo源码阅读笔记（2、对C语言原生的线程安全以及同步的API的封装）\nmuduo源码阅读笔记（3、线程和线程池的封装）\nmuduo源码阅读笔记（4、异步日志）\nmuduo源码阅读笔记（5、Channel和Poller）\nmuduo源码阅读笔记（6、ExevntLoop和Thread）\nmuduo源码阅读笔记（7、EventLoopThreadPool）\nmuduo源码阅读笔记（8、定时器TimerQueue）\nmuduo源码阅读笔记（9、TcpServer）\nmuduo源码阅读笔记（10、TcpConnection）\nmuduo源码阅读笔记（11、TcpClient）\n前言\n本章新涉及的文件有：\n\nTcpClient.h&#x2F;cc：和TcpServer不同的是，TcpClient位于客户端，主要是对客户发起的连接进行管理，TcpClient只有一个loop，也会和TcpConnection配合，将三次握手连接成功的sockfd交由TcpConnection管理。\n\nConnector.h&#x2F;cc：Muduo将一个客户端的sock分成了两个阶段，分别是：连接阶段、读写阶段，Connector就是负责fd的连接阶段，当一个sockfd连接成功后，将sockfd传给TcpClient，由TcpClient将sockfd传给TcpConnection进行读写管理，Connector和TcpServer的Acceptor在设计上有这类似的思想，不同的是，Connector是可以针对同一个ip地址进行多次连接，产生不同的sockfd、而Acceptor是去读listen sock来接收连接，产生不同sockfd。\n\n\n总体来说，TcpClient的实现是严格遵循TcpServer的实现的，\nConnector的实现提供的接口：\n\nclass Connector : noncopyable,                  public std::enable_shared_from_this&lt;Connector&gt;&#123;public:    typedef std::function&lt;void (int sockfd)&gt; NewConnectionCallback;    Connector(EventLoop* loop, const InetAddress&amp; serverAddr);    ~Connector();    void setNewConnectionCallback(const NewConnectionCallback&amp; cb)    &#123; newConnectionCallback_ = cb; &#125;    void start();  // can be called in any thread    void restart();  // must be called in loop thread    void stop();  // can be called in any thread    const InetAddress&amp; serverAddress() const &#123; return serverAddr_; &#125;    private:    enum States &#123; kDisconnected, kConnecting, kConnected &#125;;    static const int kMaxRetryDelayMs = 30*1000;    static const int kInitRetryDelayMs = 500;    void setState(States s) &#123; state_ = s; &#125;    void startInLoop();    void stopInLoop();    void connect();    void connecting(int sockfd);    void handleWrite();    void handleError();    void retry(int sockfd);    int removeAndResetChannel();    void resetChannel();    EventLoop* loop_; // 连接发起所在loop    InetAddress serverAddr_;  // 连接到哪里    bool connect_; // atomic  // 开始连接？    States state_;  // FIXME: use atomic variable // 连接状态    std::unique_ptr&lt;Channel&gt; channel_;  // fd读写以及读写事件管理，对epoll/poll/selectIO多路复用的抽象，方便跨平台。    NewConnectionCallback newConnectionCallback_; // 一般是：TcpClient::newConnection    int retryDelayMs_;  // 连接重试毫秒数。&#125;;\n\n简单记录一下连接阶段启动流程：\n调用Connector::start()-&gt;\n\nconnect_ 赋值为 true。\n\n在loop任务队列追加Connector::startInLoop()回调任务\n\n执行回调任务：Connector::startInLoop()\n\n调用Connector::connect()\n\n创建非阻塞的连接sock\n\n::connect(sock, …)\n\n调用Connector::connecting(int sockfd)\n\nnew channel(sockfd)赋值给channel_将Connector::handleWrite()和Connector::handleError()设置给cahnnel的写回调以及错误处理回调\n\n使能Poller开始监听sockfd\n\n\n\n\n\n\n\n\n当连接成功，会触发sockfd的写事件，从而调用Connector::handleWrite()-&gt;\n\n将sockfd和channel_解绑，并将channel_ rest。\n\n调用newConnectionCallback_（也即TcpClient::newConnection）将连接完成的sockfd传给TcpClient处理\n\n\n感兴趣的读者，可以自行阅读源码，了解连接过程中，stop、retry的流程。\n实现的伪代码：\nvoid Connector::start()&#123;    connect_ = true;    loop_-&gt;runInLoop(std::bind(&amp;Connector::startInLoop, this)); // FIXME: unsafe&#125;void Connector::startInLoop()&#123;    loop_-&gt;assertInLoopThread();    assert(state_ == kDisconnected);    if (connect_)&#123;        connect();    &#125;else&#123;        LOG_DEBUG &lt;&lt; &quot;do not connect&quot;;    &#125;&#125;void Connector::stop()&#123;    connect_ = false;    loop_-&gt;queueInLoop(std::bind(&amp;Connector::stopInLoop, this)); // FIXME: unsafe    // FIXME: cancel timer&#125;void Connector::stopInLoop()&#123;    loop_-&gt;assertInLoopThread();    if (state_ == kConnecting)&#123;        setState(kDisconnected);        int sockfd = removeAndResetChannel();        retry(sockfd);    &#125;&#125;void Connector::connect()&#123;    int sockfd = sockets::createNonblockingOrDie(serverAddr_.family());    int ret = sockets::connect(sockfd, serverAddr_.getSockAddr());    int savedErrno = (ret == 0) ? 0 : errno;    switch (savedErrno)&#123;        case 0:        case EINPROGRESS:        case EINTR:        case EISCONN:            connecting(sockfd);            break;        /*...*/    &#125;&#125;void Connector::connecting(int sockfd)&#123;    setState(kConnecting);    assert(!channel_);    channel_.reset(new Channel(loop_, sockfd));    channel_-&gt;setWriteCallback(        std::bind(&amp;Connector::handleWrite, this)); // FIXME: unsafe    channel_-&gt;setErrorCallback(        std::bind(&amp;Connector::handleError, this)); // FIXME: unsafe    // channel_-&gt;tie(shared_from_this()); is not working,    // as channel_ is not managed by shared_ptr    channel_-&gt;enableWriting();&#125;int Connector::removeAndResetChannel()&#123;    channel_-&gt;disableAll();    channel_-&gt;remove();    int sockfd = channel_-&gt;fd();    // Can&#x27;t reset channel_ here, because we are inside Channel::handleEvent    loop_-&gt;queueInLoop(std::bind(&amp;Connector::resetChannel, this)); // FIXME: unsafe    return sockfd;&#125;void Connector::resetChannel()&#123;    channel_.reset();&#125;void Connector::handleWrite()&#123;    LOG_TRACE &lt;&lt; &quot;Connector::handleWrite &quot; &lt;&lt; state_;    if (state_ == kConnecting)&#123;        int sockfd = removeAndResetChannel();        int err = sockets::getSocketError(sockfd);        if (err)&#123;            LOG_WARN &lt;&lt; &quot;Connector::handleWrite - SO_ERROR = &quot;                    &lt;&lt; err &lt;&lt; &quot; &quot; &lt;&lt; strerror_tl(err);            retry(sockfd);        &#125;else&#123;            setState(kConnected);            if (connect_)&#123;                newConnectionCallback_(sockfd);            &#125;else&#123;                sockets::close(sockfd);            &#125;        &#125;    &#125;else&#123;        // what happened?        assert(state_ == kDisconnected);    &#125;&#125;void Connector::handleError()&#123;    LOG_ERROR &lt;&lt; &quot;Connector::handleError state=&quot; &lt;&lt; state_;    if (state_ == kConnecting)&#123;        int sockfd = removeAndResetChannel();        int err = sockets::getSocketError(sockfd);        LOG_TRACE &lt;&lt; &quot;SO_ERROR = &quot; &lt;&lt; err &lt;&lt; &quot; &quot; &lt;&lt; strerror_tl(err);        retry(sockfd);    &#125;&#125;void Connector::retry(int sockfd)&#123;    sockets::close(sockfd);    setState(kDisconnected);    if (connect_)&#123;        LOG_INFO &lt;&lt; &quot;Connector::retry - Retry connecting to &quot; &lt;&lt; serverAddr_.toIpPort()                    &lt;&lt; &quot; in &quot; &lt;&lt; retryDelayMs_ &lt;&lt; &quot; milliseconds. &quot;;        loop_-&gt;runAfter(retryDelayMs_/1000.0, // 稍后重试                        std::bind(&amp;Connector::startInLoop, shared_from_this()));        retryDelayMs_ = std::min(retryDelayMs_ * 2, kMaxRetryDelayMs);  // 超时加倍    &#125;else&#123;        LOG_DEBUG &lt;&lt; &quot;do not connect&quot;;    &#125;&#125;\n\nTcpClient的实现提供的接口：\nclass TcpClient : noncopyable&#123;public:    // TcpClient(EventLoop* loop);    // TcpClient(EventLoop* loop, const string&amp; host, uint16_t port);    TcpClient(EventLoop* loop,            const InetAddress&amp; serverAddr,            const string&amp; nameArg);    ~TcpClient();  // force out-line dtor, for std::unique_ptr members.    void connect();    void disconnect();    void stop();    TcpConnectionPtr connection() const    &#123;    MutexLockGuard lock(mutex_);    return connection_;    &#125;    EventLoop* getLoop() const &#123; return loop_; &#125;    bool retry() const &#123; return retry_; &#125;    void enableRetry() &#123; retry_ = true; &#125;    const string&amp; name() const    &#123; return name_; &#125;    /// Set connection callback.    /// Not thread safe.    void setConnectionCallback(ConnectionCallback cb)    &#123; connectionCallback_ = std::move(cb); &#125;    /// Set message callback.    /// Not thread safe.    void setMessageCallback(MessageCallback cb)    &#123; messageCallback_ = std::move(cb); &#125;    /// Set write complete callback.    /// Not thread safe.    void setWriteCompleteCallback(WriteCompleteCallback cb)    &#123; writeCompleteCallback_ = std::move(cb); &#125;private:    /// Not thread safe, but in loop    void newConnection(int sockfd);    /// Not thread safe, but in loop    void removeConnection(const TcpConnectionPtr&amp; conn);    EventLoop* loop_; // 运行在那个loop    ConnectorPtr connector_; // avoid revealing Connector // 连接器    const string name_; // TcpClient名    ConnectionCallback connectionCallback_;   // 连接建立和断开回调    MessageCallback messageCallback_;   // 可读回调    WriteCompleteCallback writeCompleteCallback_;   // 写完回调    bool retry_;   // atomic  重连    bool connect_; // atomic  // 已经连接？    // always in loop thread    int nextConnId_;  // 字面意思    mutable MutexLock mutex_;    TcpConnectionPtr connection_ GUARDED_BY(mutex_);  // 连接读写管理器&#125;;\n\nTcpClient核心函数TcpClient::newConnection，该函数会作为连接器的回调，当sockfd连接成功后，该函数被调用，设置必要信息后，为该sockfd产生一个TcpConnection对象，后续该fd的读写，全权交由TcpConnection处理。逻辑比较简单，实现如下：\n实现的伪代码：\nTcpClient::TcpClient(EventLoop* loop,                     const InetAddress&amp; serverAddr,                     const string&amp; nameArg)  : loop_(CHECK_NOTNULL(loop)),    connector_(new Connector(loop, serverAddr)),    name_(nameArg),    connectionCallback_(defaultConnectionCallback),    messageCallback_(defaultMessageCallback),    retry_(false),    connect_(true),    nextConnId_(1)&#123;        connector_-&gt;setNewConnectionCallback(        std::bind(&amp;TcpClient::newConnection, this, _1));    // FIXME setConnectFailedCallback    LOG_INFO &lt;&lt; &quot;TcpClient::TcpClient[&quot; &lt;&lt; name_            &lt;&lt; &quot;] - connector &quot; &lt;&lt; get_pointer(connector_);&#125;void TcpClient::connect()&#123;    // FIXME: check state    LOG_INFO &lt;&lt; &quot;TcpClient::connect[&quot; &lt;&lt; name_ &lt;&lt; &quot;] - connecting to &quot;            &lt;&lt; connector_-&gt;serverAddress().toIpPort();    connect_ = true;    connector_-&gt;start();&#125;void TcpClient::disconnect()&#123;    connect_ = false;    &#123;        MutexLockGuard lock(mutex_);        if (connection_)&#123;            connection_-&gt;shutdown();        &#125;    &#125;&#125;void TcpClient::stop()&#123;    connect_ = false;    connector_-&gt;stop();&#125;void TcpClient::newConnection(int sockfd)&#123;    loop_-&gt;assertInLoopThread();    InetAddress peerAddr(sockets::getPeerAddr(sockfd));    char buf[32];    snprintf(buf, sizeof buf, &quot;:%s#%d&quot;, peerAddr.toIpPort().c_str(), nextConnId_);    ++nextConnId_;    string connName = name_ + buf;    InetAddress localAddr(sockets::getLocalAddr(sockfd));    // FIXME poll with zero timeout to double confirm the new connection    // FIXME use make_shared if necessary    TcpConnectionPtr conn(new TcpConnection(loop_,                                            connName,                                            sockfd,                                            localAddr,                                            peerAddr));    conn-&gt;setConnectionCallback(connectionCallback_);    conn-&gt;setMessageCallback(messageCallback_);    conn-&gt;setWriteCompleteCallback(writeCompleteCallback_);    conn-&gt;setCloseCallback(        std::bind(&amp;TcpClient::removeConnection, this, _1)); // FIXME: unsafe    &#123;        MutexLockGuard lock(mutex_);        connection_ = conn;    &#125;    conn-&gt;connectEstablished(); // 同一loop，可以直接调用&#125;void TcpClient::removeConnection(const TcpConnectionPtr&amp; conn)&#123;    loop_-&gt;assertInLoopThread();    assert(loop_ == conn-&gt;getLoop());    &#123;        MutexLockGuard lock(mutex_);        assert(connection_ == conn);        connection_.reset();    &#125;    loop_-&gt;queueInLoop(std::bind(&amp;TcpConnection::connectDestroyed, conn));    if (retry_ &amp;&amp; connect_)&#123;    LOG_INFO &lt;&lt; &quot;TcpClient::connect[&quot; &lt;&lt; name_ &lt;&lt; &quot;] - Reconnecting to &quot;                &lt;&lt; connector_-&gt;serverAddress().toIpPort();    connector_-&gt;restart();    &#125;&#125;\n\n细节明细：疑问\n在TcpConnection::handleClose()实现当中，为什么没有调用close，关闭sockfd？也看了一下TcpConnection的析构、TcpConnection::connectDestroyed()，没有一个地方调用了close来关闭sockfd\n解答\n在 TcpConnection 对象析构的时候。TcpConnection 持有一个 Socket 对象，Socket 是一个 RAII handler，它的析构函数会 close(sockfd_)。这样，如果发生 TcpConnection 对象泄漏，那么我们从 &#x2F;proc&#x2F;pid&#x2F;fd&#x2F; 就能找到没有关闭的文件描述符，便于查错。\n原文链接：https://blog.csdn.net/Solstice/article/details/6208634\n总结Muduo设计的TcpServer和TcpClient代码思想及其统一，一些算法题也是需要这样的抽象思维，所以我认为这也是以后从事it最重要的品质，可以避免很多不必要的bug。\n","tags":["高性能服务器框架"]},{"title":"muduo源码阅读笔记（9、TcpServer）","url":"/2024/01/18/muduo/TcpServer/","content":"Muduo源码笔记系列：\nmuduo源码阅读笔记（0、下载编译muduo）\nmuduo源码阅读笔记（1、同步日志）\nmuduo源码阅读笔记（2、对C语言原生的线程安全以及同步的API的封装）\nmuduo源码阅读笔记（3、线程和线程池的封装）\nmuduo源码阅读笔记（4、异步日志）\nmuduo源码阅读笔记（5、Channel和Poller）\nmuduo源码阅读笔记（6、ExevntLoop和Thread）\nmuduo源码阅读笔记（7、EventLoopThreadPool）\nmuduo源码阅读笔记（8、定时器TimerQueue）\nmuduo源码阅读笔记（9、TcpServer）\nmuduo源码阅读笔记（10、TcpConnection）\n前言\n本章涉及的文件有：\n\nTcpServer.h&#x2F;cc：一个主从Reactor模型的TcpServer，主EventLoop接收连接，并且将连接sock fd负载均衡分发给一个IOLoop。\n\nAcceptor.h&#x2F;cc：一个监听套接字的包装器，内部创建了一个Channel管理连接套接字的回调。\n\nSocket.h&#x2F;cc：封装原生socket，提供绑定、监听、接受连接、设置socket属性等接口。\n\nSocketsOps.h&#x2F;cc：Socket.h&#x2F;cc接口的底层实现，在创建套接字（::socket()&#x2F;::accept()）时，会将socketfd设置为非阻塞。\n\nInetAddress.h&#x2F;cc：对sockaddr_in&#x2F;sockaddr_in6网络地址进行封装，使其更方便使用。\n\n\n本章重点集中在1、2，对于3、4、5，见名知意即可，感兴趣的读者，可以自行深入阅读。\nAcceptor的实现提供的接口：\n\n\n////// Acceptor of incoming TCP connections.///class Acceptor : noncopyable&#123;public:    typedef std::function&lt;void (int sockfd, const InetAddress&amp;)&gt; NewConnectionCallback;    Acceptor(EventLoop* loop, const InetAddress&amp; listenAddr, bool reuseport);    ~Acceptor();    void setNewConnectionCallback(const NewConnectionCallback&amp; cb)    &#123; newConnectionCallback_ = cb; &#125;    void listen();    bool listening() const &#123; return listening_; &#125;    // Deprecated, use the correct spelling one above.    // Leave the wrong spelling here in case one needs to grep it for error messages.    // bool listenning() const &#123; return listening(); &#125;private:    void handleRead();    EventLoop* loop_; // 绑定在哪个EventLoop上    Socket acceptSocket_; // accept sock fd    Channel acceptChannel_; // 对accept sock fd channel的封装    NewConnectionCallback newConnectionCallback_;   // 连接sock fd分发器    bool listening_;  // accept sock fd已经listen？    int idleFd_;  // 预留一个fd，以免文件描述符消耗完毕，无法继续处理连接。&#125;;\n\n其实从成员变量就可以看出来，Acceptor和TimeQueue有着及其相似的地方。\n实现的伪代码：\nAcceptor::Acceptor(EventLoop* loop, const InetAddress&amp; listenAddr, bool reuseport)  : loop_(loop),    acceptSocket_(sockets::createNonblockingOrDie(listenAddr.family())),    acceptChannel_(loop, acceptSocket_.fd()),    listening_(false),    idleFd_(::open(&quot;/dev/null&quot;, O_RDONLY | O_CLOEXEC))&#123;    assert(idleFd_ &gt;= 0);    acceptSocket_.setReuseAddr(true);       // 地址重用    acceptSocket_.setReusePort(reuseport);  // 端口重用    acceptSocket_.bindAddress(listenAddr);  // 地址绑定    acceptChannel_.setReadCallback(         // 设置回调        std::bind(&amp;Acceptor::handleRead, this));&#125;Acceptor::~Acceptor()&#123;    acceptChannel_.disableAll();    acceptChannel_.remove();    ::close(idleFd_);&#125;void Acceptor::listen()&#123;    loop_-&gt;assertInLoopThread();    listening_ = true;    acceptSocket_.listen(); // sock fd 开始listen    acceptChannel_.enableReading(); // 向Poller注册sock fd&#125;void Acceptor::handleRead()&#123;    loop_-&gt;assertInLoopThread();    InetAddress peerAddr;    //FIXME loop until no more    int connfd = acceptSocket_.accept(&amp;peerAddr);    if (connfd &gt;= 0)&#123;        // string hostport = peerAddr.toIpPort();        // LOG_TRACE &lt;&lt; &quot;Accepts of &quot; &lt;&lt; hostport;        if (newConnectionCallback_)&#123;            newConnectionCallback_(connfd, peerAddr);   // 连接分发器        &#125;else&#123;            sockets::close(connfd);        &#125;    &#125;else&#123;        LOG_SYSERR &lt;&lt; &quot;in Acceptor::handleRead&quot;;        // Read the section named &quot;The special problem of        // accept()ing when you can&#x27;t&quot; in libev&#x27;s doc.        // By Marc Lehmann, author of libev.        if (errno == EMFILE) &#123; // 超过文件描述符最大限制            ::close(idleFd_);   // 归还预留的idleFd_，利用idleFd_来接收连接            idleFd_ = ::accept(acceptSocket_.fd(), NULL, NULL); // 接受连接            // 什么也不做就关闭连接。            ::close(idleFd_);            idleFd_ = ::open(&quot;/dev/null&quot;, O_RDONLY | O_CLOEXEC);    // 继续预留fd。        &#125;    &#125;&#125;\n\n细节明细：疑问\nAcceptor::idleFd_成员变量存在的意义？Acceptor::handleRead函数中为什么出现EMFILE错误时，关闭idleFd_，::accept接受连接，又关闭idleFd_，又打开idleFd_？\n解答\n出现EMFILE错误，关闭 idleFd_ 后，执行 ::accept 操作，通常情况下这个操作并不会失败。这是因为在 ::accept 函数成功返回时，会返回一个值和关闭idleFd_前值相同的连接文件描述符，这个文件描述符会被用于处理新连接。这里主要目的是消耗一个连接，尽管什么也不做。但可以确保服务器能够继续正常运行。\nTcpServer的实现提供的接口：\n////// TCP server, supports single-threaded and thread-pool models.////// This is an interface class, so don&#x27;t expose too much details.class TcpServer : noncopyable&#123;public:    typedef std::function&lt;void(EventLoop*)&gt; ThreadInitCallback;    enum Option&#123;        kNoReusePort,        kReusePort,    &#125;;    TcpServer(EventLoop* loop,            const InetAddress&amp; listenAddr,            const string&amp; nameArg,            Option option = kNoReusePort);    ~TcpServer();  // force out-line dtor, for std::unique_ptr members.    const string&amp; ipPort() const &#123; return ipPort_; &#125;    const string&amp; name() const &#123; return name_; &#125;    EventLoop* getLoop() const &#123; return loop_; &#125;    // 必须再start前调用    void setThreadNum(int numThreads);    void setThreadInitCallback(const ThreadInitCallback&amp; cb)    &#123; threadInitCallback_ = cb; &#125;    /// valid after calling start()    std::shared_ptr&lt;EventLoopThreadPool&gt; threadPool()    &#123; return threadPool_; &#125;    /// Starts the server if it&#x27;s not listening.    ///    /// It&#x27;s harmless to call it multiple times.    /// Thread safe.    void start();    /// Set connection callback.    /// Not thread safe.    void setConnectionCallback(const ConnectionCallback&amp; cb)    &#123; connectionCallback_ = cb; &#125;    /// Set message callback.    /// Not thread safe.    void setMessageCallback(const MessageCallback&amp; cb)    &#123; messageCallback_ = cb; &#125;    /// Set write complete callback.    /// Not thread safe.    void setWriteCompleteCallback(const WriteCompleteCallback&amp; cb)    &#123; writeCompleteCallback_ = cb; &#125;private:    /// Not thread safe, but in loop    void newConnection(int sockfd, const InetAddress&amp; peerAddr);    /// Thread safe.    void removeConnection(const TcpConnectionPtr&amp; conn);    /// Not thread safe, but in loop    void removeConnectionInLoop(const TcpConnectionPtr&amp; conn);    typedef std::map&lt;string, TcpConnectionPtr&gt; ConnectionMap;    EventLoop* loop_;  // the acceptor loop    const string ipPort_; // ip:port    const string name_; // TcpServer Name    std::unique_ptr&lt;Acceptor&gt; acceptor_; // avoid revealing Acceptor    std::shared_ptr&lt;EventLoopThreadPool&gt; threadPool_; // 线程池    ConnectionCallback connectionCallback_; // 连接建立和断开回调    MessageCallback messageCallback_;   // 读数据回调    WriteCompleteCallback writeCompleteCallback_;   // 数据发送完毕的回调    ThreadInitCallback threadInitCallback_;    AtomicInt32 started_; // TcpServer启动了？    // always in loop thread    int nextConnId_;  // 连接计数器    ConnectionMap connections_; // 连接信息维护&#125;;\n\n简单画了一下TcpServer整体架构图：\n\n整体流程就是：\n\n客户端发送连接请求。\n\nlisten套接字所在的EventLoop（假设为base loop）接受连接请求并创建io套接字。\n\nbase loop 通过EventLoopThreadPool的负载均衡算法选择一个io EventLoop，将io套接字传给该loop。\n\n客户端和指定的loop进行TCP通信。\n\n\n\n实现伪代码：\nTcpServer::TcpServer(EventLoop* loop,                     const InetAddress&amp; listenAddr,                     const string&amp; nameArg,                     Option option)  : loop_(CHECK_NOTNULL(loop)),    ipPort_(listenAddr.toIpPort()),    name_(nameArg),    acceptor_(new Acceptor(loop, listenAddr, option == kReusePort)),    threadPool_(new EventLoopThreadPool(loop, name_)),    connectionCallback_(defaultConnectionCallback),    messageCallback_(defaultMessageCallback),    nextConnId_(1)&#123;    acceptor_-&gt;setNewConnectionCallback(    // 设定连接分发器        std::bind(&amp;TcpServer::newConnection, this, _1, _2));    &#125;TcpServer::~TcpServer()&#123;    loop_-&gt;assertInLoopThread();    LOG_TRACE &lt;&lt; &quot;TcpServer::~TcpServer [&quot; &lt;&lt; name_ &lt;&lt; &quot;] destructing&quot;;    for (auto&amp; item : connections_)&#123;        TcpConnectionPtr conn(item.second);        item.second.reset();    // 释放引用计数        conn-&gt;getLoop()-&gt;runInLoop( // 在自己的loop中执行连接销毁。            std::bind(&amp;TcpConnection::connectDestroyed, conn));    &#125;&#125;// 在start前调用void TcpServer::setThreadNum(int numThreads)&#123;    assert(0 &lt;= numThreads);    threadPool_-&gt;setThreadNum(numThreads);&#125;void TcpServer::start()&#123;    if (started_.getAndSet(1) == 0)&#123;    threadPool_-&gt;start(threadInitCallback_);    assert(!acceptor_-&gt;listening());    loop_-&gt;runInLoop(        std::bind(&amp;Acceptor::listen, get_pointer(acceptor_)));  // get_pointer(acceptor_)获取裸指针。    &#125;&#125;void TcpServer::newConnection(int sockfd, const InetAddress&amp; peerAddr)&#123;    loop_-&gt;assertInLoopThread();    EventLoop* ioLoop = threadPool_-&gt;getNextLoop();    char buf[64];    snprintf(buf, sizeof buf, &quot;-%s#%d&quot;, ipPort_.c_str(), nextConnId_);    ++nextConnId_;    string connName = name_ + buf;    LOG_INFO &lt;&lt; &quot;TcpServer::newConnection [&quot; &lt;&lt; name_            &lt;&lt; &quot;] - new connection [&quot; &lt;&lt; connName            &lt;&lt; &quot;] from &quot; &lt;&lt; peerAddr.toIpPort();    InetAddress localAddr(sockets::getLocalAddr(sockfd));    // FIXME poll with zero timeout to double confirm the new connection    // FIXME use make_shared if necessary    TcpConnectionPtr conn(new TcpConnection(ioLoop,                                            connName,                                            sockfd,                                            localAddr,                                            peerAddr));    connections_[connName] = conn;    conn-&gt;setConnectionCallback(connectionCallback_);    conn-&gt;setMessageCallback(messageCallback_);    conn-&gt;setWriteCompleteCallback(writeCompleteCallback_);    conn-&gt;setCloseCallback(        std::bind(&amp;TcpServer::removeConnection, this, _1)); // FIXME: unsafe    ioLoop-&gt;runInLoop(std::bind(&amp;TcpConnection::connectEstablished, conn));&#125;void TcpServer::removeConnection(const TcpConnectionPtr&amp; conn)&#123;    // FIXME: unsafe    loop_-&gt;runInLoop(std::bind(&amp;TcpServer::removeConnectionInLoop, this, conn));    // 尽管conn是引用，bind包装后也会增加引用计数&#125;void TcpServer::removeConnectionInLoop(const TcpConnectionPtr&amp; conn)&#123;    loop_-&gt;assertInLoopThread();    LOG_INFO &lt;&lt; &quot;TcpServer::removeConnectionInLoop [&quot; &lt;&lt; name_            &lt;&lt; &quot;] - connection &quot; &lt;&lt; conn-&gt;name();    size_t n = connections_.erase(conn-&gt;name());    (void)n;    assert(n == 1);    EventLoop* ioLoop = conn-&gt;getLoop();    ioLoop-&gt;queueInLoop(        std::bind(&amp;TcpConnection::connectDestroyed, conn));&#125;\n\n这里简单备忘一下\n连接建立的回调过程：\n\nbase loop中listen套接字触发可读事件，调用Acceptor::handleRead函数处理事件（由Acceptor::acceptChannel_注册）\n\n调用::accept接受连接并创建sockfd。将sockfd作为参数，调用Acceptor::newConnectionCallback_连接分发回调，也即TcpServer::newConnection（由TcpServer构造函数设置）\n\n利用负载均衡算法，选择一个合适的ioloop，然后为连接拼接一个唯一的connect name，并用sockfd、ioloop等创建一个TcpConnection对象（智能指针），设置好回调。将 &lt;key : connect name, value : TcpConnection&gt; 作为TcpServer::connections_的一个记录（TcpConnection对象引用计数加一）。最后向ioloop的任务队列中添加一项回调任务：TcpConnection::connectEstablished，并将TcpConnection对象作为回调任务的参数。\n\n执行TcpConnection::connectEstablished，连接建立。\n\n\n连接拆除的回调过程：\n\n调用TcpConnection::closeCallback_回调，即TcpServer::removeConnection，传入TcpConnection对象（引用）作为参数。\n\n向base loop任务队列添加一项回调任务：TcpServer::removeConnectionInLoop，传入TcpConnection对象（引用）作为参数。\n\n执行TcpServer::removeConnectionInLoop：通过TcpConnection对象的name，在TcpServer::connections_上删除连接记录（智能指针引用计数减一），然后向TcpConnection对象的ioloop的任务队列添加一项回调任务：TcpConnection::connectDestroyed，同样以TcpConnection对象作为回调任务的参数。\n\n\n本章完结\n","tags":["高性能服务器框架"]},{"title":"muduo源码阅读笔记（3、线程和线程池的封装）","url":"/2024/01/12/muduo/ThreadAndThreadPool/","content":"muduo源码阅读笔记（0、下载编译muduo）\nmuduo源码阅读笔记（1、同步日志）\nmuduo源码阅读笔记（2、对C语言原生的线程安全以及同步的API的封装）\nmuduo源码阅读笔记（3、线程和线程池的封装）\nmuduo源码阅读笔记（4、异步日志）\nmuduo源码阅读笔记（5、Channel和Poller）\nmuduo源码阅读笔记（6、ExevntLoop和Thread）\nmuduo源码阅读笔记（7、EventLoopThreadPool）\nmuduo源码阅读笔记（8、定时器TimerQueue）\nmuduo源码阅读笔记（9、TcpServer）\nmuduo源码阅读笔记（10、TcpConnection）\n闲聊\nMuduo对线程和线程池的封装，涉及得到源码也不多，大概加一起300多行，这部分读者可以好好精读一下。\n线程在阅读cpp的源码，分析一个类具体的实现的时候，首先应该看类的.h文件，主要看类的成员变量有哪些，毕竟成员函数，就是对成员变量进行代码层面的操作的。\n提供的接口：\n\nclass Thread : noncopyable &#123;public:    typedef std::function&lt;void ()&gt; ThreadFunc;    explicit Thread(ThreadFunc, const string&amp; name = string());    // FIXME: make it movable in C++11    ~Thread();    void start();    int join(); // return pthread_join()    bool started() const &#123; return started_; &#125;    // pthread_t pthreadId() const &#123; return pthreadId_; &#125;    pid_t tid() const &#123; return tid_; &#125;    const string&amp; name() const &#123; return name_; &#125;    static int numCreated() &#123; return numCreated_.get(); &#125;private:    void setDefaultName();    bool       started_;    // 线程正在运行？    bool       joined_;     // 有其他线程对该线程join？    pthread_t  pthreadId_;  // Posix中的线程id    pid_t      tid_;        // 线程真实的id（进程id）    ThreadFunc func_;       // 线程回调函数    string     name_;       // 线程名    CountDownLatch latch_;  // 在父线程继续运行前，确保创建的子线程正在运行    static AtomicInt32 numCreated_; // 对已经创建的线程计个数，所有线程对象共享的变量，线程默认名可能会用到&#125;;\n\n线程启动流程：\n\nThread::start()  -&gt; \n\npthread_create(…, &amp;detail::startThread,…)    -&gt; \n\nstartThread(void* obj)   -&gt;\n\nThreadData::runInThread()    -&gt; \n\nThread::func_()\n\n\n\n\n\n\n\n\n\n实现的伪代码：\nstruct ThreadData &#123;    typedef muduo::Thread::ThreadFunc ThreadFunc;    ThreadFunc func_;    string name_;    pid_t* tid_;    CountDownLatch* latch_;    ThreadData(ThreadFunc func,                const string&amp; name,                pid_t* tid,                CountDownLatch* latch)    : func_(std::move(func)),        name_(name),        tid_(tid),        latch_(latch)    &#123; &#125;    void runInThread()&#123;        *tid_ = muduo::CurrentThread::tid();  // 获取&amp;&amp;缓存tid，并赋值给Thread::tid_对象        tid_ = NULL;  // 取消对Thread::tid_的指向，保证安全        latch_-&gt;countDown();  // 倒计数器减为0（初始化为1），通知父线程，子线程启动完毕        latch_ = NULL;  // 取消对Thread::latch__的指向，保证安全        muduo::CurrentThread::t_threadName = name_.empty() ? &quot;muduoThread&quot; : name_.c_str(); // 将线程名赋值给线程全局变量        ::prctl(PR_SET_NAME, muduo::CurrentThread::t_threadName); // 设置真正的线程的进程名。        try&#123;            func_();  // 真正开始执行Thread的回调函数            muduo::CurrentThread::t_threadName = &quot;finished&quot;;        &#125;        catch (...)&#123;            // 异常处理...        &#125;    &#125;&#125;;/** pthread_create回调函数*/void* startThread(void* obj)&#123;    ThreadData* data = static_cast&lt;ThreadData*&gt;(obj);    data-&gt;runInThread();    delete data;    return NULL;&#125;Thread::Thread(ThreadFunc func, const string&amp; n)  : started_(false),    joined_(false),    pthreadId_(0),    tid_(0),    func_(std::move(func)),    name_(n),    latch_(1)&#123;    setDefaultName();&#125;Thread::~Thread()&#123;    /*    * 如果线程启动了并且没有任何线程join过该线程，就调用pthread_detach将该线程分    * 离，接触父子关系，让该线程自生自灭。    */    if (started_ &amp;&amp; !joined_)&#123;        pthread_detach(pthreadId_);    &#125;&#125;void Thread::setDefaultName()&#123;    int num = numCreated_.incrementAndGet();  // 原子自增    if (name_.empty())&#123;  // 如果用户没有设置name        char buf[32];        snprintf(buf, sizeof buf, &quot;Thread%d&quot;, num); // 利用numCreated_构造一个唯一的name        name_ = buf;    &#125;&#125;void Thread::start()&#123;    assert(!started_);    started_ = true;    /*    * ThreadData对象的创建是为了父子线程数据的初始化和传输，比如，Thread对象最开始被创建    * 在父线程，子线程要执行的回调函数，以及其线程名，而且，Thread对象的一些成员变量还要等    * 子线程启动后，才能得知，比如tid，此外，父子线程同步也要依附子线程的启动。    */    // FIXME: move(func_)    detail::ThreadData* data = new detail::ThreadData(func_, name_, &amp;tid_, &amp;latch_);    if (pthread_create(&amp;pthreadId_, NULL, &amp;detail::startThread, data))&#123;        started_ = false;        delete data; // or no delete?        LOG_SYSFATAL &lt;&lt; &quot;Failed in pthread_create&quot;;    &#125;else&#123;        latch_.wait();  // 父子线程的同步，在子线程启动后，会调用latch_.down()唤醒父线程。        assert(tid_ &gt; 0);    &#125;&#125;  /*  * 简单的调用join即可  */int Thread::join()&#123;    assert(started_);    assert(!joined_);    joined_ = true;    return pthread_join(pthreadId_, NULL);&#125;\n\n细节明细：疑问：\nMuduo网络库在封装Thread时，为什么不直接向子线程传递Thread对象本身，反而去创建一个ThreadData对象去传递呢？\n解答：\nMuduo网络库在封装Thread时选择创建一个ThreadData对象而不是直接向子线程传递Thread对象本身，有一些合理的设计考虑：\n\n线程安全性： 直接向子线程传递Thread对象可能会引入线程安全的问题。Thread对象的生命周期和线程的执行是相关联的，如果在子线程中直接访问或修改Thread对象，可能导致竞态条件和不确定的行为。通过ThreadData的设计，可以更好地封装线程私有数据，确保线程安全性。\n\n封装线程私有数据： ThreadData的存在允许封装线程私有数据，这些数据对于特定线程是独立的。如果直接传递Thread对象，就需要确保Thread对象的成员变量在多线程环境下的正确性和安全性，而通过ThreadData可以更容易实现这一点。\n\n用户数据传递： ThreadData允许用户在创建线程时传递额外的用户数据。这使得用户可以通过ThreadData传递一些上下文信息，而不必直接依赖Thread对象。\n\n解耦设计： 通过ThreadData的设计，Thread类的内部实现与线程的具体执行逻辑解耦。Thread对象可以专注于线程的管理，而线程的执行逻辑则通过ThreadData实现，提高了代码的模块化和可维护性。\n\n\nMuduo中，对Posix线程调用fork函数的处理:\n背景： 我有写过一个deamo，结果表明，posix线程在调用fork后，子进程并不会复制父进程的所有线程，即子进程只有一个线程（也是子线程的主线程），该线程就是父进程调用fork函数的子线程的上下文复制版。\nMuduo中的处理： Muduo设计了一个ThreadNameInitializer类，该类定义了一个全局对象，在程序创建时，构造函数会调用pthread_atfork函数，设置了chile回调afterFork()，在子进程被创建时，系统会调用afterFork()回调，重新设置子进程中主线程的线程局部变量包括：t_cachedTid、t_threadName等。\n回调代码如下：\nvoid afterFork()&#123;    muduo::CurrentThread::t_cachedTid = 0;    muduo::CurrentThread::t_threadName = &quot;main&quot;;    CurrentThread::tid();    // no need to call pthread_atfork(NULL, NULL, &amp;afterFork);&#125;\n\n线程池Muduo设计的线程池，可以直接当模板来使用，设计的非常精妙。不仅线程池的初始化和运行，而且线程池的析构停止做的也非常清晰。\n架构图：简单画一下线程池的架构图吧：\n\n线程池的每个线程执行的线程回调，都是处于一个while循环中，循环往复的执行：\n\n到任务队列取回调任务。\n执行回调任务。\n回到1。\n\n当然，如果线程池停止了，就会跳出循环。\n提供的接口：\nclass ThreadPool : noncopyable&#123;public:    typedef std::function&lt;void ()&gt; Task;    explicit ThreadPool(const string&amp; nameArg = string(&quot;ThreadPool&quot;));    ~ThreadPool();    // Must be called before start().    void setMaxQueueSize(int maxSize) &#123; maxQueueSize_ = maxSize; &#125;    void setThreadInitCallback(const Task&amp; cb)    &#123; threadInitCallback_ = cb; &#125;    void start(int numThreads);    void stop();    const string&amp; name() const    &#123; return name_; &#125;    size_t queueSize() const;    // Could block if maxQueueSize &gt; 0    // Call after stop() will return immediately.    // There is no move-only version of std::function in C++ as of C++14.    // So we don&#x27;t need to overload a const&amp; and an &amp;&amp; versions    // as we do in (Bounded)BlockingQueue.    // https://stackoverflow.com/a/25408989    void run(Task f);private:    bool isFull() const REQUIRES(mutex_);    void runInThread();    Task take();    mutable MutexLock mutex_;                       // 线程池全局互斥锁    Condition notEmpty_ GUARDED_BY(mutex_);         // 等待任务队列非空的条件变量    Condition notFull_ GUARDED_BY(mutex_);          // 等待任务队列未满的条件变量    string name_;                                   //线程池的名，线程名就是依据它来拼接    Task threadInitCallback_;                       // 线程池中，线程共享的线程初始化的回调    std::vector&lt;std::unique_ptr&lt;muduo::Thread&gt;&gt; threads_; // 线程池本体，除了调用start时会写，其他时期都是读，可以不受mutex保护    std::deque&lt;Task&gt; queue_ GUARDED_BY(mutex_);     // 任务队列    size_t maxQueueSize_;                           //任务队列中，最多存放任务数，不受互斥锁保护。（原生数据类型在很少去写的情况下（可能不太严格，但是为了性能，也无所谓）看成是原子的）    bool running_;                                  // 线程池是否正在运行，不受互斥锁保护。（原生数据类型在很少去写的情况下（可能不太严格，但是为了性能，也无所谓）看成是原子的）&#125;;\n\n注意：\n线程池中，在调用ThreadPool::start()启动线程池之前，必须先调用ThreadPool::setMaxQueueSize来设定任务队列的最大任务数。\n实现的伪代码：\nThreadPool::ThreadPool(const string&amp; nameArg)  : mutex_(),    notEmpty_(mutex_),    notFull_(mutex_),    name_(nameArg),    maxQueueSize_(0),    running_(false)&#123;&#125;ThreadPool::~ThreadPool()&#123;    /*    * 如果线程池在析构时，没有停止，就调用stop()回收。    */    if (running_)&#123;        stop();    &#125;&#125;/** 调用start前先设置任务队列的最大任务数。* 参数：提供创建线程池的线程数*/void ThreadPool::start(int numThreads)&#123;    assert(threads_.empty());    running_ = true;    threads_.reserve(numThreads);   // 提前预留好内存    for (int i = 0; i &lt; numThreads; ++i)&#123;   // 创建numThreads个线程，并起名。        char id[32];        snprintf(id, sizeof id, &quot;%d&quot;, i+1);        threads_.emplace_back(new muduo::Thread(                std::bind(&amp;ThreadPool::runInThread, this), name_+id));        threads_[i]-&gt;start();   // 启动    &#125;    if (numThreads == 0 &amp;&amp; threadInitCallback_)&#123;        // 如果numThreads为零，就让父线程作为任务执行线程，并调用线程初始化函数。        threadInitCallback_();    &#125;&#125;void ThreadPool::stop()&#123;    &#123;        MutexLockGuard lock(mutex_);        running_ = false;        notEmpty_.notifyAll();  // 唤醒所有调用notFull_.wait()的函数        notFull_.notifyAll();   // 唤醒所有调用notEmpty_.wait()的函数    &#125;    for (auto&amp; thr : threads_)&#123;        // 除了调用start时会写，其他时期都是读，可以不受mutex保护        thr-&gt;join();    &#125;&#125;size_t ThreadPool::queueSize() const&#123;    MutexLockGuard lock(mutex_);    return queue_.size();&#125;void ThreadPool::run(Task task)&#123;    if (threads_.empty())&#123;  // 既然无线程可用，线程就自己执行任务。        task();    &#125;else&#123;        MutexLockGuard lock(mutex_);        while (isFull() &amp;&amp; running_)&#123;            // while解决惊群，可能会被take（任务队列未满）或stop（线程池停止）唤醒            notFull_.wait();        &#125;        if (!running_) return;        assert(!isFull());        queue_.push_back(std::move(task));  // 存任务        notEmpty_.notify(); // 任务队列至少有一个任务，所以非空，调用一下notEmpty_.notify();    &#125;&#125;ThreadPool::Task ThreadPool::take()&#123;    MutexLockGuard lock(mutex_);  // always use a while-loop, due to spurious wakeup    while (queue_.empty() &amp;&amp; running_)&#123;        // while解决惊群，可能会被run（任务队列非空）或stop（线程池停止）唤醒        notEmpty_.wait();    &#125;    Task task;    if (!queue_.empty())&#123;   // 任务队列为空，就返回空任务，一般是调用stop停止线程池导致。        task = queue_.front();  // 取任务。        queue_.pop_front();     // pop任务        if (maxQueueSize_ &gt; 0)&#123;             /*            * 如果任务太多，达到了上限，如果其他线程还想，加入任务，会阻塞在run函数的            * notFull_.wait()上。保证任务队列任务数量不超过上限，此时在take函数中占            * 锁取走了一个任务，任务队列一定未满，所以放心调用notFull_.notify()通知            * run函数即可。            */            notFull_.notify();          &#125;// else 若maxQueueSize_ == 0，说明任务队列大小无限，不存在执行notFull_.wait()分支的线程，无需调用notFull_.notify()。    &#125;    return task;&#125;bool ThreadPool::isFull() const&#123;    mutex_.assertLocked();    return maxQueueSize_ &gt; 0 &amp;&amp; queue_.size() &gt;= maxQueueSize_;&#125;void ThreadPool::runInThread()&#123;    try&#123;        if (threadInitCallback_)&#123;            threadInitCallback_();  // 执行一下初始化回调        &#125;        while (running_)&#123;   // 就算要停止了，如果任务队列里面还有任务不继续消化任务吗？            Task task(take());  // 取任务            if (task)&#123;  // 空任务一般发生在线程池停止阶段。                task(); // 执行任务。            &#125;        &#125;    &#125;catch (...)&#123;        // 异常处理...    &#125;&#125;\n\n细节明细：疑问：\nMuduo的线程池中为什么要设置一个maxQueueSize_成员来限制任务队列的大小？\n解答：\n在Muduo网络库的线程池中设置maxQueueSize_成员来限制任务队列的大小是为了防止无限制的任务积压，以保护系统的稳定性和资源管理。这样的设计有以下几个原因：\n\n资源控制： 通过设置任务队列的最大大小，可以控制线程池在高负载情况下的资源占用。如果不限制任务队列大小，当任务提交速度远远大于线程池处理速度时，可能会导致任务队列无限增长，消耗大量内存资源，最终导致系统资源耗尽。\n\n避免任务积压： 如果任务队列无限制增长，可能导致待处理的任务数量不断累积，进而导致系统的响应时间变长。通过设置最大队列大小，可以避免任务积压，确保系统对任务的响应是有限度的。\n\n反馈机制： 当任务队列达到最大大小时，新的任务可能会被拒绝或者触发一些警告机制。这样的反馈机制可以让开发者或者系统管理员及时感知到系统的负载情况，并采取相应的措施，如调整线程池大小、优化任务处理逻辑等。（尽管在Muduo中，在代码实现上并没有实现这一点）\n\n\n疑问：\n在ThreadPool::runInThread函数中，如果线程池要停止了（running_ &#x3D;&#x3D; false），假如任务队列里面还有任务不继续消化任务吗？我看Muduo的实现是，线程池停止，即使任务队列还有任务，ThreadPool::runInThread()也会直接跳出循环。为什么要这样设计？\n解答：\n线程池的停止有两种：\n\nGraceful Shutdown： 线程池的停止过程可能是优雅的，即允许已经在任务队列中的任务执行完毕，但不再接受新的任务。在这种情况下，可以通过设置running_为false来触发线程池停止，但允许已在队列中的任务继续执行。\n\n快速停止： 另一种设计考虑是快速停止，即立即停止线程的执行，无论任务队列中是否还有任务。这可能是为了迅速释放线程池占用的资源，例如在应用程序关闭时。在这种情况下，即使有未执行的任务，也可以选择快速停止线程池。\n\n\n具体选择采用哪种停止方式取决于应用程序的需求和设计目标。如果对任务的完成有严格的要求，可以选择优雅停止，确保所有任务得以执行。如果更注重迅速释放资源，可以选择快速停止。\n在Muduo中的设计选择了快速停止，一旦running_为false，即使任务队列中还有任务，线程也会直接退出。这种设计可能符合Muduo网络库的使用场景和性能需求。\nmaxQueueSize_ &#x3D;&#x3D; 0的特殊含义：\n在Muduo网络库中，仔细梳理线程池的源码逻辑可以发现，如果maxQueueSize_的大小被设置为0，表示任务队列的大小没有限制，即队列可以无限增长。ThreadPool::isFull函数会始终返回false，此时在调用ThreadPool::run向任务队列添加任务时，会无条件将任务添加到任务队列，而且ThreadPool::take函数中，由于maxQueueSize_ &#x3D;&#x3D; 0，也不会去调用notFull_.notify()通知阻塞在ThreadPool::run的线程，因为在maxQueueSize_ &#x3D;&#x3D; 0条件下根本不可能有线程会阻塞在ThreadPool::run中。\n\n本章完结\n","tags":["高性能服务器框架"]},{"title":"muduo源码阅读笔记（2、对C语言原生的线程安全以及同步的API的封装）","url":"/2024/01/11/muduo/ThreadSafeAndSync/","content":"Muduo源码笔记系列：\nmuduo源码阅读笔记（0、下载编译muduo）\nmuduo源码阅读笔记（1、同步日志）\nmuduo源码阅读笔记（2、对C语言原生的线程安全以及同步的API的封装）\nmuduo源码阅读笔记（3、线程和线程池的封装）\nmuduo源码阅读笔记（4、异步日志）\nmuduo源码阅读笔记（5、Channel和Poller）\nmuduo源码阅读笔记（6、ExevntLoop和Thread）\nmuduo源码阅读笔记（7、EventLoopThreadPool）\nmuduo源码阅读笔记（8、定时器TimerQueue）\nmuduo源码阅读笔记（9、TcpServer）\nmuduo源码阅读笔记（10、TcpConnection）\n闲聊\n首先感慨一句，muduo库对C语言原生的线程安全以及同步的API的封装，真的称得上是教科书式的，非常精妙、规范，很值得学习。\n读者在阅读muduo源码的时候，看到类定义的类名称被一些宏定义修饰、以及类的成员变量被一些宏定义修饰时，可以直接忽略，无视这些宏。因为这些东西的存在完全不影响整体的功能。简单来说就是吓唬人的。不仅如此，在看muduo以及其他的源码的时候，我们没必要转牛角尖，死扣细节，对于一个类，如果我们可以猜到他的功能以及怎么实现的，我们可以直接看他在源码中的使用即可，没必要在这细节上面浪费精力，专注整体架构，以及思想，不太过专注细节，才是阅读一份源码的正确套路。\n原子操作提到原子操作，不得不顺便提一下c++ std::atomic的原子操作以及它的内存序，这个知识点，以后的博客再来记录。\n这里是muduo对gcc提供的原子操作api的封装：\n\ntemplate&lt;typename T&gt;class AtomicIntegerT : noncopyable&#123;public:    AtomicIntegerT()    : value_(0)    &#123;    &#125;    T get()    &#123;        return __sync_val_compare_and_swap(&amp;value_, 0, 0);    &#125;    T getAndAdd(T x)    &#123;        return __sync_fetch_and_add(&amp;value_, x);    &#125;    T addAndGet(T x)    &#123;        return getAndAdd(x) + x;    &#125;    T incrementAndGet()    &#123;        return addAndGet(1);    &#125;    T decrementAndGet()    &#123;        return addAndGet(-1);    &#125;    void add(T x)    &#123;        getAndAdd(x);    &#125;    void increment()    &#123;        incrementAndGet();    &#125;    void decrement()    &#123;        decrementAndGet();    &#125;    T getAndSet(T newValue)    &#123;        return __sync_lock_test_and_set(&amp;value_, newValue);    &#125;private:    volatile T value_;&#125;;\n\n\n函数原型：type __sync_val_compare_and_swap(type *ptr, type oldval, type newval, ...)\n 参数：\n\ntype：被操作的数据类型，可以是整数类型、指针等。\nptr：要进行 CAS 操作的地址，通常是一个指针。\noldval：期望的旧值。\nnewval：新值。\n\n 描述：\n 该函数的作用是，如果 *ptr 的当前值等于 oldval，则将 *ptr 的值设置为 newval，并返回 *ptr 之前的值。如果 *ptr 的当前值不等于 oldval，则不进行任何操作，直接返回 *ptr 的当前值。\n 这样的操作是原子的，即在多线程环境下，不会被其他线程中断，确保了操作的一致性。CAS 操作通常用于实现锁、同步原语和非阻塞算法等。\n\n函数原型：type __sync_fetch_and_add(type *ptr, type value, ...)\n 参数：\n\ntype：被操作的数据类型，可以是整数类型、指针等。\nptr：要进行自增操作的地址，通常是一个指针。\nvalue：要添加到 *ptr 的值。\n\n 描述：\n 该函数的作用是，将 *ptr 的值与 value 相加，并返回 *ptr 之前的值。这个操作是原子的，确保在多线程环境下不会被其他线程中断，从而保证了操作的一致性。自增操作通常用于实现计数器等场景。\n\n函数原型：type __sync_lock_test_and_set(type *ptr, type value, ...)\n 参数：\n\ntype：被操作的数据类型，可以是整数类型、指针等。\nptr：要进行测试并设置的地址，通常是一个指针。\nvalue：将要设置到 *ptr 的值。\n\n 描述：\n 该函数的作用是，返回 *ptr 之前的值，并将 *ptr 的值设置为 value。这个操作是原子的，确保在多线程环境下不会被其他线程中断，从而保证了操作的一致性。\n\n\n互斥锁这里对互斥锁本身的科普就简要概括，主要专注muduo对Posix中的互斥锁的封装思想。\n互斥量资源的管理：\nclass CAPABILITY(&quot;mutex&quot;) MutexLock : noncopyable&#123;public:    MutexLock()    : holder_(0)    &#123;        MCHECK(pthread_mutex_init(&amp;mutex_, NULL));    &#125;    ~MutexLock()    &#123;        assert(holder_ == 0);        MCHECK(pthread_mutex_destroy(&amp;mutex_));    &#125;    // must be called when locked, i.e. for assertion    bool isLockedByThisThread() const    &#123;        return holder_ == CurrentThread::tid();    &#125;    void assertLocked() const ASSERT_CAPABILITY(this)    &#123;        assert(isLockedByThisThread());    &#125;    // internal usage    void lock() ACQUIRE()    &#123;        MCHECK(pthread_mutex_lock(&amp;mutex_));        assignHolder();    &#125;    void unlock() RELEASE()    &#123;        unassignHolder();        MCHECK(pthread_mutex_unlock(&amp;mutex_));    &#125;    pthread_mutex_t* getPthreadMutex() /* non-const */    &#123;        return &amp;mutex_;    &#125;private:    friend class Condition;    /*    * RAII机制，for条件变量    * 条件变量中，有详细解释其作用    */    class UnassignGuard : noncopyable    &#123;    public:        explicit UnassignGuard(MutexLock&amp; owner)            : owner_(owner)        &#123;            owner_.unassignHolder();        &#125;        ~UnassignGuard()        &#123;            owner_.assignHolder();        &#125;    private:        MutexLock&amp; owner_;    &#125;;    void unassignHolder()    &#123;        holder_ = 0;    &#125;    void assignHolder()    &#123;        holder_ = CurrentThread::tid();    &#125;    pthread_mutex_t mutex_;    pid_t holder_;&#125;;\n\n互斥锁加锁解锁的管理：\n/** RAII机制*/// Use as a stack variable, eg.// int Foo::size() const// &#123;//   MutexLockGuard lock(mutex_);//   return data_.size();// &#125;class SCOPED_CAPABILITY MutexLockGuard : noncopyable&#123;public:    explicit MutexLockGuard(MutexLock&amp; mutex) ACQUIRE(mutex)    : mutex_(mutex)    &#123;        mutex_.lock();    &#125;    ~MutexLockGuard() RELEASE()    &#123;        mutex_.unlock();    &#125;private:    MutexLock&amp; mutex_;&#125;;\n\n互斥锁加锁解锁的管理，使用了C++大名顶顶的RAII机制，\nRAII 的核心思想是： 在对象的构造函数中获取资源，在析构函数中释放资源。这种方法能够确保资源在对象的生命周期内得到正确的管理，从而避免了手动管理资源的繁琐和容易出错的问题。\n关键点：\n\n资源的获取和释放与对象的生命周期关联： 资源（如内存、文件句柄、网络连接等）的获取和释放被绑定到了对象的构造和析构过程中，确保资源在对象生命周期内正确地管理。\n\n构造函数中获取资源： 在对象的构造函数中，资源被获取。这意味着当对象被创建时，相应的资源就被分配或初始化。\n\n析构函数中释放资源： 在对象的析构函数中，资源被释放。这确保了在对象生命周期结束时，与之相关的资源会被正确释放。\n\n无需手动管理资源： 由于资源的获取和释放与对象的生命周期关联，程序员无需手动管理资源。当对象超出作用域或者被删除时，其析构函数会自动被调用，从而释放关联的资源。\n\n\n其他RAII应用的例子\n智能指针、文件处理类、数据库连接类等。\n条件变量muduo对条件变量本身的封装是没有解决惊群效应的，pthread_cond_wait函数没有放在while循环中。但是muduo在其他用到条件变量的地方，其实有利用while循环来解决惊群效应的。比如即将要聊到的CountDownLatch类的实现\nclass Condition : noncopyable&#123;public:    explicit Condition(MutexLock&amp; mutex)    : mutex_(mutex)    &#123;        MCHECK(pthread_cond_init(&amp;pcond_, NULL));    &#125;    ~Condition()    &#123;        MCHECK(pthread_cond_destroy(&amp;pcond_));    &#125;    void wait()    &#123;        /*        * 这里是raii机制的具体应用，因为MutexLock类里面有个成员变量holder_存储获取到        * mutex锁的线程id，每次线程对mutex加锁后就会将自己的tid赋值给holder_，而        * 在释放mutex锁前，会将holder_清零，以示当前mutex锁被哪个线程持有。而线程在等        * 待获取条件变量时，内部会原子加/解锁。所以为遵循holder_存在的意义，muduo为Condition        * 实现了UnassignGuard类，利用raii，在等待条件变量解锁前，在构造函数中，        * 将holder_清零；在获取到条件变量加锁后，在析构函数中，将holder_赋值为获锁线程        * 的tid。以此保证holder_严格随着获取mutex锁的线程变化。        */        MutexLock::UnassignGuard ug(mutex_);        MCHECK(pthread_cond_wait(&amp;pcond_, mutex_.getPthreadMutex()));    &#125;    // returns true if time out, false otherwise.    bool waitForSeconds(double seconds);    void notify()    &#123;        MCHECK(pthread_cond_signal(&amp;pcond_));    &#125;    void notifyAll()    &#123;        MCHECK(pthread_cond_broadcast(&amp;pcond_));    &#125;private:    MutexLock&amp; mutex_;    pthread_cond_t pcond_;&#125;;\n\n关于条件变量和信号量的使用上的差别，说老实话，就我目前的功力，还没有深刻的感受，这里先mark一下，等哪天领悟到之后，再来聊一聊。\n补充：\n\n条件变量可以在条件满足时，一次唤醒所有等待条件的线程，但是信号量则不行，只能post一个信号量（资源），唤醒一个线程。在多个线程等待一个条件的满足时再继续同时执行的场景下，适合用条件变量。（好像此时也可以用信号量，无非就是多post几次。只是，信号量不适合该场景，而条件变量更加适合）\n\nCountDownLatch（倒计数同步类）使用场景：\n\n父线程等待多个子线程启动完毕，再继续执行： 在某些并发场景中，可能需要等待多个子线程都完成某个初始化操作后，父线程才能继续执行。CountDownLatch 可以用来等待这些线程的完成。\n\n多个线程等待一个线程某个操作完毕，再继续执行： 可以使用 CountDownLatch 来协调多个线程的并发操作，确保某个操作在所有线程完成之后再执行。\n\n\n接口：\nclass CountDownLatch : noncopyable&#123;public:    explicit CountDownLatch(int count);    void wait();    void countDown();    int getCount() const;private:    mutable MutexLock mutex_;    Condition condition_ GUARDED_BY(mutex_);    int count_ GUARDED_BY(mutex_);&#125;;\n\n实现：\nCountDownLatch::CountDownLatch(int count)  : mutex_(),    condition_(mutex_),    count_(count)&#123;&#125;void CountDownLatch::wait()&#123;    MutexLockGuard lock(mutex_);    while (count_ &gt; 0)    &#123;   // while中解决了惊群效应        condition_.wait();    &#125;&#125;void CountDownLatch::countDown()&#123;    MutexLockGuard lock(mutex_);    --count_;    if (count_ == 0)    &#123;        // 减为零后，将所有处于条件等待队列的线程，移到枪锁等待队列。        condition_.notifyAll();    &#125;&#125;int CountDownLatch::getCount() const&#123;    MutexLockGuard lock(mutex_);    return count_;&#125;\n\n注意：我之前用一个demo专门实验过，实验结果表明，线程A调用pthread_cond_broadcast唤醒其他所有调用pthread_cond_wait阻塞的线程时，所有线程会处于一个枪锁状态（从条件等待队列，移到枪锁队列），线程B抢到锁处理临界资源再释放锁后，其他处于枪锁队列的线程还是处于枪锁状态，并不需要等待条件信号的到来，抢到锁就能处理临界资源。\n\n本章完结\n","tags":["高性能服务器框架"]},{"title":"muduo源码阅读笔记（8、定时器TimerQueue）","url":"/2024/01/17/muduo/TimerQueue/","content":"Muduo源码笔记系列：\nmuduo源码阅读笔记（0、下载编译muduo）\nmuduo源码阅读笔记（1、同步日志）\nmuduo源码阅读笔记（2、对C语言原生的线程安全以及同步的API的封装）\nmuduo源码阅读笔记（3、线程和线程池的封装）\nmuduo源码阅读笔记（4、异步日志）\nmuduo源码阅读笔记（5、Channel和Poller）\nmuduo源码阅读笔记（6、ExevntLoop和Thread）\nmuduo源码阅读笔记（7、EventLoopThreadPool）\nmuduo源码阅读笔记（8、定时器TimerQueue）\nmuduo源码阅读笔记（9、TcpServer）\nmuduo源码阅读笔记（10、TcpConnection）\n前言\n为了方便Poller的管理，Muduo定时器是基于文件描述符实现。\n实现定时器提供的接口：\n\nclass TimerQueue : noncopyable&#123;public:    explicit TimerQueue(EventLoop* loop);    ~TimerQueue();    ///    /// Schedules the callback to be run at given time,    /// repeats if @c interval &gt; 0.0.    ///    /// Must be thread safe. Usually be called from other threads.    TimerId addTimer(TimerCallback cb,                    Timestamp when,                    double interval);    void cancel(TimerId timerId);private:    // FIXME: use unique_ptr&lt;Timer&gt; instead of raw pointers.    // This requires heterogeneous comparison lookup (N3465) from C++14    // so that we can find an T* in a set&lt;unique_ptr&lt;T&gt;&gt;.    typedef std::pair&lt;Timestamp, Timer*&gt; Entry;    typedef std::set&lt;Entry&gt; TimerList;    typedef std::pair&lt;Timer*, int64_t&gt; ActiveTimer;    typedef std::set&lt;ActiveTimer&gt; ActiveTimerSet;    void addTimerInLoop(Timer* timer);    void cancelInLoop(TimerId timerId);    // called when timerfd alarms    void handleRead();    // move out all expired timers    std::vector&lt;Entry&gt; getExpired(Timestamp now);    void reset(const std::vector&lt;Entry&gt;&amp; expired, Timestamp now);    bool insert(Timer* timer);    EventLoop* loop_; // 定时器和哪个EventLoop关联    const int timerfd_; // timerfd_    Channel timerfdChannel_;  // 基于timerfd_的Channel    // Timer list sorted by expiration    TimerList timers_;  // 基于set的定时器（Timestamp，Timer*）    // for cancel()    ActiveTimerSet activeTimers_;    bool callingExpiredTimers_; /* atomic */    ActiveTimerSet cancelingTimers_;  // （Timer*，int64_t）&#125;;\n构造函数：\n在每个EventLoop创建时，在自己的构造函数中，创建自己的定时器TimerQueue，并将EventLoop的this指针作为TimerQueue构造函数的参数。TimerQueue的构造会创建一个timerfd，并且向EventLoop的Poller注册timerfd。这样，Poller正式开开始管理定时器。后面的Acceptor、TcpConnection使用了类似的手法。\n实现如下：\n/** @param: EventLoop的this指针*/TimerQueue::TimerQueue(EventLoop* loop)  : loop_(loop),    timerfd_(createTimerfd()),    timerfdChannel_(loop, timerfd_),    timers_(),    callingExpiredTimers_(false)&#123;  timerfdChannel_.setReadCallback(      std::bind(&amp;TimerQueue::handleRead, this));  // we are always reading the timerfd, we disarm it with timerfd_settime.  timerfdChannel_.enableReading();  // 向所在的loop中注册timerfd。&#125;\n\n关于&lt;号的万能性\n将自定义类存入std::set是要求用户实现自定义对象&lt;号重载的。思考一个问题：只重载&lt;的话，如果用户调用find成员函数时，set如何判断两个对象是否相等呢？\n其实std::set内部做两次比较即可判断两个对象是否相等。方法：当a &lt; b &#x3D;&#x3D; false &amp;&amp; b &lt; a &#x3D;&#x3D; false时，说明此时 a &#x3D;&#x3D; b。读者可以在这里仔细思考一下。Timestamp正是因为实现了&lt;才可以作为std::set的元素类型。\n一个自定义对象重载&lt;号后，不光可以通过&lt;推导出&#x3D;&#x3D;，还可以推到出&gt;、&gt;&#x3D;、&lt;&#x3D;号。参考博客\n参考boost::less_than_comparable的实现，如下：\n//已知：friend bool operator&lt;(const T&amp; x, const T&amp; y)  &#123; /*...*/&#125;// |// V//可以推导：friend bool operator&gt;(const T&amp; x, const T&amp; y)  &#123; return y &lt; x; &#125;friend bool operator&lt;=(const T&amp; x, const T&amp; y) &#123; return !static_cast&lt;bool&gt;(y &lt; x); &#125;friend bool operator&gt;=(const T&amp; x, const T&amp; y) &#123; return !static_cast&lt;bool&gt;(x &lt; y); &#125;\n\n定时器实现的伪代码：\nTimerId TimerQueue::addTimer(TimerCallback cb,                             Timestamp when,                             double interval)&#123;    Timer* timer = new Timer(std::move(cb), when, interval);    loop_-&gt;runInLoop(        std::bind(&amp;TimerQueue::addTimerInLoop, this, timer));    return TimerId(timer, timer-&gt;sequence());&#125;void TimerQueue::cancel(TimerId timerId)&#123;    loop_-&gt;runInLoop(        std::bind(&amp;TimerQueue::cancelInLoop, this, timerId));&#125;void TimerQueue::addTimerInLoop(Timer* timer)&#123;    loop_-&gt;assertInLoopThread();    bool earliestChanged = insert(timer); // timer加入最新超时的定时器被更新。    if (earliestChanged)&#123;         // 更新timerfd_的超时时间        resetTimerfd(timerfd_, timer-&gt;expiration());    &#125;&#125;void TimerQueue::cancelInLoop(TimerId timerId)&#123;    loop_-&gt;assertInLoopThread();    assert(timers_.size() == activeTimers_.size());    ActiveTimer timer(timerId.timer_, timerId.sequence_);    ActiveTimerSet::iterator it = activeTimers_.find(timer);    if (it != activeTimers_.end())&#123; // 在activeTimers_上        // 在timers_上删除timerId        delete it-&gt;first; // FIXME: no delete please        // 在activeTimers_上删除timerId    &#125;else if (callingExpiredTimers_)&#123;   // 如果正在处理超时定时器，那么timerId是有可能从activeTimers_上移除，而在handleRead::expired中        // 所以先将timerId加入cancelingTimers_列表，防止是循环定时器，又被重新加入到activeTimers_。handleRead会调用reset删除被取消的定时器。        cancelingTimers_.insert(timer);    &#125;    assert(timers_.size() == activeTimers_.size());&#125;void TimerQueue::handleRead()&#123; // timerfd_读事件处理回调    loop_-&gt;assertInLoopThread();    Timestamp now(Timestamp::now());    readTimerfd(timerfd_, now); // 清空timerfd_上的数据    std::vector&lt;Entry&gt; expired = getExpired(now);    callingExpiredTimers_ = true;    cancelingTimers_.clear();    // safe to callback outside critical section    for (const Entry&amp; it : expired)&#123;        it.second-&gt;run();   // 调用过期定时器的回调    &#125;    callingExpiredTimers_ = false;    reset(expired, now);    // 看能不能重新安装过期定时器，不能就delete。&#125;std::vector&lt;TimerQueue::Entry&gt; TimerQueue::getExpired(Timestamp now)&#123;    assert(timers_.size() == activeTimers_.size());    std::vector&lt;Entry&gt; expired;    // 根据now，在timers_中找过期的定时器，存入expired。    // ...    for (const Entry&amp; it : expired)&#123;         // 同步activeTimers_ 和 timers_        // ...    &#125;    assert(timers_.size() == activeTimers_.size());    return expired;&#125;void TimerQueue::reset(const std::vector&lt;Entry&gt;&amp; expired, Timestamp now)&#123;    Timestamp nextExpire;    for (const Entry&amp; it : expired)&#123;        ActiveTimer timer(it.second, it.second-&gt;sequence());        if (it.second-&gt;repeat()            &amp;&amp; cancelingTimers_.find(timer) == cancelingTimers_.end())&#123; // 是循环定时器，并且没有被取消。            it.second-&gt;restart(now);            insert(it.second);        &#125;else&#123;            // FIXME move to a free list            delete it.second; // FIXME: no delete please        &#125;    &#125;    if (!timers_.empty())&#123;        nextExpire = timers_.begin()-&gt;second-&gt;expiration();    &#125;    if (nextExpire.valid())&#123;        resetTimerfd(timerfd_, nextExpire);    &#125;&#125;bool TimerQueue::insert(Timer* timer)&#123;    loop_-&gt;assertInLoopThread();    assert(timers_.size() == activeTimers_.size());    bool earliestChanged = false;    Timestamp when = timer-&gt;expiration(); // timer超时时间    TimerList::iterator it = timers_.begin(); // 原来定时器中最早超时的定时器    if (it == timers_.end() || when &lt; it-&gt;first)&#123;         // 原本timers_就没有定时器 || 要插入的定时器超时时间 比 原来的timers_中第一个定时器 早。        // 都代表：插入新定时器后，最早超时时间会发生改变，需要重新设置timeFd。        earliestChanged = true;    &#125;    // 插入timers_    // std::set::insert    // 同步到activeTimers_    // std::set::insert    return earliestChanged;&#125;\n\n细节明细：疑问\n定时器模块存在的意义？\n解答\n\n事件触发机制： 定时器在Muduo中被用作一种事件触发机制。通过设置定时器，用户可以在指定的时间间隔内执行相应的操作，例如执行定时任务、发送心跳包等。这种事件触发机制有助于异步编程中的任务调度和协调。\n\n超时处理： 定时器用于处理超时事件，例如连接超时、读写操作超时等。通过设置合适的定时器，Muduo可以及时检测并处理超时情况，确保网络应用的稳定性和可靠性。\n\n可能还不太全，后面再有所感悟再来更新。。。\n\n\n\n本章完结\n","tags":["高性能服务器框架"]},{"title":"Ubuntu & CentOS 配置静态IP","url":"/2024/01/21/ops/ipconfig/","content":"CentOS配置静态IP命令：\ncd /etc/sysconfig/network-scripts/vim ifcfg-ens33systemctl restart network\n\n\n\n更改必要的项：\n...BOOTPROTO=static # 改成和我一致...ONBOOT=yes # 保持一致IPADDR=192.168.200.8    # 配置ip地址（按需）NETMASK=255.255.255.0   # ip地址的子网掩码（按需）GATEWAY=192.168.200.2   # 网关（按需）DNS1=8.8.8.8    # 配置DNS，一致就行\n\n\nUbuntu配置静态IP命令：\n# 编辑配置文件vim /etc/netplan/01-network-manager-all.yaml# 刷新一下netplan apply\n\n更改必要的项：\n在01-network-manager-all.yaml文件中renderer那一行后面追加网卡配置信息即可。地址按需配置！\nnetwork:    version: 2    renderer: NetworkManager    ethernets:        ens33:            dhcp4: no            addresses: [192.168.200.3/24]            gateway4: 192.168.200.2            nameservers:                addresses: [114.114.114.114,8.8.8.8]\n\n\n本章完结\n","tags":["Linux环境笔记","运维"]},{"title":"Ubuntu虚拟机使用纯命令行对根分区进行扩展","url":"/2024/06/09/ops/root_extend/","content":"前排提示因为Ubuntu再安装时，根分区是没有使用LVM进行磁盘管理的，所以如果想扩展根分区，我们不得不使用另外一种暴力的方法。简单来说就是利用fdisk删除原来的根分区再基于原来的起始块号重新建立一个根分区。从而达到扩展根分区的目的。\n步骤0、我在创建虚拟机的时候，硬盘只分配了50G，我的虚拟机基本配置如下：\n\n1、首先使用df和lsblk查看我分区情况：\n\nroot@lunar-virtual-machine:~/workspace# root@lunar-virtual-machine:~/workspace# df -hFilesystem      Size  Used Avail Use% Mounted ontmpfs           790M  2.0M  788M   1% /run/dev/sda3        49G   30G   17G  65% /tmpfs           3.9G     0  3.9G   0% /dev/shmtmpfs           5.0M  4.0K  5.0M   1% /run/lock/dev/sda2       512M  6.1M  506M   2% /boot/efitmpfs           790M   76K  790M   1% /run/user/128tmpfs           790M   60K  790M   1% /run/user/0root@lunar-virtual-machine:~/workspace# root@lunar-virtual-machine:~/workspace# root@lunar-virtual-machine:~/workspace# lsblkNAME   MAJ:MIN RM   SIZE RO TYPE MOUNTPOINTSloop0    7:0    0     4K  1 loop /snap/bare/5loop1    7:1    0  63.9M  1 loop /snap/core20/2264loop2    7:2    0  74.2M  1 loop /snap/core22/1122loop3    7:3    0  63.9M  1 loop /snap/core20/2318loop4    7:4    0  74.2M  1 loop /snap/core22/1380loop5    7:5    0   497M  1 loop /snap/gnome-42-2204/141loop6    7:6    0 349.7M  1 loop /snap/gnome-3-38-2004/143loop7    7:7    0 268.3M  1 loop /snap/firefox/4090loop8    7:8    0 505.1M  1 loop /snap/gnome-42-2204/176loop9    7:9    0 269.6M  1 loop /snap/firefox/4136loop10   7:10   0  91.7M  1 loop /snap/gtk-common-themes/1535loop11   7:11   0  12.9M  1 loop /snap/snap-store/1113loop12   7:12   0  12.3M  1 loop /snap/snap-store/959loop13   7:13   0  38.7M  1 loop /snap/snapd/21465loop14   7:14   0  40.4M  1 loop /snap/snapd/20671loop15   7:15   0   476K  1 loop /snap/snapd-desktop-integration/157loop16   7:16   0   452K  1 loop /snap/snapd-desktop-integration/83sda      8:0    0    50G  0 disk ├─sda1   8:1    0     1M  0 part ├─sda2   8:2    0   513M  0 part /boot/efi└─sda3   8:3    0  49.5G  0 part /var/snap/firefox/common/host-hunspell                                 /sr0     11:0    1   4.7G  0 rom  root@lunar-virtual-machine:~/workspace# \n\n2、关闭虚拟机，修改硬盘大小：\n\n3、启动虚拟机，再次使用df、lsblk查看分区情况，从lsblk命令的输出可以看到&#x2F;dev&#x2F;sda设备的容量变成了100G，但是因为我们还没进行分区，剩余的50G无法被投入使用，所以下一步开始准备扩展分区：\nroot@lunar-virtual-machine:~/workspace# root@lunar-virtual-machine:~/workspace# df -hFilesystem      Size  Used Avail Use% Mounted ontmpfs           790M  2.0M  788M   1% /run/dev/sda3        49G   30G   17G  65% /tmpfs           3.9G     0  3.9G   0% /dev/shmtmpfs           5.0M  4.0K  5.0M   1% /run/lock/dev/sda2       512M  6.1M  506M   2% /boot/efitmpfs           790M   76K  790M   1% /run/user/128tmpfs           790M   60K  790M   1% /run/user/0root@lunar-virtual-machine:~/workspace# root@lunar-virtual-machine:~/workspace# root@lunar-virtual-machine:~/workspace# lsblkNAME   MAJ:MIN RM   SIZE RO TYPE MOUNTPOINTSloop0    7:0    0     4K  1 loop /snap/bare/5loop1    7:1    0  63.9M  1 loop /snap/core20/2264loop2    7:2    0  74.2M  1 loop /snap/core22/1380loop3    7:3    0  74.2M  1 loop /snap/core22/1122loop4    7:4    0  63.9M  1 loop /snap/core20/2318loop5    7:5    0 268.3M  1 loop /snap/firefox/4090loop6    7:6    0 269.6M  1 loop /snap/firefox/4136loop7    7:7    0 349.7M  1 loop /snap/gnome-3-38-2004/143loop8    7:8    0   497M  1 loop /snap/gnome-42-2204/141loop9    7:9    0 505.1M  1 loop /snap/gnome-42-2204/176loop10   7:10   0  91.7M  1 loop /snap/gtk-common-themes/1535loop11   7:11   0  12.9M  1 loop /snap/snap-store/1113loop12   7:12   0  12.3M  1 loop /snap/snap-store/959loop13   7:13   0  40.4M  1 loop /snap/snapd/20671loop14   7:14   0  38.7M  1 loop /snap/snapd/21465loop15   7:15   0   476K  1 loop /snap/snapd-desktop-integration/157loop16   7:16   0   452K  1 loop /snap/snapd-desktop-integration/83sda      8:0    0   100G  0 disk ├─sda1   8:1    0     1M  0 part ├─sda2   8:2    0   513M  0 part /boot/efi└─sda3   8:3    0  49.5G  0 part /var/snap/firefox/common/host-hunspell                                 /sr0     11:0    1   4.7G  0 rom  root@lunar-virtual-machine:~/workspace# \n\n4、使用fdisk命令进行扩容，具体步骤是：使用p查看一下初始分区情况，一定要记住sda3的起始块号：1054720，然后使用d删除第3分区（此时很关键，一定不要使用w保持退出！），然后再n一个3分区，然后起始块就是1054720，最后一块默认就行了，将所有块都分配给根分区。然后w保持退出。\nroot@lunar-virtual-machine:~/workspace# fdisk /dev/sdaWelcome to fdisk (util-linux 2.37.2).Changes will remain in memory only, until you decide to write them.Be careful before using the write command.This disk is currently in use - repartitioning is probably a bad idea.It&#x27;s recommended to umount all file systems, and swapoff all swappartitions on this disk.Command (m for help): mHelp:  GPT   M   enter protective/hybrid MBR  Generic   d   delete a partition   F   list free unpartitioned space   l   list known partition types   n   add a new partition   p   print the partition table   t   change a partition type   v   verify the partition table   i   print information about a partition  Misc   m   print this menu   x   extra functionality (experts only)  Script   I   load disk layout from sfdisk script file   O   dump disk layout to sfdisk script file  Save &amp; Exit   w   write table to disk and exit   q   quit without saving changes  Create a new label   g   create a new empty GPT partition table   G   create a new empty SGI (IRIX) partition table   o   create a new empty DOS partition table   s   create a new empty Sun partition tableCommand (m for help): pDisk /dev/sda: 100 GiB, 107374182400 bytes, 209715200 sectorsDisk model: VMware Virtual SUnits: sectors of 1 * 512 = 512 bytesSector size (logical/physical): 512 bytes / 512 bytesI/O size (minimum/optimal): 512 bytes / 512 bytesDisklabel type: gptDisk identifier: 14A69C2D-E6FB-4626-A7E6-10A16A052F8DDevice       Start       End   Sectors  Size Type/dev/sda1     2048      4095      2048    1M BIOS boot/dev/sda2     4096   1054719   1050624  513M EFI System/dev/sda3  1054720 104855551 103800832 49.5G Linux filesystemCommand (m for help): dPartition number (1-3, default 3): 3Partition 3 has been deleted.Command (m for help): pDisk /dev/sda: 100 GiB, 107374182400 bytes, 209715200 sectorsDisk model: VMware Virtual SUnits: sectors of 1 * 512 = 512 bytesSector size (logical/physical): 512 bytes / 512 bytesI/O size (minimum/optimal): 512 bytes / 512 bytesDisklabel type: gptDisk identifier: 14A69C2D-E6FB-4626-A7E6-10A16A052F8DDevice     Start     End Sectors  Size Type/dev/sda1   2048    4095    2048    1M BIOS boot/dev/sda2   4096 1054719 1050624  513M EFI SystemCommand (m for help): nPartition number (3-128, default 3): First sector (1054720-209715166, default 1054720): 1054720Last sector, +/-sectors or +/-size&#123;K,M,G,T,P&#125; (1054720-209715166, default 209715166): Created a new partition 3 of type &#x27;Linux filesystem&#x27; and of size 99.5 GiB.Partition #3 contains a ext4 signature.Do you want to remove the signature? [Y]es/[N]o: yThe signature will be removed by a write command.Command (m for help): wThe partition table has been altered.Syncing disks.root@lunar-virtual-machine:~/workspace# \n\n5、此时再用命令查看一下分区情况，我可以看到lsblk命令将剩余的50G算作根分区的容量，但是df命令显示的根分区还是50个G。这点很好解决，继续下面步骤。\nroot@lunar-virtual-machine:~/workspace# lsblkNAME   MAJ:MIN RM   SIZE RO TYPE MOUNTPOINTSloop0    7:0    0     4K  1 loop /snap/bare/5loop1    7:1    0  63.9M  1 loop /snap/core20/2264loop2    7:2    0  74.2M  1 loop /snap/core22/1380loop3    7:3    0  74.2M  1 loop /snap/core22/1122loop4    7:4    0  63.9M  1 loop /snap/core20/2318loop5    7:5    0 268.3M  1 loop /snap/firefox/4090loop6    7:6    0 269.6M  1 loop /snap/firefox/4136loop7    7:7    0 349.7M  1 loop /snap/gnome-3-38-2004/143loop8    7:8    0   497M  1 loop /snap/gnome-42-2204/141loop9    7:9    0 505.1M  1 loop /snap/gnome-42-2204/176loop10   7:10   0  91.7M  1 loop /snap/gtk-common-themes/1535loop11   7:11   0  12.9M  1 loop /snap/snap-store/1113loop12   7:12   0  12.3M  1 loop /snap/snap-store/959loop13   7:13   0  40.4M  1 loop /snap/snapd/20671loop14   7:14   0  38.7M  1 loop /snap/snapd/21465loop15   7:15   0   476K  1 loop /snap/snapd-desktop-integration/157loop16   7:16   0   452K  1 loop /snap/snapd-desktop-integration/83sda      8:0    0   100G  0 disk ├─sda1   8:1    0     1M  0 part ├─sda2   8:2    0   513M  0 part /boot/efi└─sda3   8:3    0  99.5G  0 part /var/snap/firefox/common/host-hunspell                                 /sr0     11:0    1   4.7G  0 rom  root@lunar-virtual-machine:~/workspace# df -hFilesystem      Size  Used Avail Use% Mounted ontmpfs           790M  2.0M  788M   1% /run/dev/sda3        49G   30G   17G  65% /tmpfs           3.9G     0  3.9G   0% /dev/shmtmpfs           5.0M  4.0K  5.0M   1% /run/lock/dev/sda2       512M  6.1M  506M   2% /boot/efitmpfs           790M   76K  790M   1% /run/user/128tmpfs           790M   60K  790M   1% /run/user/0\n\n6、利用df -Th命令查看根分区的文件系统类型，可以看到挂载在&#x2F;目录下的文件系统是ext4类型文件系统，所以，我们可以使用resize2fs &#x2F;dev&#x2F;sda命令可以更新分区情况。至此，根分区扩展完毕。\nroot@lunar-virtual-machine:~/workspace# df -ThFilesystem     Type   Size  Used Avail Use% Mounted ontmpfs          tmpfs  790M  2.0M  788M   1% /run/dev/sda4      ext4    49G   30G   17G  65% /tmpfs          tmpfs  3.9G     0  3.9G   0% /dev/shmtmpfs          tmpfs  5.0M  4.0K  5.0M   1% /run/lock/dev/sda2      vfat   512M  6.1M  506M   2% /boot/efitmpfs          tmpfs  790M   76K  790M   1% /run/user/128tmpfs          tmpfs  790M   60K  790M   1% /run/user/0root@lunar-virtual-machine:~/workspace# resize2fs /dev/sdaresize2fs 1.46.5 (30-Dec-2021)resize2fs: Device or resource busy while trying to open /dev/sdaCouldn&#x27;t find valid filesystem superblock.root@lunar-virtual-machine:~/workspace# resize2fs /dev/sda3resize2fs 1.46.5 (30-Dec-2021)Filesystem at /dev/sda3 is mounted on /; on-line resizing requiredold_desc_blocks = 7, new_desc_blocks = 13The filesystem on /dev/sda3 is now 26082555 (4k) blocks long.root@lunar-virtual-machine:~/workspace# root@lunar-virtual-machine:~/workspace# root@lunar-virtual-machine:~/workspace# df -hFilesystem      Size  Used Avail Use% Mounted ontmpfs           790M  2.0M  788M   1% /run/dev/sda3        98G   30G   64G  32% /tmpfs           3.9G     0  3.9G   0% /dev/shmtmpfs           5.0M  4.0K  5.0M   1% /run/lock/dev/sda2       512M  6.1M  506M   2% /boot/efitmpfs           790M   76K  790M   1% /run/user/128tmpfs           790M   60K  790M   1% /run/user/0root@lunar-virtual-machine:~/workspace# lsblkNAME   MAJ:MIN RM   SIZE RO TYPE MOUNTPOINTSloop0    7:0    0     4K  1 loop /snap/bare/5loop1    7:1    0  63.9M  1 loop /snap/core20/2264loop2    7:2    0  74.2M  1 loop /snap/core22/1380loop3    7:3    0  74.2M  1 loop /snap/core22/1122loop4    7:4    0  63.9M  1 loop /snap/core20/2318loop5    7:5    0 268.3M  1 loop /snap/firefox/4090loop6    7:6    0 269.6M  1 loop /snap/firefox/4136loop7    7:7    0 349.7M  1 loop /snap/gnome-3-38-2004/143loop8    7:8    0   497M  1 loop /snap/gnome-42-2204/141loop9    7:9    0 505.1M  1 loop /snap/gnome-42-2204/176loop10   7:10   0  91.7M  1 loop /snap/gtk-common-themes/1535loop11   7:11   0  12.9M  1 loop /snap/snap-store/1113loop12   7:12   0  12.3M  1 loop /snap/snap-store/959loop13   7:13   0  40.4M  1 loop /snap/snapd/20671loop14   7:14   0  38.7M  1 loop /snap/snapd/21465loop15   7:15   0   476K  1 loop /snap/snapd-desktop-integration/157loop16   7:16   0   452K  1 loop /snap/snapd-desktop-integration/83sda      8:0    0   100G  0 disk ├─sda1   8:1    0     1M  0 part ├─sda2   8:2    0   513M  0 part /boot/efi└─sda3   8:3    0  99.5G  0 part /var/snap/firefox/common/host-hunspell                                 /sr0     11:0    1   4.7G  0 rom  \n\n这里额外记录一下，第6步最后的刷新文件系统状态的命令，不同的文件系统需要使用不同的命令：\n\nresize2fs &#x2F;dev&#x2F;sda 针对文件系统ext2 ext3 ext4  （一般是Ubuntu上使用\n\nxfs_growfs &#x2F;dev&#x2F;sda 针对文件系统xfs    （一般是centos上使用，而centos是使用lvm来扩展根分区的。\n\n\n磁盘分区小干货：\n硬盘分区有三种，主磁盘分区、扩展磁盘分区、逻辑分区。\n三种分区必须满足以下限制：\n\n一个硬盘主分区至少有1个，最多4个。\n\n扩展分区可以没有，最多1个。（扩展分区其实也算一种特殊的主分区）\n\n主分区+扩展分区总共不能超过4个。\n\n逻辑分区可以有若干个。\n\n\n关于分区号：\n\n1~4号：只能被主分区和扩展分区使用。\n\n5~n号：被逻辑分区使用。\n\n\n主分区可以包含一个操作系统（例如Linux、Windows）。\n扩展分区本身不能包含文件系统，只能包含逻辑分区。\n\n本章完结\n","tags":["Linux环境笔记","运维"]},{"title":"Ubuntu使用Docker搭建SonarQube企业版（含破解方法）","url":"/2025/05/15/ops/sonarqube/","content":"SonarQube介绍Sonar (SonarQube)是一个开源平台，用于管理源代码的质量。支持Java、kotlin、Scala、python、JavaScrip等二十几种编程语言的代码质量管理与检测。支持将代码扫描集成到现有的工作流，以便在项目分支和拉取请求之间进行连续的代码检查。\n分为社区版、开发版、企业版数据中心版。对于静态检测，只有开发版、企业版支持C&#x2F;C++，而我们的业务以C&#x2F;C++为主，故选择需要破解的企业版。\n安装环境搭建参考博客：https://www.codestar.top/2024/10/10/Linux/Ubuntu-docker%E6%90%AD%E5%BB%BAsonarqube%E7%A4%BE%E5%8C%BA%E7%89%88/\nDocker安装PostgreSQL容器\n直接上命令：\ndocker pull postgres\n\n如下：\n\n启动容器：\ndocker run --name postgresql \\    --restart=always \\    -e POSTGRES_USER=admin \\    -e POSTGRES_PASSWORD=123456 \\    -e POSTGRES_DB=sonarqube_db \\    -p 5432:5432 \\    -v /opt/postgres:/var/lib/postgresql/data \\    -e ALLOW_IP_RANGE=0.0.0.0/0 \\    -d postgres\n\n参数含义如下：\n\n–name：后接容器名\n–restart&#x3D;always：dcoker启动后，自动重启容器\nPOSTGRES_USER：pg数据库用户名\nPOSTGRES_PASSWORD：pg数据库密码\nPOSTGRES_DB：创建一个名为sonarqube_db的pg数据库\n-p 5432:5432：端口映射\n-v：主机目录-&gt;容器目录的映射\nALLOW_IP_RANGE&#x3D;0.0.0.0&#x2F;0：允许任何主机访问\n-d：守护进程方式创建容器\n最后一个参数：镜像名。\n\npg容器创建好后如图所示：\n\nDocker安装SonarQube容器获取企业版sonarqube镜像：\ndocker pull sonarqube:9.9-enterprise\n\n如下：\n\n我们可以使用如下命令创建并启动sonarqube容器：\n# docker run --name 容器名(可任意) -d -p 内部端口:外部端口 镜像名docker run --name sonarqube -p 9000:9000 -d sonarqube:9.9-enterprise\n\n但是我们不能向上面那样干！它默认会使用H2嵌入式数据库，并且数据库是存在于内存当中的，如果我们的docker容器重启，那么之前所有的数据都将丢失！\n和pg容器创建不同，这里需要手动创建主机到容器映射目录，并将其权限修改为777.\nmkdir -p /opt/sonarqube/data /opt/sonarqube/extensions /opt/sonarqube/logs /opt/sonarqube/confsudo chmod 777 -R /opt/sonarqube\n\n然后创建SonarQube容器实例：\ndocker run --name sonarqube \\        --restart=always \\    -e SONAR_JDBC_URL=jdbc:postgresql://192.168.0.135:5432/sonarqube_db \\    -e SONAR_JDBC_USERNAME=admin \\    -e SONAR_JDBC_PASSWORD=dc123 \\    -p 9000:9000 \\    -v /opt/sonarqube/data:/opt/sonarqube/data \\    -v /opt/sonarqube/extensions:/opt/sonarqube/extensions \\    -v /opt/sonarqube/logs:/opt/sonarqube/logs \\    -v /opt/sonarqube/conf:/opt/sonarqube/conf \\    -d sonarqube:9.9-enterprise\n\n参数含义如下（同前面重复的省略）：\n\nSONAR_JDBC_URL：pg数据库的url，ip根据主机实际情况填写\nSONAR_JDBC_USERNAME：pg数据库用户名\nSONAR_JDBC_PASSWORD：pg数据库密码\nextensions目录：sonarsube插件存放目录\nlogs：日志目录\nconf: 配置目录\n\n不出意外，容器状态会一直是restart状态，此时需要使用docker logs -f sonarqube查看对应的日志信息检查错误点。\n一般是因为容器卷映射目录权限问题，或者是需要修改主机的系统参数（报错：max virtual memory areas vm.max_map_count [65530] is too low）。\n权限问题好解决，直接chmod修改&#x2F;opt&#x2F;sonarqube目录权限为777即可。\n而系统参数问题解决方法参考如下：\n在主机上（宿主机，非docker容器！docker容器会继承宿主机的系统参数。）修改系统参数：\nvim /etc/sysctl.conf# 最后一行添加:vm.max map count=262144vm.max map count=524288sysctl -p # 加载生效\n然后重启docker：\ndocker start sonarqube\n\n\n实例运行情况：\n\n在ubuntu浏览器当中输入http://localhost:9000/ ，默认管理员用户及密码：admin&#x2F;admin。\nSonarQube汉化插件安装进入到插件商城，搜索插件chinese，按下面操作进入到插件主页进行下载：\n\n进入汉化github插件仓库，进入标签发布页，找到9.9版本的中文插件，点击即可下载：\n\n下载完成后，将插件拷贝到主机的&#x2F;opt&#x2F;sonarqube&#x2F;extensions&#x2F;plugins目录下（plugins目录需要手动创建），再次重启sonarqube容器，重新登录http://localhost:9000/，可以看到提示变成了中文，说明我们的插件安装成功了！\n\n破解破解参考博客：https://dashenxian.github.io/post/SonarQube%E7%A0%B4%E8%A7%A3\n生成license将如下这段信息全部复制，然后进行base64加密就是license的内容：\nCompany=UnknownDigest=NotRequiredEdition=EnterpriseEditionLabel=EnterpriseExpiration=2099-01-01MaxLoc=9223372036854775806Plugins=abap,cpp,plsql,security,sonarapex,swift,tsql,vbnet,cobol,pli,rpg,vbFeatures=*ServerId=*Support=falseType=ny0c\n\n同理，如果是开发版，只需将Enterprise替换为Developer。\nBase64转换如下：\nQ29tcGFueT1Vbmtub3duCkRpZ2VzdD1Ob3RSZXF1aXJlZApFZGl0aW9uPUVudGVycHJpc2UKRWRpdGlvbkxhYmVsPUVudGVycHJpc2UKRXhwaXJhdGlvbj0yMDk5LTAxLTAxCk1heExvYz05MjIzMzcyMDM2ODU0Nzc1ODA2ClBsdWdpbnM9YWJhcCxjcHAscGxzcWwsc2VjdXJpdHksc29uYXJhcGV4LHN3aWZ0LHRzcWwsdmJuZXQsY29ib2wscGxpLHJwZyx2YgpGZWF0dXJlcz0qClNlcnZlcklkPSoKU3VwcG9ydD1mYWxzZQpUeXBlPW55MGM=\n\n配置agent首先下载agent jar包：SonarQubeAgent-1.2-SNAPSHOT.jar。\n使用docker命令，将下载好的agent jar包拷贝到容器当中，这里为方便，我直接拷贝到根目录下面。：\ndocker cp ./SonarQubeAgent-1.2-SNAPSHOT.jar sonarqube:9.9-enterprise:/\n\n在sonarqube的配置文件conf&#x2F;sonar.properties（sonar.properties文件需要手动创建），里面新增如下内容：\nsonar.web.javaOpts=-javaagent:/SonarQubeAgent-1.2-SNAPSHOT.jar -Xmx1G -Xms128m -XX:+HeapDumpOnOutOfMemoryErrorsonar.ce.javaOpts=-javaagent:/SonarQubeAgent-1.2-SNAPSHOT.jar -Xmx2G -Xms128m -XX:+HeapDumpOnOutOfMemoryError\n\n重启sonarqube容器，然后License Manager当中填入上文获取的证书即可：\n\n使用这里主要叙述手工代码审查的步骤：\n\n主页右上角 -&gt; 新增项目 -&gt; 手工\n \n\n按需填写显示名和项目标识符：\n \n\n选择本地：\n \n\n（按需）将令牌修改为永不过期：\n \n\n记住这个令牌：\n \n\n选择C，C++或ObjC -&gt; Linux，下面会出现下载必要的应用包、扫描器以及在项目当中扫描器的使用教程，相关步骤代码如下：\n （注意下面这些配置都是属于客户主机的配置，即你所要审查的项目代码所在的那台主机）应用包、扫描器的安装以及环境变量的配置：\n curl --create-dirs -sSLo $HOME/.sonar/build-wrapper-linux-x86.zip http://192.168.0.135:9000/static/cpp/build-wrapper-linux-x86.zipunzip -o $HOME/.sonar/build-wrapper-linux-x86.zip -d $HOME/.sonar/export PATH=$HOME/.sonar/build-wrapper-linux-x86:$PATHexport SONAR_SCANNER_VERSION=4.7.0.2747export SONAR_SCANNER_HOME=$HOME/.sonar/sonar-scanner-$SONAR_SCANNER_VERSION-linuxcurl --create-dirs -sSLo $HOME/.sonar/sonar-scanner.zip https://binaries.sonarsource.com/Distribution/sonar-scanner-cli/sonar-scanner-cli-$SONAR_SCANNER_VERSION-linux.zipunzip -o $HOME/.sonar/sonar-scanner.zip -d $HOME/.sonar/export PATH=$SONAR_SCANNER_HOME/bin:$PATHexport SONAR_SCANNER_OPTS=&quot;-server&quot;export SONAR_TOKEN=sqp_0e3bfc1c50b71f7ba912e8e36700c315f155081d\n\n 其中ip地址以及令牌根据实际情况填写！\n\n在独立的cmake项目当中使用sonarqube：\n  项目结构如下：\n  \n  进入build目录，执行cmake ..，然后执行如下命令进行编译：\n  cd buildbuild-wrapper-linux-x86-64 --out-dir bw-output make\n\n  执行完成后会在build目录下生成一个bw-output目录，里面比较重要的是build-wrapper-dump.json文件，如果你看到该文件生成了很多配置（大几十行），说明你的配置是正确的，否则如果只有少数几行，说明配置存在问题，需要查看是否有步骤缺失。build-wrapper-linux-x86-64命令实际上会hook住编译器，从而获取项目当中各个源文件以及其依赖关系。接下来的sonar-scanner命令才是真正执行代码审查的主体。\n  然后回到项目根目录，使用扫描器进行代码审查，分析结果会传到sonarqube所属项目当中：\n  cd ..sonar-scanner \\-Dsonar.projectKey=test \\-Dsonar.sources=. \\-Dsonar.cfamily.build-wrapper-output=build/bw-output \\-Dsonar.host.url=http://192.168.0.135:9000 \\-Dsonar.login=sqp_0e3bfc1c50b71f7ba912e8e36700c315f155081d\n\n  这里主要关注这两个参数：\n  - -Dsonar.cfamily.build-wrapper-output：build-wrapper-linux-x86-64命令生成配置文件路径。\n  - -Dsonar.sources：源码根路径\n\n  步骤如下：\n  \n  等待执行完成后，输出：\n  \n  此时同样的会在项目根目录产生一个配置目录：.scannerwork，结合上述，现在项目结构为：\n  \n  在sonarqube网页上可以看到分析结果：\n  \n\n在buildroot项目当中使用sonarqube：\n  有了cmake项目使用sonarqube的经验，buildroot项目其实也类似，高度重复的部分会直接省略：\n  首先是环境变量的配置，参考步骤6的开头部分，然后是使用build-wrapper-linux-x86-64命令对buildroot包进行编译，并自动创建bw-output，这里以编译mediaserver为例：\n  # 更换项目令牌export SONAR_TOKEN=sqp_60cd6b366e2852a5fdb5f7629c5e6c73b6c94d6cbuild-wrapper-linux-x86-64 --out-dir bw-output make mediaserver\n\n  在buildroot下生成了bw-output目录：\n  \n  进入到buildroot&#x2F;output&#x2F;dc_rv1126_ipc&#x2F;build&#x2F;mediaserver&#x2F;目录（合理修改-Dsonar.sources、-Dsonar.cfamily.build-wrapper-output参数路径可以无需cd来cd去，前者代表源码根路径，后者代表执行build-wrapper-linux-x86-64命令时所指定的–out-dir的目录路径），执行如下代码：\n  sonar-scanner \\-Dsonar.projectKey=mediaserver \\-Dsonar.sources=. \\-Dsonar.cfamily.build-wrapper-output=../../../../bw-output \\-Dsonar.host.url=http://192.168.0.135:9000 \\-Dsonar.login=sqp_60cd6b366e2852a5fdb5f7629c5e6c73b6c94d6c \\-Dsonar.scm.disabled=true \\-Dsonar.scm.exclusions.disabled=true\n\n  结果如下：\n  \n  \n\n\n\n\n\n本章完结\n","tags":["Linux环境笔记","运维"]},{"title":"Qt快速入门（Opencv小案例之人脸识别）","url":"/2024/04/14/qt/OpencvFaceRecognize/","content":"编译出错记录背景因为主要使用qt，并且官网下载的win版本的编译好的opencv默认是vc的，所以我们需要自己下载opencv的源码使用mingw自行编译，我直接使用的vscode。\n报错\n报错如下：\nFatal error: can&#x27;t write 9 bytes to section ... file too big\n\n\n参考github上opencv项目的issue，解决方案如下：\n先按常规动作编译一下，然后在生成的build目录中，向cmake的中间文件：CMakeFiles\\3.27.2-msvc1\\CMakeCXXCompiler.cmake文件（其中3.27.2-msvc1目录名可能会有所不同，找同样带数字的就可以了）追加一行cmake代码：\n\nset(CMAKE_CXX_FLAGS ${CMAKE_CXX_FLAGS} “-O3”)\n\n错误即可解除。\n安装\n找了很久通过vscode按钮自定义安装路径的方法，没有找到。。。\n于是还是顺从使用linux的习惯，使用终端通过命令进行编译OpenCV命令如下：\nmkdir build &amp;&amp; cd buildcmake -DCMAKE_BUILD_TYPE=Release -DCMAKE_INSTALL_PREFIX=D:/DevelopmentToolRoot/Library/OpenCV ..# 更改cmake生成的cmake中间文件，防止报错# 添加内容：set(CMAKE_CXX_FLAGS $&#123;CMAKE_CXX_FLAGS&#125; &quot;-O3&quot;)...make -j4 &amp;&amp; make install\n\ncmake的-DCMAKE_INSTALL_PREFIX选项也是查了很久，最开始是在生成的makefile发现生成的make install的实现如下：\n# Special rule for the target installinstall: preinstall\t@$(CMAKE_COMMAND) -E cmake_echo_color &quot;--switch=$(COLOR)&quot; --cyan &quot;Install the project...&quot;\tD:\\DevelopmentToolRoot\\Complier\\VS2022\\Root\\Common7\\IDE\\CommonExtensions\\Microsoft\\CMake\\CMake\\bin\\cmake.exe -P cmake_install.cmake.PHONY : install\n\n显然，执行安装时会执行cmake_install.cmake文件，通过查看文件中的内容发现真正影响安装路径的是CMAKE_INSTALL_PREFIX，然后继续反向查找，在项目CMakeLists.txt文件中发现，有段判断CMAKE_INSTALL_PREFIX是否初始化的代码：\nif(CMAKE_INSTALL_PREFIX_INITIALIZED_TO_DEFAULT)  # https://cmake.org/cmake/help/latest/variable/CMAKE_INSTALL_PREFIX_INITIALIZED_TO_DEFAULT.html  if(NOT CMAKE_TOOLCHAIN_FILE)    if(WIN32)      set(CMAKE_INSTALL_PREFIX &quot;$&#123;CMAKE_BINARY_DIR&#125;/install&quot; CACHE PATH &quot;Installation Directory&quot; FORCE)    else()      set(CMAKE_INSTALL_PREFIX &quot;/usr/local&quot; CACHE PATH &quot;Installation Directory&quot; FORCE)    endif()  else()    # any cross-compiling    set(CMAKE_INSTALL_PREFIX &quot;$&#123;CMAKE_BINARY_DIR&#125;/install&quot; CACHE PATH &quot;Installation Directory&quot; FORCE)  endif()endif()\n可见，如果在使用cmake时用户没有设置CMAKE_INSTALL_PREFIX，系统会默认安装路径为build&#x2F;install目录下。\n注意添加环境变量。\n如果在qt中使用opencv的时候有报错：无法定位程序入口点…。\n试着将mingw&#x2F;bin目录下的libstdc++-6动态库复制到C:\\Windows\\System32下。\n如果还报错，直接在网上找别人编译好的库吧，自己编译太麻烦了，也没有必要浪费时间。推荐直接下载的链接：https://github.com/huihut/OpenCV-MinGW-Build?tab=readme-ov-file。\nQT + Opencv图片人脸识别小demo\nQT.pro文件添加opencv的include文件和静态库文件的配置，如下：\n INCLUDEPATH += D:\\DevelopmentToolRoot\\Library\\OpenCV\\OpenCV-MinGW-Build-OpenCV-3.4.8-x64\\OpenCV-MinGW-Build-OpenCV-3.4.8-x64\\includeLIBS += D:\\DevelopmentToolRoot\\Library\\OpenCV\\OpenCV-MinGW-Build-OpenCV-3.4.8-x64\\OpenCV-MinGW-Build-OpenCV-3.4.8-x64\\x64\\mingw\\lib\\libopencv_*.a\n\n 具体路径和你opencv安装路径有关，相对路径是：OpenCV&#x2F;include、OpenCV&#x2F;x64&#x2F;mingw&#x2F;lib&#x2F;libopencv_*.a。\n\n另外还需要，将OpenCV&#x2F;etc下的haarcascade_eye_tree_eyeglasses.xml和haarcascade_frontalface_alt.xml文件拷贝到qt工程目录的根目录下。\n\n\nQT代码如下：\n// ...#include &lt;opencv2/opencv.hpp&gt;using namespace cv;class FaceDialog : public QDialog&#123;    Q_OBJECTpublic:    FaceDialog(QWidget *parent = nullptr);    ~FaceDialog();private slots:    void on_m_btnRecognize_clicked();private:    Ui::FaceDialog *ui;    Mat m_image;&#125;;FaceDialog::FaceDialog(QWidget *parent)    : QDialog(parent)    , ui(new Ui::FaceDialog)&#123;    ui-&gt;setupUi(this);    cvtColor(imread(QString(&quot;C:/Users/root/Desktop/Face/tahiti.jpg&quot;).toLatin1().data()),        m_image, COLOR_BGR2RGB);    ui-&gt;m_labImage-&gt;resize(m_image.cols / 2, m_image.rows / 2);    setWindowFlag(Qt::MSWindowsFixedSizeDialogHint);    ui-&gt;m_labImage-&gt;setPixmap(QPixmap::fromImage(QImage(        m_image.data, m_image.cols, m_image.rows,        QImage::Format_RGB888).scaled(ui-&gt;m_labImage-&gt;size(),        Qt::KeepAspectRatio)));&#125;FaceDialog::~FaceDialog()&#123;    delete ui;&#125;void FaceDialog::on_m_btnRecognize_clicked()&#123;    // 人脸分类器    CascadeClassifier faceClassifier;    faceClassifier.load(&quot;C:/Users/root/Desktop/Face/haarcascade_frontalface_alt.xml&quot;);    // 眼睛分类器    CascadeClassifier eyesClassifier;    eyesClassifier.load(&quot;C:/Users/root/Desktop/Face/haarcascade_eye_tree_eyeglasses.xml&quot;);    // 灰度图    Mat gray;    cvtColor(m_image, gray, COLOR_RGB2GRAY);    equalizeHist(gray, gray); // 直方图均衡化亮度增强    Mat canvas = m_image.clone(); // 用于输出识别结果的图像    vector&lt;Rect&gt; faces; // 存放多张人脸矩形的向量    faceClassifier.detectMultiScale(gray, faces); // 人脸识别    for (Rect const&amp; face : faces) // 遍历每一张人脸的包络矩形    &#123;        // 绘制人脸包络矩形的内切椭圆        // 第三个参数表示椭圆长半轴和短半轴        ellipse(canvas,            Point(face.x + face.width / 2,                  face.y + face.height / 2),            Size(face.width / 2, face.height / 2),            0, 0, 360, Scalar(0, 255, 0), 6, 8, 0);        vector&lt;Rect&gt; eyes; // 存放多只眼睛矩形的向量        eyesClassifier.detectMultiScale(gray(face), eyes); // 眼睛识别        for (Rect const&amp; eye : eyes) // 遍历每一只眼睛的包络矩形            // 绘制眼睛包络矩形的内切椭圆            ellipse(canvas,                Point(face.x + eye.x + eye.width / 2,                      face.y + eye.y + eye.height / 2),                Size(eye.width / 2, eye.height / 2),                0, 0, 360, Scalar(0, 255, 0), 6, 8, 0);    &#125;    ui-&gt;m_labImage-&gt;setPixmap(QPixmap::fromImage(QImage(        canvas.data, canvas.cols, canvas.rows,        QImage::Format_RGB888).scaled(ui-&gt;m_labImage-&gt;size(),        Qt::KeepAspectRatio)));&#125;\n\n程序结果如下：\n输入图片：\n\n输出图片：\n\n\n本章完结\n","tags":["QT"]},{"title":"Qt快速入门（MV架构之TableView + QStandardItemModel + 自定义代理小案例）","url":"/2024/04/14/qt/QtQuickStart/","content":"关于MV架构的简单介绍在Qt框架中，代理（Delegate）、模型（Model）和视图（View）之间的关系构成了MVVM（Model-View-ViewModel）架构的一部分，尽管Qt通常使用Model-View架构。这三者之间的关系可以这样理解：\n1. Model（模型）\nModel是数据的核心代表，它负责存储和管理应用程序的数据。Model提供了数据的接口，允许View查询和修改数据。Model与View的交互是通过信号和槽机制来完成的，当Model中的数据发生变化时，它会发出信号通知View进行更新。\n2. View（视图）\nView是Model数据的展示层，它负责将数据以用户友好的形式展示出来，并接收用户的交互操作。在Qt中，View通常是通过一些控件来实现的，比如QListView、QTableView、QTreeView等。View不处理数据的逻辑，它只是简单地展示Model提供的数据。\n3. Delegate（代理）\n\n\nDelegate位于Model和View之间，充当了一个中介的角色。它允许开发者为View中的每个项创建自定义的编辑器或显示组件。代理的作用是处理View中的项的创建、显示和编辑。当用户与View交互时，代理负责将用户的输入转换为对Model的修改，同时也负责将Model的数据转换为View中的显示形式。\n代理、模型和视图之间的关系\nModel与View：Model和View之间通过数据接口进行交互。Model提供数据，View展示数据。Model通过信号通知View数据的变化，View通过槽来响应这些信号并更新显示。\nModel与Delegate：Model提供了数据的接口，而Delegate负责将这些数据以特定方式显示在View中。Delegate从Model获取数据，并将其转换为用户可以理解的形式。\nView与Delegate：View使用Delegate来创建和管理每个项的显示和编辑。Delegate为View中的项提供自定义的外观和行为，使得View可以展示复杂的数据项。\nDelegate作为中介：Delegate作为Model和View之间的中介，它处理用户的输入并将这些输入转换为对Model的操作。同时，它也负责将Model的数据格式化并展示在View中。\n需要注意的是，MV架构中，默认的代理使用的是单行文本框！\n\nUI设计本博客需要的资源链接。提取码：14ml。\n\n创建项目，选择QMainWindow作为主窗口的基类。主窗口名为：TableWindow。\n\n为项目添加一个资源文件，将项目用到的图标引用到资源文件中。\n\n双击ui文件，移除菜单栏，添加工具栏，保留状态栏。添加数个QAction，如下图：\n \n\n界面设计：\n\n\n\n控件名\nbjectname\nframeShape\nframeShadow\nreadOnly\n\n\n\nQTableView\nm_table\nwinpanel\nsunken\n\\\n\n\nQPlainTextEdit\nm_edit\nwinpanel\nsunken\n√\n\n\n\n系统信号和系统槽的连接：\n \n\n系统信号和自定义槽的连接（鼠标右击创建的QAction，选择转到槽）：\n\n\n\nQAction\n要连接的信号\n\n\n\n加粗\ntriggered(bool)\n\n\n其余的QAction（除退出外）\ntriggered()\n\n\n\n\n整体效果如下：\n\n功能实现添加一个自定义代理类，类名为ComboBoxDelegate，基类为：QStyledItemDelegate\nclass ComboBoxDelegate : public QStyledItemDelegate&#123;public:    ComboBoxDelegate();protected:    // 创建一个编辑框    QWidget * createEditor(QWidget *parent,                           const QStyleOptionViewItem &amp;option,                           const QModelIndex &amp;index) const override;    // 将模型中指定索引位置的数据设置到编辑框组件中    void setEditorData(QWidget *editor,                       const QModelIndex &amp;index) const override;    // 将编辑框中的数据放回到模型中    void setModelData(QWidget *editor,                      QAbstractItemModel *model,                      const QModelIndex &amp;index) const override;    // 根据视图的样式调整编辑组件的几何形状    void updateEditorGeometry(QWidget *editor,                              const QStyleOptionViewItem &amp;option,                              const QModelIndex &amp;index) const override;&#125;;ComboBoxDelegate::ComboBoxDelegate()&#123;&#125;/* *  1.创建一个编辑框 *  参数: *      参数1 - 新创建的编辑器的父部件 *      参数2 - 渲染视图的样式选项 *      参数3 - 要编辑位置的模型索引*/QWidget * ComboBoxDelegate::createEditor(QWidget *parent,                       const QStyleOptionViewItem &amp;option,                       const QModelIndex &amp;index) const&#123;    Q_UNUSED(option);    Q_UNUSED(index);    QComboBox* editor = new QComboBox(parent );    editor-&gt;addItem(&quot;男&quot;);    editor-&gt;addItem(&quot;女&quot;);    return editor;&#125;/* *  1.将模型中的数据放到编辑框中 *      参数1:编辑组件 *      参数2:需要设置数据的模型索引 * *  创建的组件是QComboBox - 给的是QWidget */void ComboBoxDelegate::setEditorData(QWidget *editor,                   const QModelIndex &amp;index) const&#123;    static_cast&lt;QComboBox*&gt;(editor)-&gt;setCurrentText(                index.model()-&gt;data(index, Qt::EditRole).toString());&#125;/*    1.将编辑框中的数据给到模型*/void ComboBoxDelegate::setModelData(QWidget *editor,                  QAbstractItemModel *model,                  const QModelIndex &amp;index) const&#123;    // index - 将数据放到模型的哪个位置    model-&gt;setData(index,                   static_cast&lt;QComboBox*&gt;(editor)-&gt;currentText(),                   Qt::EditRole);&#125;void ComboBoxDelegate::updateEditorGeometry(QWidget *editor,                          const QStyleOptionViewItem &amp;option,                          const QModelIndex &amp;index) const&#123;    Q_UNUSED(index);    editor-&gt;setGeometry(option.rect);&#125;\n\n在TableWindow类中，添加Model、和标签，此外手动添加一个m_selection成员的currentChanged信号的槽函数。定义如下：\nclass TableWindow : public QMainWindow&#123;    Q_OBJECTprivate slots:/*    ...*/    // item选择发生了改变    void on_m_selection_currentChanged(const QModelIndex &amp;current,                                        const QModelIndex &amp;previous);private:    void  initModel(QStringList const&amp; strings);    Ui::TableWindow *ui;    QLabel* m_labCurFile;   // 状态栏显示信息    QLabel* m_labCellPos;   // 状态栏显示信息    QLabel* m_labCellText;  // 状态栏显示信息    QStandardItemModel* m_model;    QItemSelectionModel* m_selection;&#125;;\n\nTableWindow的构造函数实现如下：\nTableWindow::TableWindow(QWidget *parent)    : QMainWindow(parent)    , ui(new Ui::TableWindow)    , m_labCurFile(new QLabel(&quot;当前文件: &quot;))    , m_labCellPos(new QLabel(&quot;单元格位置: &quot;))    , m_labCellText(new QLabel(&quot;单元格内容: &quot;))    , m_model(new QStandardItemModel(this))    , m_selection(new QItemSelectionModel(m_model))&#123;    ui-&gt;setupUi(this);    m_labCurFile-&gt;setMinimumWidth(420);    ui-&gt;statusBar-&gt;addWidget(m_labCurFile);    m_labCellPos-&gt;setMinimumWidth(190);    ui-&gt;statusBar-&gt;addWidget(m_labCellPos);    m_labCellText-&gt;setMinimumWidth(190);    ui-&gt;statusBar-&gt;addWidget(m_labCellText);    ui-&gt;m_table-&gt;setModel(m_model); // view和model的绑定    ui-&gt;m_table-&gt;setSelectionModel(m_selection);    connect(m_selection,SIGNAL(currentChanged(QModelIndex,QModelIndex)),            this, SLOT(on_m_selection_currentChanged(QModelIndex,QModelIndex)));    // 默认代理是一个编辑框。     // 添加一个自定义代理    // 选择对于某一列的数据添加自定义代理    ui-&gt;m_table-&gt;setItemDelegateForColumn(3, new ComboBoxDelegate);&#125;\n\n加粗的实现：\n// 加粗Action的槽函数void TableWindow::on_m_actBold_triggered(bool checked)&#123;    // selectedIndexes - 获取所有的选中的项的索引    for(QModelIndex const&amp; index :        m_selection-&gt;selectedIndexes())&#123;        QStandardItem* item = m_model-&gt;itemFromIndex(index);        QFont font = item-&gt;font();// 获取该item的字体        font.setBold(checked);        item-&gt;setFont(font);    &#125;&#125;\n\n打开文件的实现：\n// 打开Action的槽函数void TableWindow::on_m_actOpen_triggered()&#123;    // 1.获取文件的路径    QString path = QFileDialog::getOpenFileName(this, &quot;打开&quot;,                       QCoreApplication::applicationDirPath(),                        &quot;逗号分隔符文件(*.csv);;所有文件(*.*)&quot;);    if(path.isEmpty())        return;    //2.打开文件    // 构造一个QFile对象表示的就是path所指向的文件    QFile file(path);    // 打开file指向的文件 - 以只读和文本模式打开该文件    if(!file.open(QIODevice::ReadOnly | QIODevice::Text))        return;    // 3.读取文件    QTextStream stream(&amp;file);// 构建文本流对象, 关联已经打开的文件    //  stream.setCodec(&quot;utf-8&quot;);    QStringList strings;    // readLine - 读一行  atEnd - 文件末尾    // 将数据读出来后 - 放到strings列表中    while(!stream.atEnd())        strings.append(stream.readLine());    file.close();    initModel(strings);    m_labCurFile-&gt;setText(&quot;当前文件: &quot; + path);&#125;void  TableWindow::initModel(QStringList const&amp; strings)&#123;    m_model-&gt;clear();    // 1.strings.at(0)   获取的是strings字符串列表中的第一个字符串    // 设置列的标题 - 参数1:使用逗号将数据分割, 参数2:忽略空字符串    // 将分割后的字符串作为列的标题    m_model-&gt;setHorizontalHeaderLabels(                    strings.at(0).split(&quot;,&quot;, Qt::SkipEmptyParts));    // 2.填充数据 - 填充n-1次    // values = 1001 张飞 1996-09-12 男 13512345678    // 2.3.根据列数获取要循环放入数据的次数    int rowCount = strings.count() - 1;    for(int row = 0; row &lt; rowCount; row++)&#123;        QStringList values = strings.at(row + 1).split(&quot;,&quot;,                                                       Qt::SkipEmptyParts);        int columnCount = values.size();        for(int column = 0; column &lt; columnCount; column++)&#123;            m_model-&gt;setItem(row, column,                             new QStandardItem(values.at(column)));        &#125;    &#125;    m_selection-&gt;setCurrentIndex(                m_model-&gt;index(0,0), QItemSelectionModel::Select);&#125;\n\n保存的实现：\n//  保存Action对应的槽函数void TableWindow::on_m_actSave_triggered()&#123;    // 1.获取保存文件路径    QString path = QFileDialog::getSaveFileName(this, &quot;保存&quot;,                       QCoreApplication::applicationDirPath(),                        &quot;逗号分隔符文件(*.csv);;所有文件(*.*)&quot;);    if(path.isEmpty())        return;    // 2.打开文件    // 构造一个QFile对象表示的就是path所指向的文件    QFile file(path);    // 打开file指向的文件 - 以可读可写和文本模式  + 清空原文件打开该文件    if(!file.open(QIODevice::ReadWrite | QIODevice::Text | QIODevice::Truncate))        return;    // 3.写入文件    QTextStream stream(&amp;file);// 构建文本流对象, 关联已经打开的文件    //  stream.setCodec(&quot;utf-8&quot;);    // 4.写入列标题    // 学号  姓名  出生日期  性别  电话    // 循环遍历模型的列 逐列写入标题到文件中    int columnCount = m_model-&gt;columnCount();    for(int col = 0; col &lt; columnCount; col++)&#123;        stream &lt;&lt; m_model-&gt;horizontalHeaderItem(col)-&gt;text() &lt;&lt;                  (col == columnCount - 1 ? &quot;\\n&quot; : &quot;,&quot;);    &#125;    // 5.写入数据行    int rowCount = m_model-&gt;rowCount();    for(int row = 0; row &lt; rowCount; row++)    &#123;        for(int col = 0; col &lt; columnCount; col++)        &#123;            stream &lt;&lt; m_model-&gt;item(row, col)-&gt;text() &lt;&lt;                      (col == columnCount - 1 ? &quot;\\n&quot; : &quot;,&quot;);        &#125;    &#125;    // 6.关闭文件    file.close();&#125;\n\n预览的实现：\n//预览Action对应的槽函数void TableWindow::on_m_actPreview_triggered()&#123;    // 1.清空QPlainTextEdit内容    ui-&gt;m_edit-&gt;clear();    // 2.显示列标题    QString text;    int columnCount = m_model-&gt;columnCount();    for(int col = 0; col &lt; columnCount; col++)    &#123;        text += m_model-&gt;horizontalHeaderItem(col)-&gt;text()                + (col == columnCount - 1 ? &quot;&quot; : &quot;,&quot;);    &#125;    ui-&gt;m_edit-&gt;appendPlainText(text);    // 3.显示数据行    int rowCount = m_model-&gt;rowCount();    for(int row = 0; row &lt; rowCount; row++)    &#123;        QString text;        for(int col = 0; col &lt; columnCount; col++)        &#123;            text += m_model-&gt;item(row, col)-&gt;text() +                    (col == columnCount - 1 ? &quot;&quot; : &quot;,&quot;);        &#125;        ui-&gt;m_edit-&gt;appendPlainText(text);    &#125;&#125;\n\n添加的实现：\n// 添加Action对应的槽函数void TableWindow::on_m_actAppend_triggered()&#123;    // 1.先获取有多少列    int columnCount = m_model-&gt;columnCount();    if(!columnCount)        return;    // 2.创建空数据项    // 创建新的空数据行 - 需要columnCount个数据项    QList&lt;QStandardItem*&gt; items;// 用于存储空的数据项    for(int col = 0; col &lt; columnCount; col++)        items &lt;&lt; new QStandardItem;    // 3.插入新行    m_model-&gt;insertRow(m_model-&gt;rowCount(), items);    // 4.设置当前选择    m_selection-&gt;clearSelection();    m_selection-&gt;setCurrentIndex(                m_model-&gt;index(m_model-&gt;rowCount() -1, 0), QItemSelectionModel::Select);&#125;\n\n插入的实现：\n// 插入Action对应的槽函数void TableWindow::on_m_actInsert_triggered()&#123;    // 1.先获取有多少列    int columnCount = m_model-&gt;columnCount();    if(!columnCount)        return;    // 2.创建空数据项    // 创建新的空数据行 - 需要columnCount个数据项    QList&lt;QStandardItem*&gt; items;// 用于存储空的数据项    for(int col = 0; col &lt; columnCount; col++)        items &lt;&lt; new QStandardItem;    // 3.获取当前的模型索引    QModelIndex current = m_selection-&gt;currentIndex();    m_model-&gt;insertRow(current.row(), items);    // 4.设置当前选择    m_selection-&gt;clearSelection();    m_selection-&gt;setCurrentIndex(current, QItemSelectionModel::Select);&#125;\n\n删除的实现：\n// 删除Action对应的槽函数void TableWindow::on_m_actDelete_triggered()&#123;    // 1.先获取当前位置的索引    // 1.1.获取上一行,同一列的位置(上一行, 同一列)    // 1.2.判断当前行是否是最后一行    QModelIndex current = m_selection-&gt;currentIndex();    QModelIndex above = m_model-&gt;index( current.row() - 1, current.column());    bool last = current.row() == m_model-&gt;rowCount() - 1;    // 2.移除选中行    m_model-&gt;removeRow(current.row());    m_selection-&gt;setCurrentIndex(last ? above : current,                                 QItemSelectionModel::Select);&#125;\n\n对齐的实现：\n// 左对齐Action对应的槽函数void TableWindow::on_m_actLeft_triggered()&#123;    // 1.遍历选中的索引    for(QModelIndex const&amp; index : m_selection-&gt;selectedIndexes())    &#123;        m_model-&gt;itemFromIndex(index)-&gt;setTextAlignment(                    Qt::AlignLeft | Qt::AlignVCenter);    &#125;&#125;// 中对齐Action对应的槽函数void TableWindow::on_m_actCenter_triggered()&#123;    // 1.遍历选中的索引    for(QModelIndex const&amp; index : m_selection-&gt;selectedIndexes())    &#123;        m_model-&gt;itemFromIndex(index)-&gt;setTextAlignment(                    Qt::AlignHCenter | Qt::AlignVCenter);    &#125;&#125;// 右对齐Action对应的槽函数void TableWindow::on_m_actRight_triggered()&#123;    // 1.遍历选中的索引    for(QModelIndex const&amp; index : m_selection-&gt;selectedIndexes())    &#123;        m_model-&gt;itemFromIndex(index)-&gt;setTextAlignment(                    Qt::AlignRight | Qt::AlignVCenter);    &#125;&#125;\n\n状态栏文本动态变化的实现：\n// 选择项改变对应的槽函数void TableWindow::on_m_selection_currentChanged(const QModelIndex &amp;current,                                                const QModelIndex &amp;previous)&#123;    Q_UNUSED(previous);    if(!current.isValid())        return;    // 添加单元格位置    m_labCellPos-&gt;setText(                QString(&quot; 单元格位置:第%1行, 第%2列 &quot;).                arg(current.row() + 1).                arg(current.column() + 1));    // 添加单元格内容    QStandardItem* item = m_model-&gt;itemFromIndex(current);    m_labCellText-&gt;setText(&quot; 单元格内容: &quot; + item-&gt;text());    // 检查并更新粗体显示状态    // item-&gt;font().bold() - 判断选中的item的字体是否为 粗体    ui-&gt;m_actBold-&gt;setChecked(item-&gt;font().bold());&#125;\n\n运行效果如下：\n\n\n本章完结\n","tags":["QT"]},{"title":"增加一个Flow节点的流程","url":"/2025/07/11/rkmedia/add_flow/","content":"什么是Flow？从字面意思上理解，Flow就是流，在音视频领域，常常就能听到什么视频流、音频流，当然这些都可以统称为数据流。\n在RKMedia当中，也存在一套基于Flow（流）的专门处理音视频流的框架。这里面还牵扯pipeline的概念，所谓pipeline就是一条独立的流水线，当每一帧的原始视频图像（一般是nv12格式）流入某条流水线后，会经过一些加工处理（比如对原始图像进行缩放、裁剪、编码等），最后输出的这一帧成品数据才会被推流到服务器，具体对一帧图像的加工就是由Flow负责，当然RKMedia把流水线思想用的特别精妙，因为加工也会分很多工序，所以对应的Flow的派生类也会分很多种。每种flow分工明确，只关注自己应该对图像的那一小部分工序，然后就把加工后的图像传递给下一级flow。\n这里贴一张图，方便理解pipeline和Flow：\n\nsource flow会从相机当中读取图像数据。\n\n\nrga对图像进行缩放。\nencode对对图像进行编码。\nmuxer flow会将音视频进行一个再封装。然后推流到服务器。\nFlow上下级如何交互的？首先是有一个pipeline和flow的配置文件，配置文件描述了有几条pipeline，以及每条pipeline内部flow节点之间是什么关系，还有flow内部初始化必要的参数。\n在我们的项目中，主要使用的是：ipcamera&#x2F;app&#x2F;mediaserver&#x2F;src&#x2F;conf&#x2F;dc-rv1126-ipc&#x2F;ipc-yuyv.conf。要添加一个flow节点首先就需要在此配置文件中定义一下flow必要的属性。\n截取部分配置如下：\n&#123;    &quot;Pipe_0&quot;: &#123;        &quot;Flow_0&quot;: &#123;            &quot;flow_index&quot;: &#123;                &quot;flow_index_name&quot;: &quot;source_0&quot;,                &quot;flow_type&quot;: &quot;source&quot;,                &quot;stream_id&quot;: &quot;0&quot;,                &quot;stream_type&quot;: &quot;camera&quot;,                &quot;upflow_index_name&quot;: &quot;none&quot;            &#125;,            &quot;flow_name&quot;: &quot;source_stream_mipi&quot;,            &quot;flow_param&quot;: &#123;                &quot;name&quot;: &quot;v4l2_capture_stream&quot;            &#125;,            &quot;stream_param&quot;: &#123;                &quot;device&quot;: &quot;rkispp_m_bypass&quot;,                &quot;frame_num&quot;: &quot;6&quot;,                &quot;height&quot;: &quot;1520&quot;,                &quot;output_data_type&quot;: &quot;image:nv12&quot;,                &quot;use_libv4l2&quot;: &quot;1&quot;,                &quot;v4l2_capture_type&quot;: &quot;VIDEO_CAPTURE&quot;,                &quot;v4l2_mem_type&quot;: &quot;MEMORY_DMABUF&quot;,                &quot;virtual_height&quot;: &quot;1520&quot;,                &quot;virtual_width&quot;: &quot;2688&quot;,                &quot;width&quot;: &quot;2688&quot;            &#125;        &#125;,        // ...    &#125;,\t&quot;Pipe_1&quot;: &#123;        // ...    &#125;,    // ...&#125;\n\n在app下的Mediaserver中，flowmanager会根据配置文件中的flow_name的值通过反射创建具体的flow对象。每个flow的派生类都会在其构造函数中调用父类的Flow::InstallSlotMap方法去创建输入槽和输出槽，这两种槽可以简单理解为输入缓冲池和输出缓存池。\nflowmanager在创建flow后会对flow进行初始化，此时会根据配置文件所描述的flow之间的上下级关系来调用上级的Flow::AddDownFlow，将父级的输出槽和子级Flow做绑定。将来，父级flow调用Flow::SetOutput可将处理后的图像数据传递给子级的flow。\n这里补充一个细节：在调用Flow::InstallSlotMap方法时，除了槽的描述信息之外，还会向flow注册一个回调函数，该回调函数就是flow处理流进来的图像的核心函数，对于flow_type为io的flow，会在该回调函数的结尾调用Flow::SetOutput，最后会导致加工后的图像流向下一级flow。\n于此，我们就有了向rkmedia添加flow节点的思路。\n在项目中添加Flow的流程事先声明一下，为方便，我这里直接添加了一个flow_type为sink的flow，添加io类型的flow其实同理。\n修改配置文件在配置文件ipcamera&#x2F;app&#x2F;mediaserver&#x2F;src&#x2F;conf&#x2F;dc-rv1126-ipc&#x2F;ipc-yuyv.conf，Pipe_0最后添加一个flow节点，配置如下：\n&#123;    &quot;Pipe_0&quot;: &#123;        // ...        &quot;Flow_7&quot;: &#123;            &quot;flow_index&quot;: &#123;                &quot;flow_index_name&quot;: &quot;test_flow&quot;,                &quot;flow_type&quot;: &quot;sink&quot;,                &quot;in_slot_index_of_down&quot;: &quot;0&quot;,                &quot;out_slot_index&quot;: &quot;0&quot;,                &quot;stream_type&quot;: &quot;file&quot;,                &quot;upflow_index_name&quot;: &quot;video_enc_0&quot;            &#125;,            &quot;flow_name&quot;: &quot;test_flow&quot;,            &quot;flow_param&quot;: &#123;                &quot;mode&quot;: &quot;w+&quot;,                &quot;path&quot;: &quot;/userdata/media/photo0&quot;,                &quot;file_prefix&quot;: &quot;rga_test&quot;,                &quot;file_suffix&quot;: &quot;.jpeg&quot;,                &quot;save_mode&quot;: &quot;single_frame&quot;            &#125;,            &quot;stream_param&quot;: &#123;&#125;        &#125;    &#125;,\t&quot;Pipe_1&quot;: &#123;        // ...    &#125;,    // ...&#125;\n\n我定义的flow_name为rga_flow，为使反射匹配成功，程序相应的getflowname函数必须返回rga_flow。\n并且rga_flow的上级flow的flow_index_name为video_enc_0，video_enc_0其实就是一个编码器。\nin_slot_index_of_down和out_slot_index定义了将rga_test放在上级输出槽的哪个槽以及上级处理完图像后，将图像帧输出到子级的那个输入槽。\n编写flow派生类Flow的编写可以参考ipcamera&#x2F;external&#x2F;rkmedia&#x2F;src&#x2F;flow的实现。如下：\nnamespace easymedia &#123;static bool test_flow_cb(Flow *f, MediaBufferVector &amp;input_vector);class TestFlow : public Flow &#123;public:  TestFlow(const char *param);  virtual ~TestFlow();  static const char *GetFlowName() &#123; return &quot;test_flow&quot;; &#125;private:  friend bool test_flow_cb(Flow *f, MediaBufferVector &amp;input_vector);private:  // for test  std::chrono::steady_clock::time_point last_print_time_;&#125;;TestFlow::TestFlow(const char *param) : last_print_time_(std::chrono::steady_clock::now())&#123;  std::map&lt;std::string, std::string&gt; params;  // flow_param  if (!parse_media_param_map(param, params)) &#123;    SetError(-EINVAL);    return;  &#125;  /* to do ... parse config file. */  SlotMap sm;  sm.input_slots.push_back(0);  sm.thread_model = Model::ASYNCCOMMON;  sm.mode_when_full = InputMode::DROPFRONT;  sm.input_maxcachenum.push_back(0);        // 0就是input buffer大小无限制  sm.process = rga_flow_cb;  // no output  if (!InstallSlotMap(sm, &quot;TestFlow&quot;, 0)) &#123;    RKMEDIA_LOGI(&quot;Fail to InstallSlotMap for FileWriteFlow\\n&quot;);    return;  &#125;  SetFlowTag(&quot;TestFlow&quot;);&#125;TestFlow::~TestFlow() &#123;  StopAllThread();&#125;bool test_flow_cb(Flow *f, MediaBufferVector &amp;input_vector) &#123;    RGAFlow *flow = static_cast&lt;RGAFlow *&gt;(f);    auto &amp;buffer = input_vector[0];    UNUSED(buffer);    long int interval =      (std::chrono::duration_cast&lt;std::chrono::seconds&gt;(            std::chrono::steady_clock::now() - flow-&gt;last_print_time_))          .count();    // 每5秒打印一条日志    if (interval &gt; 5) &#123;      RKMEDIA_LOGI(&quot;TestFlow: hello world!\\n&quot;);      flow-&gt;last_print_time_ = std::chrono::steady_clock::now();    &#125;    // Is sink, so no need to call Flow::SetOutput    // 如果是io类型的flow还需要调用Flow::SetOutput将处理后的图像传递给下一级    return true;&#125;DEFINE_FLOW_FACTORY(TestFlow, Flow)const char *FACTORY(TestFlow)::ExpectedInputDataType() &#123; return nullptr; &#125;const char *FACTORY(TestFlow)::OutPutDataType() &#123; return &quot;&quot;; &#125;&#125; // namespace easymedia\n\n结果如下：\n\n\n每5秒向终端输出一条内容为：TestFlow: hello world! 的日志。\n\n\n\n本章结束\n","tags":["音视频","Rkmedia","流媒体服务器框架"]},{"title":"rkmedia之flow深入浅出","url":"/2025/05/05/rkmedia/kernel/","content":"前言谈到Flow其实绕不开Pipeline的概念，而Pipeline就是字面意思——流水线。一条Pipeline由多个flow节点构成。每个flow节点会对输入进来的数据做特殊的处理，再将数据发送给下一级flow节点，因此不同的flow节点承担不同的任务。\n在数据库领域也存在PipeLine的工程实践：一条查询sql语句最终会被解析成一个个查询计划（查询计划会被连接成Pipeline形式的树形结构），执行器会利用所绑定的查询计划，对传来的表数据进行处理，然后讲处理好的表数据传递给下一级节点。这里放上一张一条聚合查询语句被解析后所构建的Pipeline模型：\n\n上图是如下sql语句被解析后所构建的Pipeline：\n\nbustub&gt; EXPLAIN (o) SELECT colA, MAX(colB) FROM  (SELECT * FROM __mock_table_1, __mock_table_3 WHERE colA = colE) GROUP BY colA;=== OPTIMIZER ===Agg &#123; types=[max], aggregates=[#0.1], group_by=[#0.0] &#125;  NestedLoopJoin &#123; type=Inner, predicate=(#0.0=#1.0) &#125;    MockScan &#123; table=__mock_table_1 &#125;    MockScan &#123; table=__mock_table_3 &#125;\n\n从图中可以看到，在执行聚合查询时，表的扫描、表的连接、聚合操作分别由不同的节点承担。表数据的流向从下到上，所以又称为火山模型。\n在音视频当中，我们可以不严格的将flow分为三类：Source、IO、Sink。而一般数据流向顺序为：Source -&gt; IO -&gt; …(可能经过多个类型为IO的有不同业务逻辑的flow节点处理) -&gt; IO -&gt; Sink。\n\nSource：专门利用V4L2接口负责从摄像头当中获取图像数据。\n\nIO：专门对图像数据进行处理，比如：裁剪&#x2F;缩放、编码、Guard（控制拍摄图片张数）、标定、拼接等（依据具体的业务场景有不同的扩展类型）。\n\nSink：专门将编码好的图像数据 推流到rtmp&#x2F;rtsp服务器、保存成.mp4格式的文件或者JPEG格式的图片。\n\n\n从rkmedia当中认识不同功能flow前面提到的一些概念其实是比较抽象的，如果你看过一遍，也就看过一遍了。通过代码将抽象的东西具象化，才能在脑海当中加深印象。这里只放出文件，可以提前给大家解个惑，这里所列出的各种xxx_flow.cc其实是我们后面深入要讲的 Flow的派生类。xxx_flow.cc文件核心是实现了一个业务回调函数，用户只需在该回调函数当中根据业务需要编写的纯业务代码。这部分其实和本文主题是有所偏差。这里列出比较常用的几个flow。强烈建议读者学习一下这几个flow的源码，然后可以模仿添加一个自己的flow（即使你写的flow什么也不干）。可以感受一个flow当中，数据获取、处理、发送的套路。\n文件路径：external&#x2F;rkmedia&#x2F;src&#x2F;flow\n\n\n\n文件名\n作用\n\n\n\nsource_stream_flow.cc\n类型为Source的flow，利用v4l2接口从摄像头当中获取图像数据。\n\n\nfilter_flow.cc\n这个flow有段特殊，属于IO flow，可以通过配置，将其配置为不同业务功能的flow，比如前面提到过的裁剪&#x2F;缩放、Guard等功能，如果对代码足够了解的话，还可以为它编写自己的Filter来实现特定的功能。\n\n\nvideo_encoder_flow.cc\n这个flow同一属于IO类型的flow，专用于对图像数据进行编码，可按需将图像数据编码成H264&#x2F;H265&#x2F;JPEG等格式，通过对编码器进行配置可以很容易实现这点。一般会通过需改mediaserver的.conf文件（json格式）来对flow节点进行配置。\n\n\nfile_flow.cc\n属于Sink类型的flow，一般将已经编码成JPEG格式的图像数据写入到文件当中。也即保存JPEG图像\n\n\nmuxer_flow.cc&#x2F;h\n属于Sink类型的flow，将编码成H264&#x2F;H265的图像数据保存成.mp4文件 或者 推流到rtmp\n\n\n上表所列原文件会有一些共同点：\n\n继承自Flow。\n\n在构造函数当中都会解析配置，最重要的是：会创建一个类型为SlotMap的对象，然后填充好输入&#x2F;输出属性和业务回调函数 后统一调用了父类的Flow::InstallSlotMap函数进行安装。\n\n在业务回调函数当中通过f参数拿到派生类对象，通过input_vector拿到从上一级flow节点传过来的图像数据，然后对图像进行处理，最后通过Flow::SetOutput函数，将处理完毕的图像数据发送给下一级节点。\n\n源文件尾部使用统一步骤，向反射工厂注册了自定义的继承自Flow的派生类。方便通过反射机制创建对象。\n\n\n总结下来，可以按照这一套思路定义自己的Flow节点。可以参考文章：如何添加一个flow节点？ 试着动手添加一个自己的flow节点。\n到这里，我相信你心中一定会有很多疑惑：\n\n为什么要这样加flow节点？\n\n我们自定义的回调函数会在什么时机进行回调？\n\n谁会给我们自定义的回调函数传参？\n\nFlow上下级是如何连接的？\n\nSlotMap对象各个属性的秘密？\n\n调用Flow::SetOutput函数后，数据怎么就传给了下一级节点？\n\n\n别着急，我们下面会揭晓背后的秘密。\n从mediaserver当中感受Pipeline的建立首先贴一张最简单的Pipeline的结构图：\n\n图像数据流向：\ncapture flow（Source）利用v4l2接口获取图像数据，将图像数据传给编码节点（IO），编码节点将图像数据编码成H264然后分别转发发给 RTMP推流（Sink）节点和 将视频流保存为MP4格式（Sink）节点。\n广义上讲，其实Pipeline就是广义上的数据结构——树。当你后面深入了解到flow的实现后，会发现，Pipeline和树的唯一区别：树的节点仅仅是存储数据的，而Pipeline节点是包含一些处理数据的逻辑资源（包含线程、需要执行的业务代码等）。\n上图Pipeline示例结构在mediaserver当中会以json配置文件的形式存在，如下：\n配置文件一般路径：app&#x2F;mediaserver&#x2F;src&#x2F;conf&#x2F;\n&#123;    &quot;Pipe_0&quot;: &#123;        &quot;Flow_0&quot;: &#123;            &quot;flow_index&quot;: &#123;                &quot;flow_index_name&quot;: &quot;source_0&quot;,                &quot;flow_type&quot;: &quot;source&quot;,                &quot;stream_id&quot;: &quot;0&quot;,                &quot;stream_type&quot;: &quot;camera&quot;,                &quot;upflow_index_name&quot;: &quot;none&quot;            &#125;,            &quot;flow_name&quot;: &quot;source_stream&quot;,            &quot;flow_param&quot;: &#123;                &quot;name&quot;: &quot;v4l2_capture_stream&quot;            &#125;,            &quot;stream_param&quot;: &#123;                &quot;device&quot;: &quot;rkispp_m_bypass&quot;,                &quot;frame_num&quot;: &quot;6&quot;,                &quot;height&quot;: &quot;2160&quot;,                &quot;output_data_type&quot;: &quot;image:nv12&quot;,                &quot;use_libv4l2&quot;: &quot;1&quot;,                &quot;v4l2_capture_type&quot;: &quot;VIDEO_CAPTURE&quot;,                &quot;v4l2_mem_type&quot;: &quot;MEMORY_DMABUF&quot;,                &quot;virtual_height&quot;: &quot;2160&quot;,                &quot;virtual_width&quot;: &quot;3840&quot;,                &quot;width&quot;: &quot;3840&quot;            &#125;        &#125;,        &quot;Flow_1&quot;: &#123;            &quot;flow_index&quot;: &#123;                &quot;flow_index_name&quot;: &quot;video_enc_0&quot;,                &quot;flow_type&quot;: &quot;io&quot;,                &quot;in_slot_index_of_down&quot;: &quot;0&quot;,                &quot;out_slot_index&quot;: &quot;0&quot;,                &quot;stream_type&quot;: &quot;video_enc&quot;,                &quot;upflow_index_name&quot;: &quot;source_0&quot;            &#125;,            &quot;flow_name&quot;: &quot;video_enc&quot;,            &quot;flow_param&quot;: &#123;                &quot;input_data_type&quot;: &quot;image:nv12&quot;,                &quot;name&quot;: &quot;rkmpp&quot;,                &quot;need_extra_merge&quot;: &quot;1&quot;,                &quot;output_data_type&quot;: &quot;video:h265&quot;            &#125;,            &quot;stream_param&quot;: &#123;                &quot;input_data_type&quot;: &quot;image:nv12&quot;,                &quot;output_data_type&quot;: &quot;video:h265&quot;,                &quot;virtual_height&quot;: &quot;2160&quot;,                &quot;virtual_width&quot;: &quot;3840&quot;,                &quot;width&quot;: &quot;3840&quot;,                &quot;height&quot;: &quot;2160&quot;,                /* 省略一大串编码相关的配置参数 ... */            &#125;        &#125;,        &quot;Flow_2&quot;: &#123;            &quot;flow_index&quot;: &#123;                &quot;flow_index_name&quot;: &quot;muxer_0&quot;,                &quot;flow_type&quot;: &quot;sink&quot;,                &quot;in_slot_index_of_down&quot;: &quot;0&quot;,                &quot;out_slot_index&quot;: &quot;0&quot;,                &quot;stream_type&quot;: &quot;muxer&quot;,                &quot;upflow_index_name&quot;: &quot;video_enc_0&quot;            &#125;,            &quot;flow_name&quot;: &quot;muxer_flow&quot;,            &quot;flow_param&quot;: &#123;                &quot;name&quot;: &quot;muxer_flow&quot;,                &quot;path&quot;: &quot;rtmp://127.0.0.1:1935/live/mainstream&quot;,                &quot;output_data_type&quot;: &quot;flv&quot;            &#125;,            &quot;stream_param&quot;: &#123;&#125;        &#125;,        &quot;Flow_3&quot;: &#123;            &quot;flow_index&quot;: &#123;                &quot;flow_index_name&quot;: &quot;muxer_1&quot;,                &quot;flow_type&quot;: &quot;sink&quot;,                &quot;in_slot_index_of_down&quot;: &quot;0&quot;,                &quot;out_slot_index&quot;: &quot;0&quot;,                &quot;stream_type&quot;: &quot;muxer&quot;,                &quot;upflow_index_name&quot;: &quot;video_enc_0&quot;            &#125;,            &quot;flow_name&quot;: &quot;muxer_flow&quot;,            &quot;flow_param&quot;: &#123;                &quot;file_duration&quot;: &quot;60&quot;,                &quot;file_index&quot;: &quot;1&quot;,                &quot;file_time&quot;: &quot;1&quot;,                &quot;path&quot;: &quot;/userdata/media/video0&quot;,                &quot;file_prefix&quot;: &quot;main_vible&quot;,                &quot;name&quot;: &quot;muxer_flow&quot;,                &quot;enable_streaming&quot;: &quot;false&quot;            &#125;,            &quot;stream_param&quot;: &#123;&#125;        &#125;    &#125;&#125;\n\n从json配置文件当中，我们可以直观了解到，上面这段json配置 和 本小段开头所放的Pipeline结构图是相互对应的。我们可以预见，mediaserver会解析json文件，然后自动创建各种类型的flow节点，然后将他们连接起来。那么，mediaserver是怎么去实例化各种各样的flow对象的呢？\n如果你认真阅读了上一段讲解手动添加一个flow的部分，你一定留意过，在实现自己的flow时，会必须实现一个静态函数：GetFlowName，该函数会返回一个字符串，代表flow name，同时，在自定义一个flow类后，文件末尾会公式化的添上几行有关反射的代码。这一步实际上会向反射工厂注册我们自定义的Flow节点。\njson配置文件当中，每个flow节点的 flow_name 参数指定的值就是配合反射来创建一个个实例化对象的。事实上json文件当中的flow_name值，必须等于我们自定义flow所实现静态函数：GetFlowName的返回值。mediaserver当中会利用json配置文件里面每一个flow节点的flow_name字段到反射工程当中去创建Flow的实例化对象，反射工程会利用我们自定义Flow时所实现的GetFlowName函数返回值和flow_name做比对最终确定去实例化哪一个自定义的Flow。 \n现在注意集中在json配置文件当中，从配置文件当中可以了解到，一个flow节点有四个参数：flow_index、flow_name、flow_param、stream_param\n\nflow_index：这个参数当中的内容是本文我们需要重点关注的，它定义了flow节点上下级关系以及flow节点的类型。\n\nflow_name：该参数上面详细描述过，主要告诉反射机制去实例化哪一种类型的flow。\n\nflow_param：自定义的flow节点在实例化过程中自身可能需要的一些参数。该参数本节无需重点关心！\n\nstream_param：自定义的flow节点可能会借助其他的子插件，来实现自己的业务逻辑，比如编码节点会引用mpp的东西，而mpp本身的创建又需要一些参数，stream_param定义的一些参数就是为实例化子插件而生的。该参数本节无需重点关心！\n\n\n最后，我们将注意集中在mediaserver堆Pipeline的构建上。如下\n在app&#x2F;mediaserver&#x2F;src&#x2F;mediaserver.cpp文件当中，mediaserver.cpp是mmediaserver main函数所以文件，里面定义了MediaServer类，main写了什么我们无需关心，理所应当的是，main函数一定创建了MediaServer对象，重点关注MediaServer的构造函数：\nMediaServer::MediaServer() &#123;  LOG_DEBUG(&quot;media servers setup ...\\n&quot;);  // 略 ...  flow_manager-&gt;ConfigParse(media_config);  flow_manager-&gt;CreatePipes();  // 略 ...  LOG_DEBUG(&quot;media servers setup ok\\n&quot;);&#125;\n\n第一步，解析json配置，app&#x2F;mediaserver&#x2F;src&#x2F;flows&#x2F;flow_manager.cpp对ConfigParse的实现：\nint FlowManager::ConfigParse(std::string conf) &#123;  LOG_INFO(&quot;flow manager parse config\\n&quot;);  // 解析json配置文件并构建FlowParser对象（后面会利用FlowParser对象真正创建每一个flow）  flow_parser_.reset(new FlowParser(conf.c_str()));  // 将数据库的配置同步到mediaserver当中。  SyncConfig();  return 0;&#125;\n\n第二步，构建Pipeline。\nint FlowManager::CreatePipes() &#123;  LOG_INFO(&quot;flow manager create flow pipe\\n&quot;);  for (int index = 0; index &lt; flow_parser_-&gt;GetPipeNum(); index++) &#123;    auto flow_pipe = std::make_shared&lt;FlowPipe&gt;();    auto &amp;flow_units = flow_parser_-&gt;GetFlowUnits(index);    flow_pipe-&gt;CreateFlows(flow_units);    flow_pipes_.emplace_back(flow_pipe);  &#125;  // ...  for (int index = 0; index &lt; flow_pipes_.size(); index++) &#123;    auto &amp;flow_pipe = flow_pipes_[index];    flow_pipe-&gt;InitFlows();  &#125;  // ...  return 0;&#125;\n\n利用反射实例化flow对象（flow之间连接还未建立）：\nvoid FlowPipe::CreateFlows(flow_unit_v &amp;flows) &#123;  std::string param;  for (auto &amp;iter : flows) &#123;    int ret = CreateFlow(iter);    if (!ret)      flow_units_.emplace_back(iter);  &#125;&#125;int FlowPipe::CreateFlow(std::shared_ptr&lt;FlowUnit&gt; flow_unit) &#123;  // ...  // 反射实例化flow对象  auto flow = easymedia::REFLECTOR(Flow)::Create&lt;easymedia::Flow&gt;(      flow_name.c_str(), param.c_str());  if (!flow) &#123;    LOG_ERROR(&quot;Create flow %s failed\\n&quot;, flow_name.c_str());    LOG_ERROR(&quot;flow param :\\n%s\\n&quot;, param.c_str());    exit(EXIT_FAILURE);  &#125;  flow-&gt;RegisterEventHandler(flow, FlowEventProc);  flow_unit-&gt;SetFlow(flow);  return 0;&#125;\n\n连接flow，构建Pipeline（建立flow之间的连接）：\nint FlowPipe::InitFlows() &#123;  for (int flow_index = flow_units_.size() - 1; flow_index &gt;= 0; flow_index--) &#123;    InitFlow(flow_index);  &#125;  return 0;&#125;int FlowPipe::InitFlow(int flow_index) &#123;  auto &amp;flow_unit = flow_units_[flow_index];  auto &amp;flow = flow_unit-&gt;GetFlow();  auto tsream_type = flow_unit-&gt;GetStreamType();  auto flow_type = flow_unit-&gt;GetFlowType();  if (FlowType::SOURCE == flow_type)    return 0;  // ...  // 节点上级的 flow_index_name  auto upflow_index_name = flow_unit-&gt;GetUpFlowIndexName();  // 把图像数据传递给本级flow的哪一个输入槽。  int out_slot_index = flow_unit-&gt;GetOutSlotIndex();  // 本级flow和上级flow哪一个输出槽做绑定。  int in_slot_index_of_down = flow_unit-&gt;GetInSlotIndexOfDown();  // 一个flow节点可以有多个上级。  auto v = SplitStringToVector(upflow_index_name);  for (auto name : v) &#123;    int upflow_index = GetFlowIndex(name);    auto &amp;upflow = flow_units_[upflow_index]-&gt;GetFlow();    // 根据输入槽和输出槽，建立flow上下级联系。    upflow-&gt;AddDownFlow(flow, in_slot_index_of_down, out_slot_index);    out_slot_index++;  &#125;  return 0;&#125;\n\n这里备注一下flow_index_name和flow_name的区别，flow_index_name在同一个pipeline下是唯一的，用于唯一索引同一个pipeline下的flow节点。同一Pipeline下，不同的flow_index_name可能有相同的flow_name。flow_name用于给反射机制使用，去实例化不同类型的flow节点。\n有关输入槽&#x2F;输出槽的概念，读者可能会有些迷糊。不要急，下一小结，将会解答这里的疑惑。\nFlow的实现前面已经花了大量篇幅，介绍Flow的作用，以及在mediaserver当中，Flow是怎么一步步被构建成PipeLine的。本段将深入讲解Flow的具体实现，一一解答上面抛出的疑问。\n源码之前了无秘密。现在把注意力集中在两个最重要的文件：\n\nexternal&#x2F;rkmedia&#x2F;src&#x2F;flow.cc\nexternal&#x2F;rkmedia&#x2F;include&#x2F;easymedia&#x2F;flow.h\n\n首先是两个枚举类的定义：\nenum class Model &#123; NONE, ASYNCCOMMON, ASYNCATOMIC, SYNC &#125;;// PushModeenum class InputMode &#123; NONE, BLOCKING, DROPFRONT, DROPCURRENT &#125;;\n\n对enum class Model的解释：\n\nASYNCCOMMON：Flow节点是异步节点，输入槽是一个队列，输出槽也是一个队列。同时，Flow节点启动时，会创建一个线程，线程不断执行：\n从输入槽当中获取图像数据。\n将图像数据仍给回调函数（此回调函数正是用户自定义的业务回调函数）。\n业务回调函数在处理完数据后，会将图像数据放到对应的输出槽。\n将输出槽当中的图像数据放到下一级子节点的输入槽当中。\n\n\nASYNCATOMIC：大部分特性同ASYNCCOMMON，唯一的区别是输入槽和输出槽使用的不是队列缓存数据，而是使用的Buffer对象指针，方便理解的话，读者可以理解为长度固定为1的队列。同ASYNCCOMMON，会在一个单独的线程当中，执行1~4步逻辑。\nSYNC：代表Flow节点是同步节点，输入&#x2F;输出槽是一个Buffer对象指针，并且Flow节点启动时不会创建线程。1~4步逻辑会和父节点使用同一个线程，在Flow节点的父节点处理线程当中执行。\n\n对于 enum class InputMode， 该枚举类仅在flow 为Model::ASYNCCOMMON模式下有意义 ，它代表在节点输入槽队列（如果队列长度有限制的话）满时，Flow::SendInput函数的溢出策略：\n\nBLOCKING：阻塞，知道队列不满。\nDROPFRONT：丢弃队列前部数据。\nDROPCURRENT：丢弃当前的数据。\n\n前面反复提到上面**输入槽&#x2F;输出槽，这两个名词分别对应 Flow::Input、Flow::FlowMap**两个内嵌类。而在Flow输入输出可能有多个槽，所以在Flow当中有两个槽数组：Flow::v_input、Flow::downflowmap，结合他们的特性以及命名，所以本文我就大胆称其为XX槽。具体输入&#x2F;输出槽的实现比较简单，这里就不贴代码了，对实现细节有把控读者，可以自行阅读源码。\n这里使用一张图来描绘了Flow核心部件以及流程：\n\n比较重要的核心代码：\nvoid FlowCoroutine::RunOnce() &#123;  bool ret = true;  // 从每一个输入槽当中，获取一帧图像数据  (this-&gt;*fetch_input_func)(in_vector);  if (flow-&gt;GetRunTimesRemaining()) &#123;    is_processing = true;    // 执行用户自定义的业务逻辑回调    ret = (*th_run)(flow, in_vector);    is_processing = false;  &#125;  // 多个输出槽  for (int idx : out_slots) &#123;    auto &amp;fm = flow-&gt;downflowmap[idx];    std::list&lt;Flow::FlowInputMap&gt; flows;    fm.list_mtx.read_lock();    flows = fm.flows; // 输出槽可能绑定了多个Sub Flow    fm.list_mtx.unlock();    // 调用Sub Flow的SendInput函数，将输出槽的数据传递给Sub Flow的输入槽。    (this-&gt;*send_down_func)(fm, in_vector, flows, ret);  &#125;  for (auto &amp;buffer : in_vector)    buffer.reset();  pthread_yield();&#125;\n\n在上面的图解当中，没能表现输出槽的内部结构，下面对输出槽的内部结构进行补充：\n\n从输出槽的结构我们可以了解到，输出槽是维护一个Flow链表的，这里就可以回答开头提到的第四个问题：Flow上下级是如何连接的？\n正是由输出槽来维护一个子节点链表，从而使Flow在处理完图像数据后，将处理完的图像数据分发给相应的每一个子节点。在mediaserver的.conf配置文件当中flow_index配置项所描述的输入槽索引&#x2F;输出槽索引也正是这里输入输出槽数组的索引下标。\n在自定义一个Flow节点时，一个Flow节点可能会定义多个输入槽，这取决于特定的业务场景，比如MuxerFlow在打包成.mp4格式的视频可能同时需要视频数据和音频数据、或者双光谱图像的同步节点等。但是一个Flow节点的输入槽只能和一个上级绑定！除非你在业务回调函数当中有办法区分输入的Buffer是来自哪个上级！\n一个FLow节点也可能会定义多个输出槽，这取决于特定的业务场景，比如VideoEncoderFlow节点在编码完一张图像后，可能同时需要输出编码好的图像数据和extra data。与输入槽不同，一个FLow节点的输出槽是可以同时绑定多个Sub Flow（子节点）节点的！\n一句话总结Flow的实现：不严谨的来说，每个Flow节点既是消费者又是生产者，并且FLow的Coroutine实际上是一种递归的逻辑。\n由于代码需要兼顾同步和异步模式，所以里面的类包括：FlowCoroutine、Input、FlowMap在构造时会设置大量的回调。至此，Flow的原理已经分析清楚，现在，你可以试着去精读一下external&#x2F;rkmedia&#x2F;src&#x2F;flow.cc文件当中的源代码，重点可以看一下：InstallSlotMap、SetOutput、SendInput、AddDownFlow函数，相信阅读起来会轻松很多。\n\n本章结束\n","tags":["音视频","Rkmedia","流媒体服务器框架"]},{"title":"STM32快速入门（ADC数模转换）","url":"/2024/06/09/stm32/ADC/","content":"前言ADC数模转换存在的意义就是将一些温度传感器、各自数据传感器产生的模拟信号转换成方便识别和计算的数字信号。\n导航图24 通用定时器框图：\n\n图片截取自STM32 F1XX中文参考手册。还是以框图为中心，来叙述我对ADC的理解。\nACD实现细节\n\n核心原理所谓ADC转换目的是为了将连续变化的模拟量转变成数字，方便程序的计算。这里的模拟量不单单指代那些以正弦规律变化的波形，只要是连续变化的波形，我们都称它为模拟量。简单来说，数模转化器就是按一定分辨率对连续变化的模拟信号进行切分，每一段都会给他进行数字编码，当然分辨越大，模拟信号被切分的越细，精度也会越精确。图片引用自知乎，如有侵权，可联系我将其删除，如图：\n\nADC转换器的实现是：首先对输入的模拟信号进行采样，因为数模转换的精度是确定的，所以会以精度来进行一个二分，每次取命中的精度范围的中间数值，然后将数值通过DA转换，转换成模拟量，然后和采样的模拟量比较，判断大小，再进行一次二分，最终确定采样的模拟量对应的数字编码是多少。这里帖一张简单8位的ADC内部构造的原理图，图片引用自江协科技，如有侵权，联系我将其删除，如图2。\n\n图24的中间部分回到图24，框图左侧ADCx_INx就是各个GPIO端口也称为通道，模拟信号可以从这里输入，旁边的GPIO端口矩形框，内部就是一个硬件开关，STTM32中的ACD一次可以处理多个ADC通道的转换，通道的转换就靠硬件开关来选择。我们这里主要讲规则通道的转换。注入通道原理其实是一样的。规则通道有16路通道，而输出寄存器只有一个，所以在完成一次通道的转换，我们就应该快速讲输出寄存器的值读取走，不然就会覆盖，一般会配合DMA使用。中间的模拟至数字转换器的内部原理就是图2所示。ADCCLK就是给ADC转换器的驱动时钟，和图2的CLOCK引脚对应。\n图24的上半部分图2上部分有连接到输出寄存器（包括1个规则通道寄存器、4个注入通道寄存器）的各种标志位，这些标志位都有相应的寄存器，并且也能触发中断。模拟看门狗的作用在图中也描述的非常明了，就是给定一个检测范围，在范围中就会触发标志位或中断。\n图24的下半部分下半部分就是描绘数模转换触发的一些方式，这里是支持硬件触发的软件触发。硬件触发包括主模式下定时器的TRGO输出、以及定时器的输出通道、外部中断的触发等。各种触发方式和图2的START引脚对应。\n实现ADC转换的细节记录1. 首先要区分：间断模式和扫描模式、单次转换和连续转换。\n所谓间断模式，就是一次触发只转换部分通道。所谓扫描，就是一次触发将规则组（注入组）的所有待转换的通道都转换完。\n所谓单次转换，就是将规则组转换完了，就停止转换。所谓连续转换，就是规则组（注入组）转换完了，就自动从头开始新一轮的转换。\n间断模式和扫描模式通过配置ADC_CR1.SCAN[8]可以开启或者关闭扫描模式，规则组和注入组共用这一位。通过配置ADC_CR1.JDISCEN[12]、ADC_CR1.DISCEN[11]分别可以配置注入组或者规则组去启用或禁用间断模式。通过配置ADC_CR1.DISCNUM[15:13]可以配置间断模式下规则组一次触发事件转换的通道数目，这里只强调规则组！中文手册并没提到注入组，目前不确认规则组是否也受该位的影响！扫描模式下，只有最后一个条目转换完毕才会置位EOC。\n具体使用连续转换还是单次转换，由ADC_CR2.CONT[1]控制。\n2. 关于看门狗的细节\n规则组和注入组可以独立的开启模拟看门狗，分别使用ADC_CR1.AWDEN[23]、ADC_CR1.JAWDEN[22]，通过ADC_CR1.AWDSGL[9]位可以实现扫描模式下让看门狗只监控一个特定通道，监视的通道号由ADC_CR1.AWDCH[4:0]位给出。\n3. 模式选择和触发方式\n本文只讲解独立模式的配置，由ADC_CR1.DUALMOD[19:16]可以配置是独立模式还是双模式。规则组和注入组都可以独立配置触发方式。规则组通过ADC_CR2.EXTSEL[19:17]选择触发源。典型值是[111：软件触发SWSTART]，还需要使用ADC_CR2.EXTTRIG[20]使能外部触发源。ADC_CR2.SWSTART[22]置位可激活软件触发，使规则组开始转换。\n4. 规则组和通道的关系\nSTM32F103系列规则组可以有16个条目（entry）（标号从1开始，范围[1, 16]），通道一共有18个（标号从0开始，范围[0, 17]）。\nADC1的模拟输入通道16和通道17在芯片内部分别连到了温度传感器和VREFINT。\nADC2的模拟输入通道16和通道17在芯片内部连到了VSS。\nADC3模拟输入通道9、14、15、16、17与Vss相连。\n每个通道可以单独配置其采样时间。ADC_SMPRx（x&#x3D;1、2）。通过配置ADC_SQRx（x&#x3D;1、2、3）可以配置规则组每个条目指向哪个通道。其中ADC_SQR1.L[23:20]可设置规则组中有效条目的长度（也即通道数目）。\n盗取江协科技的图片如下：\n\nADC转换的库函数实现硬件接线图如下：\n\n\nGPIO对应的ADC如下：\n\n由表可知，我们需要配置的是ADC3\nGPIO配置如下：\n\n核心代码如下：\nvoid LunarADCInit(void) &#123;\tGPIO_InitTypeDef GPIOF8_Cfg;\tADC_InitTypeDef ADC3_Cfg;\t// 先配置ADCCLK预分频器\t\t12M HZ\tRCC_ADCCLKConfig(RCC_PCLK2_Div6);\t// 打开ADC1时钟\tRCC_APB2PeriphClockCmd(RCC_APB2Periph_ADC3, ENABLE);\tRCC_APB2PeriphClockCmd(RCC_APB2Periph_GPIOF, ENABLE);\tGPIOF8_Cfg.GPIO_Mode = GPIO_Mode_AIN;\tGPIOF8_Cfg.GPIO_Pin = GPIO_Pin_8;\tGPIOF8_Cfg.GPIO_Speed = GPIO_Speed_50MHz;\tGPIO_Init(GPIOF, &amp;GPIOF8_Cfg);\tADC3_Cfg.ADC_ContinuousConvMode = ENABLE;\t\t\t\t\t// 连续转换使能\tADC3_Cfg.ADC_DataAlign = ADC_DataAlign_Left;\tADC3_Cfg.ADC_ExternalTrigConv = ADC_ExternalTrigConv_None;\t// 软件触发\tADC3_Cfg.ADC_Mode = ADC_Mode_Independent;\t\t\t\t\t// 独立模式\tADC3_Cfg.ADC_NbrOfChannel = 1;\t\t\t\t\t\t\t\t// 只转换一个序列\tADC3_Cfg.ADC_ScanConvMode = ENABLE;\t\t\t\t\t\t\t// 使用扫描模式\tADC_Init(ADC3, &amp;ADC3_Cfg);\tADC_RegularChannelConfig(ADC3, ADC_Channel_6, 1, ADC_SampleTime_7Cycles5);\t// 配置规则序列寄存器以及通道采样时间\tADC_Cmd(ADC3, ENABLE);\tADC_ResetCalibration(ADC3);\twhile (ADC_GetResetCalibrationStatus(ADC3) == SET);\t// 硬件置0\tADC_StartCalibration(ADC3);\twhile (ADC_GetCalibrationStatus(ADC3) == SET);\t// 硬件置0\tADC_SoftwareStartConvCmd(ADC3, ENABLE);\twhile (ADC_GetFlagStatus(ADC3, ADC_FLAG_STRT) == RESET);&#125;  int main() &#123;\t// 初始化usart\tLunarInitUSART1();\tLunarADCInit();\tSYSTick_Init();\tLunarNVICInit();\tprintf(&quot;stm32 启动\\n&quot;);\tint t = 3;\twhile(1) &#123;\t\tprintf(&quot;light:%d\\r\\n&quot;, ADC_GetConversionValue(ADC3));\t\twhile (t &gt; 0)&#123;\t\t\tt--;\t\t\tDelay_Ms(1000);\t\t&#125;\t\tt= 3;\t&#125;\treturn 0; &#125;\n\n\n实验现象就是手机背光照射光敏电阻时，ADC转换数值变小。\n\n\n本章完结\n","tags":["STM32"]},{"title":"STM32快速入门（总线协议之I2C一主多从（软件实现 & 硬件实现））","url":"/2024/05/12/stm32/I2C/","content":"前言支持一对多（一主多从）、多对多传输（多主多从），只支持半双工，一般有两根数据线：SCL（Serial Clock Line）、SDA(Serial Data Line)\nI2C有两种实现方式：一种是GPIO软件模拟，另一种是直接使用I2C硬件。\nI2C软件实现软件实现I2C的好处就是我们可以任意的选取GPIO口作为SDA、SCL线。\n硬件电路的设计硬件连接图如下：\n\n\n\nSDA是数据线，SCL是时钟线。时钟线完全由主机控制。而从机只有对SDA的（短暂）控制权。通常使用I2C通通信，SCL和SDA线上都会接一个上拉电阻，要么是在外设的内部，要么是在外面可以直接通过外设的硬件接线图看到。接下来讨论外接电阻存在的意义。\n如下：\n\n在配置成推挽输出模式下，可以直接读输入寄存器从而获取引脚状态。但推挽输出配置外接电阻是没有任何意义的，因为输出0或输出1完全由IO决定。可以说只要IO口是推挽输出的状态那么SDA线的控制权就不可能移交给从机。想要放弃对SDA的控制就必须将IO口配置成输出的模式。甚至在极端情况下，在主输出1从输出0会造成短路。\n\n在配置成开漏输出模式下，也可以直接读输入寄存器从而获取引脚状态，通过置输出寄存器为1这样引脚处于高阻态（引脚悬空）主机可以方便的释放SDA的控制权。不需要像推挽输出那样配置成输入模式。通过读取输入寄存器，可以方便的接收数据。主机、外设引脚都处于开漏模式，需要输出0就向输出寄存器写0，将引脚拉低；需要输出1就向输出寄存器写1将引脚断开，让上拉电阻提供一个弱上拉。并且也不存在短路的风险，即使在主输出1从输出0的情况下，因为高电平是上拉电阻提供，中间怎么都会存在电阻。\n\n\n所以一因为推挽输出输出的是强高低电平，在同一时刻，主机上拉输出1，某一个从机下拉输出0，这样就造成了短路！！！所以，为了防止这种情况，所有机器的SCL、SDA都被配置成开漏输出，只能输出强低电平0，同时外接一个弱的上拉电阻，以输出弱的高电平，杜绝了短路去危险情况。同时外接上拉电阻 + 开漏输出不需要频繁切换输入输出模式。\n所以我们实现的软件I2C也会将SCL、SDA引脚置为开漏输出的模式。\nI2C的起始位 &amp; 停止位起始位和停止位的波形如下：\n\n简述一下，起始位就是在SCL高电平期间将SDA拉低，停止位就是在SCL为高电平期间将SDA拉高。\n这样设计的目的和I2C数据位的传输有关，I2C中，一个数据位的传输遵循：在SCL低电平期间，写数据的一方向SDA写1（释放SDA）或者写0（拉低SDA），同时读的一方不允许去读数据（SDA）。在SCL高电平期间，读数据的一方在SDA上进行读数据，同时写的一方不想允许更改数据（SDA）。也就是所谓的‘低电平放，高电平取’原则。\n而起始位和停止位恰好违背上面的原则，让通信双方能够很好的辨识出一个通信的周期。\n此外，系统空闲时，两根总线都是被弱上拉拉高的高电平状态。至于为什么是弱上拉，我们后面讨论。\n首部紧跟起始位的是：设备地址 + 寄存器地址\n对于7位设备地址：\n第一个字节必须是：\n| 设备地址（7位） | 读写位（1位）|# 读写位为0代表写，为1代表读。\n\n对于10位设备地址：\n前两个字节必须是：\n| 1111 0 | 设备地址（2位） | 读写位（1位） | | 设备地址（8位） |# 读写位为0代表写，为1代表读。\n\n本博客主要讨论7位地址的情况。\n在写设备地址之后的一个字节是要写的寄存器地址，寄存器地址就是要写的设备的寄存器的物理地址，一般是8位地址。I2C中只有写才有寻址的能力（第二个发送的字节为寄存器地址），读只能顺序去读（不能直接发送寄存器地址）。要想实现随机读，就需要在发送一个‘幽灵’写设备地址后重新发送一个：起始位 + 读设备地址，从而实现了随机读。简单讲随机读就是一种复合模式。也即：随机读 &#x3D; 随机写 + 直接读。\n在帧头传输完毕后，就是正式读写数据的传输。\nI2C一个Byte的发送写遵循低放高取原则，因为是写操作，在传输数据时从机不具备操作SDA的权利，从机的SDA引脚保持高组态，SDA的高低电平由主机决定，主机通过操作SCL控制数据的传输进度。但是当一个Byte位传输完毕，主机会短暂释放SDA等待从机回复一个bit位的ACK，此时从机短暂拥有SDA的控制权。多次写入的数按最开始的寄存器地址依次排列。主机发送停止位代表传输周期结束。\n\n主机发送起始位\n\n主机发送写设备地址\n\n主机获取从机的ack\n\n主机发送寄存器地址\n\n主机获取从机的ack\n\n主机发送待写数据\n\n主机获取从机的ack（重复6~7步可以重复写\n\n主机发送停止位\n\n\n注意1： I2C是高位先行。\n注意2： 连续的写会纯在单页回滚问题，eeprom默认1页是8Byte，写到8的整数倍会导致指针混滚！这点需要注意。\nI2C一个Byte的读取读取同样遵循低放高取原则，因为是读操作，在传输数据时主机不具备操作SDA的权利，主机的SDA引脚保持高组态，SDA的高低电平由从机决定，主机通过操作SCL控制数据的传输进度。但是当一个Byte位接收完毕，从机会短暂释放SDA等待主机回复一个bit位的ACK，此时主机短暂拥有SDA的控制权，当主机回复0（ACK），从机会继续发送数据；当主机回复1（NACK），从机会停止发送数据。一般情况下主机回复NACK后会接着回复一个停止位。\n直接（顺序）读：\n\n主机发送起始位\n\n主机发送读设备地址\n\n主机获取从机的ack\n\n主机读取从机发来的数据\n\n主机发送NACK，结束传输（ACK可以回到4，继续读\n\n主机送停止位\n\n\n顺序读地址只能从从设备的0开始，每次读，从设备的地址指针会自增1。不存在混滚问题。\n随机读：\n\n主机发送起始位\n\n主机发送写设备地址\n\n主机获取从机的ack\n\n主机发送寄存器地址\n\n主机获取从机的ack\n\n主机发送起始位（\t Restart\n\n主机发送读设备地址\n\n主机获取从机的ack\n\n主机读取从机发来的数据\n\n主机发送NACK，结束传输（ACK可以回到9，继续读\n\n主机送停止位\n\n\n总结一下读写特点：写可以寻址（寻址从设备的寄存器），读只能顺序读，随机读 &#x3D; 随机写 + 顺序读。\n软件实现核心代码需要另外注意的是，如果要对EEPROM外设进行读写，在写后不能直接进行读，需要延时至少5ms，给EEPROM一点操作时间，否则直接读出来的数据我测的是一直为0xff！！！\n代码如下：\n/*    i2c.h*/#define SCL_PORT GPIOB#define SCL_PIN GPIO_Pin_6#define SDA_PORT GPIOB#define SDA_PIN GPIO_Pin_7// .../*    i2c.c*//** * @description: I2C端口GPIO初始化 * @return &#123;*&#125; */void Lunar_I2CInit(void) &#123;\tGPIO_InitTypeDef GPIOB_Cfg;\t// PA\tRCC_APB2PeriphClockCmd(RCC_APB2Periph_GPIOB, ENABLE);\t// 两个引脚都配置为开漏输出\tGPIOB_Cfg.GPIO_Mode = GPIO_Mode_Out_OD;\tGPIOB_Cfg.GPIO_Pin = GPIO_Pin_6 | GPIO_Pin_7;\tGPIOB_Cfg.GPIO_Speed = GPIO_Speed_50MHz;\tGPIO_Init(GPIOB, &amp;GPIOB_Cfg);    // 最开始都为释放状态的高电平\tLunar_SetSCL(Bit_SET);\tLunar_SetSDA(Bit_SET);&#125;/** * @description: 控制SCL引脚的电平状态，完全由主机控制 * @param &#123;uint8_t&#125; BitVal * @return &#123;*&#125; */void Lunar_SetSCL(uint8_t BitVal) &#123;    GPIO_WriteBit(SCL_PORT, SCL_PIN, (BitAction)BitVal);\t// 来一点点延时\tDelay_Us(10);&#125;/** * @description: 控制SDA引脚的状态，可由主机控制，也可由从机控制 * @param &#123;uint8_t&#125; BitVal * @return &#123;*&#125; */void Lunar_SetSDA(uint8_t BitVal) &#123;    GPIO_WriteBit(SDA_PORT, SDA_PIN, (BitAction)BitVal);\t// 来一点点延时\tDelay_Us(10);&#125;/** * @description: 获取SDA电平状态 * @return &#123;uint8_t&#125; */uint8_t Lunar_GetSDA(void) &#123;    uint8_t rt = GPIO_ReadInputDataBit(SDA_PORT, SDA_PIN);\t// 来一点点延时\tDelay_Us(10);    return rt;&#125;/*************************************************************************************************//** * @description: 发送一个起始位 * @return &#123;void&#125; */void Lunar_Start(void) &#123;\t// 先将SCL、SDA拉高\tLunar_SetSCL(1);\tLunar_SetSDA(1);\tif (Lunar_GetSDA() == 0) &#123;\t\tprintf(&quot;error slave hold the SDA!!!\\r\\n&quot;);\t&#125;\t// 再将SDA拉低\tLunar_SetSDA(0);\t// 再将SCL拉低、准备发送数据\tLunar_SetSCL(0);&#125;/** * @description: 发送一个停止位 * @return &#123;void&#125; */void Lunar_Stop(void) &#123;\t// 确保SDA为0\tLunar_SetSDA(0);\t// 将SCL拉高\tLunar_SetSCL(1);\t// 将SDA拉高\tLunar_SetSDA(1);\t\tif (Lunar_GetSDA() == 0) &#123;\t\tprintf(&quot;error slave hold the SDA!!!\\r\\n&quot;);\t&#125;\t// Lunar_SetSCL(0);&#125;/** * @description: 写状态下等待从设备ACK的回复 * @return &#123;uint8_t&#125;：读到ACK还是NACK */uint8_t Lunar_ReadACK(void) &#123;\tuint8_t rt = 0;\t// 释放SDA\tLunar_SetSDA(1);\t// 拉高SCL\tLunar_SetSCL(1);\t// 读SDA\trt = Lunar_GetSDA();\tLunar_SetSCL(0);\treturn rt;&#125;/** * @description: 读状态下向从设备回复一个ACK * @return &#123;*&#125; */void Lunar_WriteACK(void) &#123;\t// SDA置0\tLunar_SetSDA(0);\t// SCL置1\tLunar_SetSCL(1);\t// SCL置0\tLunar_SetSCL(0);\t// 释放SDA拥有权\tLunar_SetSDA(1);&#125;/** * @description: 读状态下向从设备回复一个NACK * @return &#123;*&#125; */void Lunar_WriteNACK(void) &#123;\t// SDA置1\tLunar_SetSDA(1);\t// SCL置1\tLunar_SetSCL(1);\t// SCL置0\tLunar_SetSCL(0);\t// 释放SDA拥有权\tLunar_SetSDA(1); // ???&#125;/** * @description: 读一个Byte * @return &#123;uint8_t&#125;：返回读到的Byte */uint8_t Lunar_ReadByte(void) &#123;\tuint8_t rt = 0;\t// 此出主机不应该拥有SDA的控制权！\tLunar_SetSDA(1);\tfor (int i = 0; i &lt; 8; i++) &#123;\t\t// 先拉高SCL\t\tLunar_SetSCL(1);\t\t// 读SDA\t\trt |= Lunar_GetSDA() &lt;&lt; (7 - i);\t\tprintf(&quot;%d, &quot;, rt);\t\t// 拉低SCL\t\tLunar_SetSCL(0);\t&#125;\t\tprintf(&quot;\\r\\n&quot;);\treturn rt;&#125;/** * @description: 写一个Byte * @param &#123;uint8_t&#125; data：要写入的数据 * @return &#123;*&#125; */void Lunar_WriteByte(uint8_t data) &#123;\t// 此处主机应该拥有SDA的控制权\tfor (int i = 0; i &lt; 8; i++) &#123;\t\t// 写一个bit\t\tLunar_SetSDA((data &gt;&gt; (7 - i)) &amp; 1);\t\t// 拉高SCL\t\tLunar_SetSCL(1);\t\t// 拉低SCL\t\tLunar_SetSCL(0);\t&#125;&#125;/** * @description: 获取指定设备指定寄存器地址的值 * @param &#123;uint8_t&#125; device_addr：设备地址是7位 * @param &#123;uint8_t&#125; registry_addr * @return &#123;uint8_t&#125; */uint8_t Lunar_GetRegistryByte(uint8_t device_addr, uint8_t registry_addr) &#123;\tuint8_t rt = 0;\tuint8_t tag = 0;\t// 发送起始位\tLunar_Start();\t// 发送设备地址\tLunar_WriteByte((device_addr &lt;&lt; 1) | 0);\t// 写设备地址\t// 获取从机的ack\ttag = tag || Lunar_ReadACK();\t// 发送寄存器地址\tLunar_WriteByte(registry_addr);\t// 获取从机的ack\ttag = tag || Lunar_ReadACK();\t// Restart\t// 发送起始位\tLunar_Start();\t// 发送设备地址\tLunar_WriteByte((device_addr &lt;&lt; 1) | 1);\t// 读设备地址\t// 获取从机的ack\ttag = tag || Lunar_ReadACK();\t// 读取从机发来的数据\trt = Lunar_ReadByte();\t// 发送NACK，结束传输\tLunar_WriteNACK();\t// 发送停止位\tLunar_Stop();\tif (tag) &#123;\t\tprintf(&quot;in Lunar_GetRegistryByte Lunar_ReadACK faild!\\r\\n&quot;);\t&#125;\treturn rt;&#125;/** * @description: 设置指定设备指定寄存器地址的值 * @param &#123;uint8_t&#125; device_addr * @param &#123;uint8_t&#125; registry_addr * @param &#123;uint8_t&#125; data * @return &#123;*&#125; */void Lunar_SetRegistryByte(uint8_t device_addr, uint8_t registry_addr, uint8_t data) &#123;\tuint8_t tag = 0;\t// 发送起始位\tLunar_Start();\t// 发送设备地址\tLunar_WriteByte((device_addr &lt;&lt; 1) | 0);\t// 写设备地址\t// 获取从机的ack\ttag = tag || Lunar_ReadACK();\t// 发送寄存器地址\tLunar_WriteByte(registry_addr);\t// 获取从机的ack\ttag = tag || Lunar_ReadACK();\t// 发送待写数据\tLunar_WriteByte(data);\t// 获取从机的ack\ttag = tag || Lunar_ReadACK();\t// 发送停止位\tLunar_Stop();\tif (tag) &#123;\t\tprintf(&quot;in Lunar_SetRegistryByte Lunar_ReadACK faild!\\r\\n&quot;);\t&#125;&#125;\n\nI2C硬件实现导航图242 I2C框图：\n\n实现细节从图242可以看到，和USART类似，I2C也配备了一组移位寄存器 + 数据寄存器的组合，只不过I2C只有一根数据线，所以只有一组寄存器组合。\n对于于比较器和自身地址寄存器，I2C硬件可以在主从两种模式下工作，当作为从模式时，设备就会拥有一个设备地址，自身地址寄存器就是存储该从设备地址的地方，比较器就是完成设备地址匹配的功能。此外，I2C可以通过帧错误校验计算模块完成CRC校验的功能。\n然后就是SCL时钟线的时钟控制模块，其作用就是控制SCL线时钟的，具体实现细节我没有去深究，但它的作用肯定是根据I2C规定的协议去控制SCL引脚上的时钟。\n最后，左下部分就是一些可以让用户控制的寄存器，这些寄存器的作用都可以在中文手册中查阅。\n这里贴一张来自江协科技的I2C简化框图：\n\n使用库函数实现硬件I2C的套路写流程：\n\n发送起始位。\n\n等待EV5事件，也就是等待BUSY(SCL被拉低) &amp; MSL（处于主模式，在start被发出硬件自动置位） &amp; SB（start被发出） 被置位。\n\n发送写设备地址。\n\n等待EV6（发送）事件，也就是BUSY（SCL被拉低）&amp; MSL（处于主模式，在start被发出硬件自动置位）&amp;&amp; ADDR（设备地址已经发送，并且收到从机的ACK）&amp; TXE（发送寄存器为空） &amp; TRA（发送模式，依据发送设备地址最后一位设置） 被置位。\n\n这里有一个理论上的EV8_1事件，但是库函数没有对应的宏，大体意思就是发送寄存器和发送移位寄存器同时为空。\n\n发送寄存器地址。\n\n等待EV8事件，也就是TRA（发送模式，依据发送设备地址最后一位设置）&amp; BUSY（SCL被拉低） &amp; MSL（处于主模式，在start被发出硬件自动置位） &amp; TXE（发送寄存器为空，收到从机的ACK回复才会置位）被置位。\n\n发送待写数据。\n\n等待EV8_2事件，也就是在EV8事件基础上多了一个BTF标志位，代表发送寄存器和发送移位寄存器都空了，数据线上也没有需要传输的数据了，该发的数据发干净了。\n\n发送停止位，请求停止传输。\n\n\n重复执行8~9可实现多次写。\n这里可以配合中文手册的图理解写的套路：\n\n读流程：\n\n发送起始位。\n\n等待EV5事件，也就是等待BUSY(SCL被拉低) &amp; MSL（处于主模式，在start被发出硬件自动置位） &amp; SB（start被发出） 被置位。\n\n发送写设备地址。\n\n等待EV6（发送）事件，也就是BUSY（SCL被拉低）&amp; MSL（处于主模式，在start被发出硬件自动置位）&amp; ADDR（设备地址已经发送，并且收到从机的ACK）&amp; TXE（发送寄存器为空） &amp; TRA（发送模式，依据发送设备地址最后一位设置） 被置位。\n\n这里有一个理论上的EV8_1事件，但是库函数没有对应的宏，大体意思就是发送寄存器和发送移位寄存器同时为空。起提示作用。\n\n发送寄存器地址。\n\n等待EV8_2事件，也就是TRA（发送模式，依据发送设备地址最后一位设置）&amp; BUSY（SCL被拉低） &amp; MSL（处于主模式，在start被发出硬件自动置位） &amp; TXE（发送寄存器为空，收到从机的ACK回复才会置位）&amp; BTF（数据寄存器和移位寄存器都为空）被置位。\n\n发送起始位。\n\n等待EV5事件，也就是等待BUSY(SCL被拉低) &amp; MSL（处于主模式，在start被发出硬件自动置位） &amp; SB（start被发出） 被置位。\n\n发送读设备地址。\n\n等待EV6（接收）事件，也就是BUSY（SCL被拉低）&amp; MSL（处于主模式，在start被发出硬件自动置位）&amp; ADDR（设备地址已经发送，并且收到从机的ACK） 被置位。\n\nEV6_1 &#x2F; EV7_1 需要提前设置ack位和停止产生位。如果想继续读数据，就不执行该步，循环执行13~14即可实现多次读。\n\n等待EV7事件，也就是BUSY（SCL被拉低）&amp; MSL（处于主模式，在start被发出硬件自动置位）&amp; RXNE（接收数据寄存器非空）被置位。\n\n接收从机发来的数据。\n\n\n这里可以配合中文手册的图理解读的套路，手册提供的是直接顺序读的时序，但是前面软件部分说过，需要实现随机读，可以使用‘幽灵’写 + 顺序读的方式实现，上述步骤也正是这样做的。\n这里可以配合中文手册的图理解读的套路：\n\n这里提供I2C_CR1寄存器一些相关位的描述，方便读者分析：\n\n这里提供I2C_SR1寄存器一些相关位的描述，方便读者分析：\n\n\n\n这里提供I2C_SR2寄存器一些相关位的描述，方便读者分析：\n\n库函数实现代码在实现硬件I2C时，需要将SCL、SDA所在GPIO的引脚都配置为开漏复用输出的模式，复用是为了将IO口控制权交给片上外设，开漏是为了遵循协议。开漏的具体原因在软件部分也进行过深入探讨，这里就不过多赘述。\n\n\n\n在写硬件实现I2C的代码的时候，出现了一个非常奇怪的BUG，在第一次执行Lunar_GetRegistryByte时，程序一直卡在Lunar_GetRegistryByte函数的第一个while循环中，查了半天发现是START位发不出去，因为BUSY一直是被置位（总线被拉低，一直处于通信状态）的状态。然后检测Lunar_I2CInit函数，发现只要一开启I2C1时钟，BUSY就是被置位的状态。经过百度，在I2C被初始化前，使用I2C_SoftwareResetCmd库函数进行一次复位才得以解决。\n最后需要注意的是使用I2C_Send7bitAddress函数发送设备地址需要注意提前将地址左移一位，因为I2C_Send7bitAddress函数的实现它是直接在你提供的设备地址末尾置读写位的。\n硬件实现核心代码如下：\n/** * @description: I2C端口GPIO初始化 * @return &#123;*&#125; */void Lunar_I2CInit(void) &#123;\tGPIO_InitTypeDef GPIOB_Cfg;\tI2C_InitTypeDef I2C1_Cfg;\t// PA\tRCC_APB2PeriphClockCmd(RCC_APB2Periph_GPIOB, ENABLE);\t// 两个引脚都配置为复用开漏输出\tGPIOB_Cfg.GPIO_Mode = GPIO_Mode_AF_OD;\tGPIOB_Cfg.GPIO_Pin = GPIO_Pin_6 | GPIO_Pin_7;\tGPIOB_Cfg.GPIO_Speed = GPIO_Speed_50MHz;\tGPIO_Init(GPIOB, &amp;GPIOB_Cfg);\t// 开启I2C1时钟\tRCC_APB1PeriphClockCmd(RCC_APB1Periph_I2C1, ENABLE);\tI2C1_Cfg.I2C_Ack = I2C_Ack_Enable;\t\t\t\t\t\t\t\t\t// 默认回复ACK\tI2C1_Cfg.I2C_AcknowledgedAddress = I2C_AcknowledgedAddress_7bit;\t// 7bit设备地址\tI2C1_Cfg.I2C_ClockSpeed = 50000;\t\t\t\t\t\t\t\t\t// 50KHZ\tI2C1_Cfg.I2C_DutyCycle = I2C_DutyCycle_2;\t\t\t\t\t\t\t// 该位仅对100KHZ ~ 400KHZ的高速频率有效，我们设置的50KHZ其实不起作用，高频率模式下，从下降沿变换到上升沿有一个过程，所以需要适当增大占空比\tI2C1_Cfg.I2C_Mode = I2C_Mode_I2C;\tI2C1_Cfg.I2C_OwnAddress1 = 0x00;\t\t\t\t\t\t\t\t\t// 随便自定义一个不冲突的主机设备地址\t// 必须使用软件对I2C进行复位，不然，I2C1的BUSY位一直会处于置位的状态！！！\tI2C_SoftwareResetCmd(I2C1, ENABLE);\tI2C_SoftwareResetCmd(I2C1, DISABLE);\tI2C_Init(I2C1, &amp;I2C1_Cfg);\t\tI2C_Cmd(I2C1, ENABLE);\t// 使能\t\t// I2C_GenerateSTOP(I2C1, ENABLE);&#125;/** * @description: 获取指定设备指定寄存器地址的值 * @param &#123;uint8_t&#125; device_addr：设备地址是7位 * @param &#123;uint8_t&#125; registry_addr * @return &#123;uint8_t&#125; */uint8_t Lunar_GetRegistryByte(uint8_t device_addr, uint8_t registry_addr) &#123;\tuint8_t rt = 0;\tdevice_addr = device_addr &lt;&lt; 1;\t// 发送起始位\tI2C_GenerateSTART(I2C1, ENABLE);\t// 等待EV5事件，\t// 也就是等待BUSY(SCL被拉低) &amp; \t// MSL（处于主模式，在start被发出硬件自动置位） &amp; \t// SB（start被发出） 被置位\twhile(I2C_CheckEvent(I2C1, I2C_EVENT_MASTER_MODE_SELECT) != SUCCESS);\t// 发送设备地址\tI2C_Send7bitAddress(I2C1, device_addr, I2C_Direction_Transmitter);\t// 写设备地址\t// 等待EV6（发送）事件，\t// 也就是BUSY（SCL被拉低）&amp; \t// MSL（处于主模式，在start被发出硬件自动置位）&amp;\t// ADDR（设备地址已经发送，并且收到从机的ACK）&amp;\t// TXE（发送寄存器为空） &amp; \t// TRA（发送模式，依据发送设备地址最后一位设置） 被置位。\twhile(I2C_CheckEvent(I2C1, I2C_EVENT_MASTER_TRANSMITTER_MODE_SELECTED) != SUCCESS);\t// 这里有一个理论上的EV8_1事件，但是库函数没有对应的宏，大体意思就是发送寄存器和发送移位寄存器同时为空。起提示作用。\t// 发送寄存器地址\tI2C_SendData(I2C1, registry_addr);\t// 等待EV8_2事件，\t// 也就是TRA（发送模式，依据发送设备地址最后一位设置）&amp; \t// BUSY（SCL被拉低） &amp; \t// MSL（处于主模式，在start被发出硬件自动置位） &amp; \t// TXE（发送寄存器为空，收到从机的ACK回复才会置位）&amp; \t// BTF（数据寄存器和移位寄存器都为空）被置位。\twhile(I2C_CheckEvent(I2C1, I2C_EVENT_MASTER_BYTE_TRANSMITTED) != SUCCESS);\t// Restart\t// 发送起始位\tI2C_GenerateSTART(I2C1, ENABLE);\t// 等待EV5事件，\t// 也就是等待BUSY(SCL被拉低) &amp; \t// MSL（处于主模式，在start被发出硬件自动置位） &amp; \t// SB（start被发出） 被置位\twhile(I2C_CheckEvent(I2C1, I2C_EVENT_MASTER_MODE_SELECT) != SUCCESS);\t// 发送设备地址\tI2C_Send7bitAddress(I2C1, device_addr, I2C_Direction_Receiver);\t// 读设备地址\t// 等待EV6（接收）事件，\t// 也就是BUSY（SCL被拉低）&amp; \t// MSL（处于主模式，在start被发出硬件自动置位）&amp; \t// ADDR（设备地址已经发送，并且收到从机的ACK） 被置位。\twhile(I2C_CheckEvent(I2C1, I2C_EVENT_MASTER_RECEIVER_MODE_SELECTED) != SUCCESS);\t// EV6_1 / EV7_1 需要提前设置ack位和停止产生位。\tI2C_AcknowledgeConfig(I2C1, DISABLE);\tI2C_GenerateSTOP(I2C1, ENABLE);\t// 等待EV7，也就是BUSY（SCL被拉低）&amp; \t// MSL（处于主模式，在start被发出硬件自动置位）&amp; \t// RXNE（接收数据寄存器非空）被置位。\twhile(I2C_CheckEvent(I2C1, I2C_EVENT_MASTER_BYTE_RECEIVED) != SUCCESS);\t// 接收从机发来的数据\trt = I2C_ReceiveData(I2C1);\t// 恢复ACK确认\tI2C_AcknowledgeConfig(I2C1, ENABLE);\treturn rt;&#125;/** * @description: 设置指定设备指定寄存器地址的值 * @param &#123;uint8_t&#125; device_addr * @param &#123;uint8_t&#125; registry_addr * @param &#123;uint8_t&#125; data * @return &#123;*&#125; */void Lunar_SetRegistryByte(uint8_t device_addr, uint8_t registry_addr, uint8_t data) &#123;\tdevice_addr = device_addr &lt;&lt; 1;\t// 发送起始位\tI2C_GenerateSTART(I2C1, ENABLE);\t// 等待EV5事件，\t// 也就是等待BUSY(SCL被拉低) &amp; \t// MSL（处于主模式，在start被发出硬件自动置位） &amp; \t// SB（start被发出） 被置位\twhile(I2C_CheckEvent(I2C1, I2C_EVENT_MASTER_MODE_SELECT) != SUCCESS);\t// 发送设备地址\tI2C_Send7bitAddress(I2C1, device_addr, I2C_Direction_Transmitter);\t// 写设备地址\t// 等待EV6（发送）事件，\t// 也就是BUSY（SCL被拉低）&amp; \t// MSL（处于主模式，在start被发出硬件自动置位）&amp; \t// ADDR（设备地址已经发送，并且收到从机的ACK）&amp; \t// TXE（发送寄存器为空） &amp; \t// TRA（发送模式，依据发送设备地址最后一位设置） 被置位。\twhile(I2C_CheckEvent(I2C1, I2C_EVENT_MASTER_TRANSMITTER_MODE_SELECTED) != SUCCESS);\t// 这里有一个理论上的EV8_1事件，但是库函数没有对应的宏，大体意思就是发送寄存器和发送移位寄存器同时为空。起提示作用\t// 发送寄存器地址\tI2C_SendData(I2C1, registry_addr);\t//  等待EV8事件，\t// 也就是TRA（发送模式，依据发送设备地址最后一位设置）&amp; \t// BUSY（SCL被拉低） &amp; \t// MSL（处于主模式，在start被发出硬件自动置位） &amp; \t// TXE（发送寄存器为空，收到从机的ACK回复才会置位）被置位。\twhile(I2C_CheckEvent(I2C1, I2C_EVENT_MASTER_BYTE_TRANSMITTING) != SUCCESS);\t// 发送待写数据\tI2C_SendData(I2C1, data);\t//  等待EV8_2事件，也就是在EV8事件基础上多了一个BTF标志位，代表发送寄存器和发送移位寄存器都空了，数据线上也没有需要传输的数据了，该发的数据发干净了。\twhile(I2C_CheckEvent(I2C1, I2C_EVENT_MASTER_BYTE_TRANSMITTED) != SUCCESS);\t// 发送停止位，请求停止传输\tI2C_GenerateSTOP(I2C1, ENABLE);&#125;\n\n\n感谢江协科技提供的STM32教学视频。\n感谢为STM32 f100ZET6中硬件I2C的问题提供解决方案的作者，原帖如下：https://blog.csdn.net/jatamatadada/article/details/40860619\n\n本章完结\n","tags":["STM32"]},{"title":"STM32快速入门（定时器之输入捕获）","url":"/2024/05/08/stm32/TimerCapture/","content":"前言本节主要讲解STM32利用通用定时器，在输入引脚出现指定电平跳变时，将CNT的值锁存到CCR寄存器当中，从而计算PWM波形的频率、占空比、脉冲间隔、电平持续时间等。其功能的应用有：波形采样。\n导航图98 通用定时器框图：\n\n图片引自STM32 F1XX系列的中文参考手册。在通用定时器章节的定时器架构图中，本章讲解的定时器输入捕获功能位于左下角的红色矩形中。\n定时器输入捕获的实现细节\n\n参考中文手册，实现细节图123如下：\n\n它内部实现是：根据用户设定的极性，采集输入方波信号上升沿&#x2F;下降沿，将每次上升沿&#x2F;下降沿的CNT寄存器的值抓取到CCR寄存器中，从而可以获取到输入信号的特性。\n参考图123，从左向右介绍控制细节对于一个通用定时器，有四个通道可作为输入（或输出），信号输入进来首先会经过滤波器进行滤波，消除不稳定的干扰信号，用户可以通过配置 TIMx_CCMR1.IC1F[7:4] 选择采样模式，可以以不同频率不同次数进行采样滤波。如下图。\n\n采样可选频率来源有F_CK_INT和F_DTS。其中，F_CK_INT就是定时器的内部时钟（F103默认72M HZ），而F_DTS其实间接取自F_CK_INT的分频。通过配置 TIMx_CR1.CKD[9:8] 可设置F_DTS的分频系数。如下：\n\n经过滤波器滤波后的信号在图123中被标记为TI1F，TI1F会传入中间部分的边沿检测器，边沿检测器会根据输入的TI1F分拣出波形的每个上升沿和下降沿，根据输入信号的每一个上升沿&#x2F;下降沿，向上升沿输出引脚&#x2F;下降沿引脚输出一个小方波，从而给后面的选择器进行选择，图中间部分有上下两个矩形，在中文手册中，所有类似这样的矩形都是选择器， TIMx_CCER.CC1P[1] 正是通过控制选择器来实现极性的选择。经过极性选择后的波形在图123被标记为TI1FP1。图中还有一个被标记为TI1F_ED的输出，TI1FP1和TI1F_ED的区别是前者是经过选择的上升沿或是下降沿的边沿指示信号，而后者是上升沿和下降沿的边沿指示信号，频率上来讲TI1F_ED会更高。注意这里边沿指示信号和源信号的区别，我最开始看这张图的中间部分就非常迷糊。\n接着看右边最大的那个选择器，该选择器就是配置三路的哪一路作为IC1的输入。三路输入分别是：TI1FP1（对应TIMX_CH1）、TI2FP1（对应TIMX_CH2）、TRC（主从模式下，来自主定时器的信号），通过配置 TIMx_CCMR1.CC1S[1:0] 可以控制选择器选择哪一路。同时后面的预分频器可以通过 TIMx_CCMR1.IC1PSC[3:2] 来调节。如下图60：\n\n最后配置使能寄存器 TIMx_CCER.CC1E[0] 就能使能定时器的输入啦！\n精妙设计一细心的读者在看到图98 红色矩形部分时，应该会注意通道TIMX_CH1和通道TIMX_CH2中间部分是存在交叉的，这里放一张特写图。\n\n这TI1FP1和TI1FP2的信号源都是来自TIMX_CH1，图123的描述其实有些瑕疵。TI1FP1和TI1FP2的信号源相同，并且可以分别独立的控制去选择极性。也就是说完整的图123应该是有两路TI1FP的，并且可以单独的控制其极性。 如果只使用一路的捕获，我们一次只能测量信号源的频率；而有了这种交叉的设计，我们就可以实现对一个信号源，同时测量其频率和占空比。图60表述了将ICX映射到哪一路，通过配置 TIMx_CCMR1.CC1S[1:0] 可以选择。\n精妙设计二STM32 F1XX里面定时器的设计特别精妙，利用好定时器的主从模式可以实现硬件全自动化复位操作。比如：我们可以利用TI1FP1的信号实现定时器的自动复位，步骤如下：\n\n配置 TIMx_SMCR.TS[6:4] 为101，这样滤波后的定时器输入1(TI1FP1)作为定时器触发源。 这里的主次好像是两个定时器，但实际上都是一个定时器扮演。\n\n配置 TIMx_SMCR.SMS[2:0] 为100，这样在收到TI1FP1的触发信号就会将定时器复位。从而达到清零的目的。\n\n\n涉及的寄存器如下：\n\n此外，还可以实现定时器级联的效果，比如使用一个定时器作为另一个定时器的预分频。根据中文参考手册配置步骤如下：\n\n除了上面提到的用法，定时器其实还要很多奇妙的用法。具体可以查询中文参考手册。中文参考手册很多东西写的其实非常详细了，就是初学者来说，可能很难耐心去阅读。这点真的要好好锤炼，中文都看不下去，更何况以后还要接触英文的。\n定时器实现输入捕获的步骤综上，可以总结出配置定时器输入部分的套路：\n\n通过 TIMx_CCMR1.IC1F[7:4] 配置滤波器，选择其频率和采样次数。\n\n通过 TIMx_CCER.CC1P[1] 配置要捕获的极性（上升沿还是下降沿）。\n\n通过 TIMx_CCMR1.CC1S[1:0] 可以配置图123中，右边那个最大的选择器，选择三路的哪一路作为IC1的来源。\n\n通过 TIMx_CCMR1.IC1PSC[3:2] 可以配置图123中，右边那个分频器的分频系数。\n\n通过 TIMx_CCER.CC1E[0] 可以使能捕获输入。\n\n\n定时器实现输入捕获的库函数实现本节输入捕获实验会复用定时器输出PWM（输出在PB5口）的呼吸灯实验的代码，经过查表，会将原PB5端口输出的PWM信号使用杜邦线，引到PA0端口并且作为TIM2定时器输入。 IO口需要的配置如下：\n\n\n核心代码如下：\nvoid LunarInitTIM3() &#123;\tGPIO_InitTypeDef GPIOB5_Cfg;\tTIM_TimeBaseInitTypeDef TIM3_Cfg;\tTIM_OCInitTypeDef TIM3_OCCfg;\t// 配置GPIO \tBEGIN\t// 开启复用时钟\tRCC_APB2PeriphClockCmd(RCC_APB2Periph_AFIO, ENABLE);\t// 部分重映射\tGPIO_PinRemapConfig(GPIO_PartialRemap_TIM3, ENABLE);\t// 初始化GPIOB5为推挽复用输出\tRCC_APB2PeriphClockCmd(RCC_APB2Periph_GPIOB, ENABLE); \tGPIOB5_Cfg.GPIO_Mode = GPIO_Mode_AF_PP; \tGPIOB5_Cfg.GPIO_Pin = GPIO_Pin_5; \tGPIOB5_Cfg.GPIO_Speed = GPIO_Speed_2MHz; \tGPIO_Init(GPIOB, &amp;GPIOB5_Cfg);\t// 配置GPIO \tEND    // 定时器时基配置   BEGIN\t// 打开TIM3所需要的时钟 APB1\tRCC_APB1PeriphClockCmd(RCC_APB1Periph_TIM3, ENABLE);\tTIM_TimeBaseStructInit(&amp;TIM3_Cfg);\t// 配置使用内部时钟 72M Hz\tTIM_InternalClockConfig(TIM3);    // 这里配置定时器更新频率是1000HZ\tTIM3_Cfg.TIM_CounterMode = TIM_CounterMode_Up;\tTIM3_Cfg.TIM_Period = 100 - 1;\tTIM3_Cfg.TIM_Prescaler = 720 - 1;\tTIM_TimeBaseInit(TIM3, &amp;TIM3_Cfg);\t// 因为TIM_TimeBaseInit会置TIMx_EGR.UG[0]为1，产生一个更新事件，\t// 去同步影子寄存器的值，而该更新事件又会产生一个多余的中断，所以，\t// 我们需要在开启中断之前，手动清楚更新事件标志位\tTIM_ClearFlag(TIM3, TIM_FLAG_Update);    // 定时器时基配置   END\t// 配置TIM3的PWM输出\tBEGIN\tTIM_OCStructInit(&amp;TIM3_OCCfg);\tTIM3_OCCfg.TIM_OCMode = TIM_OCMode_PWM1;\tTIM3_OCCfg.TIM_OCPolarity = TIM_OCPolarity_High;\tTIM3_OCCfg.TIM_OutputState = TIM_OutputState_Enable;\tTIM3_OCCfg.TIM_Pulse = 80;\tTIM_OC2Init(TIM3, &amp;TIM3_OCCfg);\t// 配置TIM3的PWM输出\tEND\t// 使能arr和ccr寄存器的影子功能\tTIM_OC2PreloadConfig(TIM3, TIM_OCPreload_Enable);\tTIM_ARRPreloadConfig(TIM3, ENABLE);\t// 使能更新中断\t// TIM_ITConfig(TIM3, TIM_IT_Update, ENABLE);\t// 开启定时器\tTIM_Cmd(TIM3, ENABLE);&#125;void LunarInitTIM2() &#123;\tGPIO_InitTypeDef GPIOA0_Cfg;\tTIM_TimeBaseInitTypeDef TIM2_Cfg;\tTIM_ICInitTypeDef TIM2_IC1Cfg, TIM2_IC2Cfg;\t// 配置GPIO \tBEGIN\tRCC_APB2PeriphClockCmd(RCC_APB2Periph_GPIOA, ENABLE);\tGPIOA0_Cfg.GPIO_Mode = GPIO_Mode_IN_FLOATING; \tGPIOA0_Cfg.GPIO_Pin = GPIO_Pin_0; \tGPIOA0_Cfg.GPIO_Speed = GPIO_Speed_2MHz; \tGPIO_Init(GPIOA, &amp;GPIOA0_Cfg);\t// 配置GPIO \tEND    // 定时器时基配置   BEGIN\t// 打开TIM2所需要的时钟 APB1\tRCC_APB1PeriphClockCmd(RCC_APB1Periph_TIM2, ENABLE);\tTIM_TimeBaseStructInit(&amp;TIM2_Cfg);\t// 配置使用内部时钟 72M Hz\tTIM_InternalClockConfig(TIM2);    // 这里配置定时器更新频率是1000HZ\tTIM2_Cfg.TIM_CounterMode = TIM_CounterMode_Up;\t// TIM2_Cfg.TIM_Period = 100 - 1;\tTIM2_Cfg.TIM_Period = 0xffff;\tTIM2_Cfg.TIM_Prescaler = 720 - 1;\tTIM_TimeBaseInit(TIM2, &amp;TIM2_Cfg);\tTIM_ClearFlag(TIM2, TIM_FLAG_Update);\t// 定时器时基配置   END\t// 配置TIM2进行输入捕获\t\tBEGIN\tTIM_ICStructInit(&amp;TIM2_IC1Cfg);\tTIM_ICStructInit(&amp;TIM2_IC2Cfg);\tTIM2_IC1Cfg.TIM_Channel = TIM_Channel_1;\tTIM2_IC1Cfg.TIM_ICFilter = 0x4;\tTIM2_IC1Cfg.TIM_ICPolarity = TIM_ICPolarity_Rising;\tTIM2_IC1Cfg.TIM_ICPrescaler = TIM_ICPSC_DIV1;\tTIM2_IC1Cfg.TIM_ICSelection = TIM_ICSelection_DirectTI;\tTIM_ICInit(TIM2, &amp;TIM2_IC1Cfg);\t// 实现同时捕获上升下降沿。\tTIM2_IC2Cfg.TIM_Channel = TIM_Channel_2;\tTIM2_IC2Cfg.TIM_ICFilter = 0x4;\tTIM2_IC2Cfg.TIM_ICPolarity = TIM_ICPolarity_Falling;\tTIM2_IC2Cfg.TIM_ICPrescaler = TIM_ICPSC_DIV1;\tTIM2_IC2Cfg.TIM_ICSelection = TIM_ICSelection_IndirectTI;\tTIM_ICInit(TIM2, &amp;TIM2_IC2Cfg);\t// 配置TIM2进行输入捕获\t\tEND\t// 利用从模式配置自动重置。\tTIM_SelectInputTrigger(TIM2, TIM_TS_TI1FP1);\tTIM_SelectSlaveMode(TIM2, TIM_SlaveMode_Reset);\t// 使能arr寄存器的影子功能\t// ccr寄存器只读\tTIM_ARRPreloadConfig(TIM2, ENABLE);\t// 使能更新中断\t// TIM_ITConfig(TIM2, TIM_IT_Update, ENABLE);\t// 开启定时器\tTIM_Cmd(TIM2, ENABLE);&#125;int main() &#123;\t// 初始化串口\tLunarInitUSART1();\t// 初始化定时器\tLunarInitTIM3();\tLunarInitTIM2();\tSYSTick_Init();\tint dir = 0, cr = 0;\twhile(1) &#123;\t\tDelay_Ms(100);\t\tprintf(&quot;PWM f = %d &quot;, 100000 / (TIM_GetCapture1(TIM2) + 1));\t\tprintf(&quot;PWM f = %f \\n&quot;, (float)(TIM_GetCapture2(TIM2) + 1) / (TIM_GetCapture1(TIM2) + 1));\t&#125;\treturn 0;&#125;\n\n\n实验结果就是从串口中，我们可以看到PB5输出的PWM波形的频率和占空比值。\n\n\n本章完结\n","tags":["STM32"]},{"title":"STM32快速入门（定时器之输出PWM波形）","url":"/2024/05/12/stm32/TimerPWM/","content":"前言本节主要讲解STM32利用通用定时器，利用CCR和CNT寄存器，输出指定占空比和频率的PWM波形。其功能的应用有：实现LED呼吸灯的效果、控制步进电机、控制直流电机转速等。\n导航图98 通用定时器框图：\n\n图片引自STM32 F1XX系列的中文参考手册。在通用定时器章节的定时器架构图中，本章讲解的定时器输出功能位于右下角的红色矩形中。\n定时器实现PWM输出的实现细节\n\n参考中文手册，实现细节图125如下：\n\n它内部实现是：输出模式控制器通过比较TIMx_CCR1（比较捕获寄存器）和TIMx_CNT（计数器）的值，由输出模式控制器来确定输出高（有效）电平，还是低（无效）电平，用户可以通过改变TIMx_CCR1寄存器的值来改变PWM的占空比。这通常会将输出模式控制器配置成PWM模式1或PWM模式2，两种模式就是互为取反的关系，同时这两种模式也是输出模式控制器最常用的配置。\n对于原理图125左侧的输出模式控制器：\n该部分作用是：控制输出模式控制器的输出行为。这里将输出模式控制器的输出标记为OC1REF。\n输出模式控制器有7种配置，这些配置是通过操作 TIMx_CCMR1.OC1M[6:4] 实现，7种配置在中文手册中的描述如下，手册中的描述可能太晦涩，配合江科的表格会更友好：\n\n\n对于原理图125右侧：\n右侧包括一个极性选择器和输出使能电路。\n该部分作用是：1、对输出电压的极性进行控制。2、控制输出电路的使能。\n我们可以通过配置 TIMx_CCER.CC1P[1] 控制选择器是直接选择OC1REF波形（输出模式控制器的输出）还是选择OC1REF的反相波形。也就是说，输出模式控制器配置成PWM1&#x2F;WPM2可以实现对OC1REF的反相，配置 TIMx_CCER.CC1P[1] 间接配置选择器也能实现对OC1REF的反相。通过配置 TIMx_CCER.CC1E[0] 可以实现对输出电路的使能。\n由定时器输出PWM的原理可以得出调节占空比的公式，PWM频率就是定时器溢出的周期、占空比就是TIMx_CCR1的值，其计算公式如下：\n\n关于定时器的PWM模块还需提一句的是，有三个寄存器存在缓冲寄存器&#x2F;影子寄存器的概念的，这三个寄存器分别是：ARR、PSC、CCRx。影子寄存器的存在延续旧值的生命周期，这样让旧值继续该时期的使命。如果用户提供的新值立马生效，系统就会出于一种未定义的状态。有了缓冲寄存器&#x2F;影子寄存器的概念，在一个更新周期中真正起作用的是影子寄存器，而用户想要修改预分频控制寄存器，会先将值写到缓冲器中，待这个更新周期过去，才会将缓冲器的值给到影子寄存器\n下面中文手册的的两张时序图可以很好的说明了：\n\n缓冲寄存器 —-&gt; 预分频控制寄存器\n影子寄存器 —-&gt; 预分频缓冲器\n还需注意的是：TIMx_CR1.ARPE[7]、TIMx_CCMR1.OC1PE[3]寄存器可以让用户选择ARR、CCRx是否启用影子寄存器的功能，而PSC寄存器默认必须使用影子寄存器的功能，但是用户可以通过TIM_PrescalerConfig函数动态配置计数器的预分频系数，它的第三个参数可以选择TIM_PSCReloadMode_Immediate，这会让定时器立即产生一个更新事件，间接实现了立即更新的效果。\n定时器实现PWM输出的步骤综上，可以总结出配置定时器输出部分的套路：\n\n我们需要把 TIMx_CCMR1.CC1S[1:0] 配置为00，这样CC1通道就被配置为输出\n\n通过配置 TIMx_CCMR1.OC1M[6:4] ，这里将输出模式控制寄存器配置成PWM1模式。即110。\n\n配置原理图右部分是否开启反相 TIMx_CCER.CC1P[1] ，这里配置为0不反相。\n\n最后使能 TIMx_CCER.CC1E[0] 位，来使能原理图右边的输出使能电路。\n\n\n一般的话，我门还会配置 TIMx_CCMR1.OC1PE[3] 、 TIMx_CR1.ARPE[7]，分别启用TIMx_CCR1、TIMx_ARR寄存器的影子功能。\n定时器实现PWM输出的库函数实现因为我的开发板LED0被焊在了PB5所以，所以需要将定时器的PWM波形输出到PB5上。经过查表需要对TIM3_CH2输出进行一个重映射；此外还要将PB5配置成复用推挽输出的状态。\n\n\n定时器PWM输出配置，实例代码如下：\nvoid LunarInitTIM3() &#123;\tTIM_TimeBaseInitTypeDef TIM3_Cfg;\tGPIO_InitTypeDef GPIOB5_Cfg;\tTIM_OCInitTypeDef TIM3_OCCfg;    // 定时器时基配置   BEGIN\t// 打开TIM3所需要的时钟 APB1\tRCC_APB1PeriphClockCmd(RCC_APB1Periph_TIM3, ENABLE);\tTIM_TimeBaseStructInit(&amp;TIM3_Cfg);\t// 配置使用内部时钟 72M Hz\tTIM_InternalClockConfig(TIM3);    // 这里配置定时器更新频率是1000HZ\tTIM3_Cfg.TIM_CounterMode = TIM_CounterMode_Up;\tTIM3_Cfg.TIM_Period = 100 - 1;\tTIM3_Cfg.TIM_Prescaler = 720 - 1;\tTIM_TimeBaseInit(TIM3, &amp;TIM3_Cfg);\t// 因为TIM_TimeBaseInit会置TIMx_EGR.UG[0]为1，手动产生一个更新事件，\t// 同时会同步影子寄存器的值，而该更新事件又会产生一个多余的中断，所以，\t// 我们需要在开启中断之前，手动清楚更新事件标志位\tTIM_ClearFlag(TIM3, TIM_FLAG_Update);    // 定时器时基配置   END\t// 配置GPIO \tBEGIN\t// 开启复用时钟\tRCC_APB2PeriphClockCmd(RCC_APB2Periph_AFIO, ENABLE);\t// 部分重映射\tGPIO_PinRemapConfig(GPIO_PartialRemap_TIM3, ENABLE);\t// 初始化GPIOB5为推挽复用输出\tRCC_APB2PeriphClockCmd(RCC_APB2Periph_GPIOB, ENABLE); \tGPIOB5_Cfg.GPIO_Mode = GPIO_Mode_AF_PP; \tGPIOB5_Cfg.GPIO_Pin = GPIO_Pin_5; \tGPIOB5_Cfg.GPIO_Speed = GPIO_Speed_2MHz; \tGPIO_Init(GPIOB, &amp;GPIOB5_Cfg);\t// 配置GPIO \tEND\t// 配置TIM3的PWM输出\tBEGIN\tTIM_OCStructInit(&amp;TIM3_OCCfg);\tTIM3_OCCfg.TIM_OCMode = TIM_OCMode_PWM1;\tTIM3_OCCfg.TIM_OCPolarity = TIM_OCPolarity_High;\tTIM3_OCCfg.TIM_OutputState = TIM_OutputState_Enable;\tTIM3_OCCfg.TIM_Pulse = 0;\tTIM_OC2Init(TIM3, &amp;TIM3_OCCfg);\t// 配置TIM3的PWM输出\tEND\t// 使能arr和ccr寄存器的影子功能\tTIM_OC2PreloadConfig(TIM3, TIM_OCPreload_Enable);\tTIM_ARRPreloadConfig(TIM3, ENABLE);\t// 使能更新中断\t// TIM_ITConfig(TIM3, TIM_IT_Update, ENABLE);\t// 开启定时器\tTIM_Cmd(TIM3, ENABLE);&#125;int main() &#123;\t// 初始化定时器\tLunarInitTIM3();    // 初始化系统定时器\tSYSTick_Init();\tint dir = 0, cr = 0;\twhile(1) &#123;\t\tTIM_SetCompare2(TIM3, cr);\t\tDelay_Ms(20);\t\tif (dir == 0) &#123;\t\t\tcr++;\t\t\tif (cr &gt; 99) &#123;\t\t\t\tdir = 1;\t\t\t\tcr = 99;\t\t\t&#125;\t\t&#125; else &#123;\t\t\tcr--;\t\t\tif (cr &lt; 0) &#123;\t\t\t\tdir = 0;\t\t\t\tcr = 0;\t\t\t&#125;\t\t&#125;\t&#125;\treturn 0;&#125;\n\n\n实验结果就是PB5处的LED灯实现了呼吸的效果。\n\n\n本章完结\n","tags":["STM32"]},{"title":"STM32快速入门（串口传输之USART）","url":"/2024/05/08/stm32/USART/","content":"前言USART串口传输能实现信息在设备之间的点对点传输，支持单工、半双工、全全双工，一般是有三个引脚：TX、RX、SW_RX（共地）。不需要一根线来同步时钟。最大优点是可以和电脑通信，实现程序调试的功能。\n导航图248 USART框图：\n\n图片引自STM32 F1XX系列的中文参考手册。\nUSART发送和接收的实现细节\n\n第一部分首先，对于图248的1号矩形框部分。该部分负责数据的发送和接收。（类似人体的四肢\n截取了中文手册有关USART的一幅时序图，如下：\n\n首先解释一下空闲帧和断开帧：\n从图中可以看到，空闲帧包括了停止位。而断开帧是10位低电平，后跟停止位(当m&#x3D;0时)；或者11位低电平，后跟停止位(m&#x3D;1时)。不可能传输更长的断开帧(长度大于10或者11位)。\n发送流程：\n\n引脚处于空闲状态时，一般是高电平状态。发送使能位被使能：USART_CR1.TE[3] 位被置为1。\n\n（由用户）数据写到发送数据寄存器。在写之前，用户会等待 USART_SR.TXE[7] 被硬件置位，只有该位被置为才说明发送数据寄存器为空，此时写入数据就是安全的，不会造成覆盖的问题。\n\n（以下步骤都是由硬件完成）将发送数据寄存器的内容移到发送移位寄存器，同时将USART_SR.TXE[7] 置位。以示发送数据寄存器为空。\n\n发送一个起始位。（低电平）\n\n从最低位开始，左移位将发送移位寄存器的值按位发送到TX引脚（对发送方）。\n\n如果用户使能了 USART_CR1.PCE[10] ，会根据 USART_CR1.PS[9] 发送一个校验位。\n\n最后，根据 USART_CR2.STOP[13:12] 的配置发送若干个停止位。(高电平)\n\n将状态寄存器 USART_SR.TC[6] 置位，表示数据的一帧发送完成。\n\n\n接收流程：\n\n接收使能位被使能：USART_CR1.RE[2]。\n\n（以下未特别说明，都是由硬件完成）从RX引脚（对接收方）检测到起始位，接收移位寄存器准备接收数据。\n\n接收移位寄存器从最高位开始，左移位依次按位从RX引脚（对接收方）接收数据。\n\n接收到停止位。\n\n如果使能了校验位的话，根据配置进行数据校验。\n\n校验合格的话，就将接收移位寄存器的值移到接收数据寄存器。\n\n将 USART_SR.RXNE[5] 置位，表示接收数据寄存器非空，提醒用户接收到数据了。\n\n（由用户）读取接收数据寄存器的数据。\n\n\n注意：\n\n虽然用户可以操作的寄存器只有一个USART_DR，但是实际上发送和接收数据寄存器在硬件上是各自一个！这样的设计也是双缓冲的实践。\n\n在发送和接收数据之前，用户必须统一设置两端的波特率、校验方式、停止位的数量、字长。否则这四项数据不一致，一定会造成传输错误，导致传输无法进行。其原因从上面的传输流程很容易推断。\n\n\n有关状态寄存器的位的解释如下：\n\n\n上面对过载错误位做了一个特写。这是因为我再编码的过程中遇到的一个BUG。排查了半天，原因是当RXNEIE接收中断位使能时，发送方的ORE标志位和RXNE标志位的置位都会触发RXNE事件的中断，当中断处理函数在处理完毕后，只复位RXNE标志，而不管ORE，后续还是会不断的产生中断。所以根据手册（手册其实是有误的），我们需要先读USART_SR，在读USART_DR将ORE标志位清除。（注意！库函数Clear类函数不能清楚ORE位！），这里放一张中断请求对应的事件表：\n\n第二部分对于图248的2号矩形框部分。该部分负责接收和发送的控制，（类似人体大脑。\n图中可以看到有很多的控制器、控制寄存器、标志寄存器等。我们可以设置相应的寄存器从而控制收发来实现一些功能。具体寄存器的功能可以参考中文手册，这里不过多赘述。\n第三部分对于图248的3号矩形框部分。该部分负责控制接收和发送的时钟。接收和发送的时钟也称之为波特率，通过波特率，通信双方就能协调其收发的频率（类似人体心脏。\n从图248的3号矩形框部分，可知，发送和接受器时钟是相等的。而时钟最开始是来自F_PCLK，送和接受器的时钟是对F_PCLK进行了一个 (16 * USARTDIV) 分频，USARTDIV是一个可调的定点小数。\n\n\n这里解释一下中文手册里面“如何从USART_BRR寄存器值得到USARTDIV”的示例一。 最开始看到这个例子我也是很懵的，什么是定点小数？这是怎么用整数来表示小数的？为什么 &lt;Fraction (USARTDIV) &#x3D; 12&#x2F;16 &#x3D; 0.75&gt; 这里要除以16？原理是这样的：\nUSART_BRR寄存器里面按定点小数的方式存放USARTDIV的值。只使用了16位，高12位存放小数的整数部分，低4位存放的是小数部分。整数部分很好说，直接存放进去就好了。而小数部分呢，因为小数部分一定是小于1的，所以，它根据低4位所能代表的值，将1划分成了2^4份，也就是16份，每一份占1&#x2F;16，所以我们要将小数部分表示成4位整数就将小数乘以16并向上取整即可。溢出的话就向整数部分进一。反之，要从4位整数还原小数，就用4位整数乘以1&#x2F;16。\n中文手册总结了一个公式：\n波特率 &#x3D; F_PCLK &#x2F; (16 * USARTDIV)\n通信必须维持相同的波特率。双方各自通过调节USARTDIV，就可以在不同环境下将双方但的波特率调成一样的。\n此外，还应该说明的是，公式中，有一个乘以 1 &#x2F; 16 的操作，这么做的目的是发送接收控制器里面有一个比波特率大16倍的采样频率。采样频率起到很好的滤波效果，它会对每一位进行16次采样。采样对于起始位的探测非常的精妙。并且，对于数据位，中间的8、9、10次采样会起到决定性作用。\n起始位探测：\n\n首先，我们称对第3、5、7次采样为第一阶段采样，对第8、9、10次采样为第二阶段采样。\n\n如果该序列不完整，那么接收端将退出起始位侦测并回到空闲状态(不设置标志位)等待下降沿。\n\n两个阶段检测的全是0，则确认收到起始位，这时设置RXNE标志位，如果RXNEIE&#x3D;1，则产生中断。\n\n如果两阶段中3个采样点上仅有2个是0，那么起始位仍然是有效的，但是会设置NE噪声标志位。如果不能满足这个条件，则中止起始位的侦测过程，接收器会回到空闲状态(不设置标志位)。\n\n如果两个阶段只有一个阶段中3个采样点上仅有2个是’0’，那么起始位仍然是有效的，但是会设置NE噪声标志位。\n\n\n数据位噪声探测：\n\n对数据位的采样只有一个阶段采样有效，即对8、9、10次采样。\n上方图片的下面的表格已经规定了采样的值和有效性的映射。读者可以好好的品味一下。\n最后，注意因为定点数表示小数是有精度的，所以波特率的计算是存在误差的，具体误差可以查阅中文手册。此外通过中文手册可知F_PCLK有两种情况：\n\nPCLK1用于USART2、3、4、5。\n\nPCLK2用于USART1\n\n\nUSART发送和接收的配置步骤USART的配置步骤比较简单。\n\n通信双方确定好波特率、停止位数、校验方式、字长。\n\n通过 USART_SR.RXNE[5] 产生的中断（接收数据寄存器非空），去异步接收数据。\n\n通过直接读写USART_DR寄存器可以实现数据的接收和发送。\n\n需要的话，可以等待 USART_SR.TC[6] 被硬件置位，来确保发送完成。\n\n处理中断后，一定要注意彻底清除中断相应的标志位！防止中断假触发！\n\n\nUSART发送和接收的代码我的开发板硬件连接图如下，所以本实验使用USART1进行串口通信。\n\n并且，将PA9、PA10分别配置成推挽复用输出、浮空输入或带上拉输入。\n\n\n代码如下：\nint fputc(int ch,FILE *p) &#123;//函数默认的，在使用printf函数时自动调用\tUSART_SendData(USART1,(u8)ch);\t\twhile(USART_GetFlagStatus(USART1,USART_FLAG_TXE)==RESET);\treturn ch;&#125;void LunarNVICInit()&#123;\tNVIC_InitTypeDef NVIC_Cfg;\t// 配置系统中断分组\tNVIC_PriorityGroupConfig(NVIC_PriorityGroup_2);\t// CPU上开启USART的中断\tNVIC_Cfg.NVIC_IRQChannel = USART1_IRQn;\tNVIC_Cfg.NVIC_IRQChannelCmd = ENABLE;\tNVIC_Cfg.NVIC_IRQChannelPreemptionPriority = 2;\tNVIC_Cfg.NVIC_IRQChannelSubPriority = 2;\tNVIC_Init(&amp;NVIC_Cfg);&#125;void LunarInitUSART1() &#123;\tGPIO_InitTypeDef GPIOA9_Cfg, GPIOA10_Cfg;\tUSART_InitTypeDef USART1_Cfg;\t// PA\tRCC_APB2PeriphClockCmd(RCC_APB2Periph_GPIOA,ENABLE);\t// 初始化GPIOA9为复用 （发送\tGPIOA9_Cfg.GPIO_Mode = GPIO_Mode_AF_PP;\tGPIOA9_Cfg.GPIO_Pin = GPIO_Pin_9;\tGPIOA9_Cfg.GPIO_Speed = GPIO_Speed_50MHz;\tGPIO_Init(GPIOA, &amp;GPIOA9_Cfg);\t// 初始化GPIOA10为复用 （接收\tGPIOA10_Cfg.GPIO_Mode = GPIO_Mode_IN_FLOATING;\tGPIOA10_Cfg.GPIO_Pin = GPIO_Pin_10;\tGPIO_Init(GPIOA, &amp;GPIOA10_Cfg);\t// USART1\tRCC_APB2PeriphClockCmd(RCC_APB2Periph_USART1,ENABLE);\tUSART1_Cfg.USART_BaudRate = 115200;\tUSART1_Cfg.USART_Mode = USART_Mode_Rx | USART_Mode_Tx;\tUSART1_Cfg.USART_Parity = USART_Parity_No;\tUSART1_Cfg.USART_StopBits = USART_StopBits_1;\tUSART1_Cfg.USART_WordLength = USART_WordLength_8b;\tUSART1_Cfg.USART_HardwareFlowControl = USART_HardwareFlowControl_None;\tUSART_Init(USART1, &amp;USART1_Cfg);\t// 接收中断\tUSART_ITConfig(USART1, USART_IT_RXNE, ENABLE);\t// 打开usart\tUSART_Cmd(USART1, ENABLE);&#125;// 中断处理程序void USART1_IRQHandler(void) &#123;\tif(USART_GetITStatus(USART1, USART_IT_RXNE) != RESET) &#123;\t// 接收数据中断\t\tuint16_t data = USART_ReceiveData(USART1);\t\tUSART_SendData(USART1, data);\t\twhile(USART_GetFlagStatus(USART1, USART_FLAG_TXE) == RESET);\t\t\t\tUSART_ClearFlag(USART1, USART_FLAG_TXE);\t\t\t&#125; else &#123;\t\t// 其他中断不做处理\t&#125;\t\t// 顺序去读SR和DR清楚ORE位   if (USART_GetFlagStatus(USART1, USART_FLAG_ORE) != RESET)&#123;\t\tUSART_ReceiveData(USART1);\t\t// USART_ClearFlag(USART1, USART_FLAG_ORE); // 函数USART_ClearFlag清楚不了USART_FLAG_ORE！！！   &#125;&#125;  int main() &#123;\t// 初始化usart\tLunarInitUSART1();\tLunarNVICInit();\tprintf(&quot;stm32 启动\\n&quot;);\twhile(1) &#123;\t&#125;\treturn 0; &#125;\n\n\n实验结果就是上位机通过给串口发送字符串，上位机接收框出现回显的效果。\n\n\n本章完结\n","tags":["STM32"]},{"title":"重写Sylar基于协程的服务器（2、配置模块的设计）","url":"/2024/02/01/sylar/Configure/","content":"重写Sylar基于协程的服务器系列：\n 重写Sylar基于协程的服务器（0、搭建开发环境以及项目框架 || 下载编译简化版Sylar）\n 重写Sylar基于协程的服务器（1、日志模块的架构）\n重写Sylar基于协程的服务器（2、配置模块的设计）\n重写Sylar基于协程的服务器（3、协程模块的设计）\n重写Sylar基于协程的服务器（4、协程调度模块的设计）\n重写Sylar基于协程的服务器（5、IO协程调度模块的设计）\n重写Sylar基于协程的服务器（6、HOOK模块的设计）\n重写Sylar基于协程的服务器（7、TcpServer &amp; HttpServer的设计与实现）\n配置模块存在的必要性\n一个服务器软件可能会运行在不同的机器上，机器的配置、网络环境、实际需求等都是千变万化，在服务器软件中，为了适应这些变化，可能就是调整几个变量的值。开发人员不可能每次外在因素的改变就重新编译软件再发布，这明显是不现实的。于是配置模块就在这时发挥它的关键作用，利用好配置模块就不需要再次编译，让配置模块自己加载参数，动态调节就行了。\n配置模块的设计与实现配置模块序列化和反序列化效果（支持std::各种容器YAML 的基本语法如下：\n\n大小写敏感。\n\n利用缩进表示层级关系，缩进只能使用空格，空格的数量不重要。\n\n‘#’表示注释。\n\n数据类型：对象，键值对的集合，即K-V对。数组，一组按次序排列的值。纯量（scalars），单个的、不可再分的值，包括字符串、布尔值、整数、浮点数、Null、时间、日期。\n\n\n测试配置文件定义了一个key为space，value也是一个map类型的节点，该map有两个kv对，它们的key分别是vec、num，value分别是数组类型和纯量类型。如下\ntest_config.yml:\n\nspace:  vec: [0, 1, 2, 3, 4, 5, 6, 7, 8, 9]  num: 7\n\n测试代码：\nvoid test_logConfig()&#123;    // 创建配置变量    lunar::ConfigVar&lt;std::vector&lt;int&gt;&gt;::ptr vec =         lunar::ConfigVarMgr::GetInstance()-&gt;lookUp(&quot;space.vec&quot;, std::vector&lt;int&gt;(), &quot;vec test&quot;);    lunar::ConfigVar&lt;int&gt;::ptr num = lunar::ConfigVarMgr::GetInstance()-&gt;lookUp(&quot;space.num&quot;, int(), &quot;num test&quot;);    // 解析配置文件    lunar::ConfigVarMgr::GetInstance()-&gt;loadFromFile(&quot;test_config.yml&quot;);    // 反序列化配置 &amp;&amp; 输出到控制台    LUNAR_LOG_INFO(g_logger) &lt;&lt; vec-&gt;toString();    LUNAR_LOG_INFO(g_logger) &lt;&lt; num-&gt;toString();&#125;\n\n解析结果：\n[root@localhost build]# ../bin/test_config 2024-01-31 21:02:02     2433    unknow  4294967295      [INFO]  [system]        /root/workspace/lunar/tests/test_config.cc:53       - 0- 1- 2- 3- 4- 5- 6- 7- 8- 92024-01-31 21:02:02     2433    unknow  4294967295      [INFO]  [system]        /root/workspace/lunar/tests/test_config.cc:54       7\n\n配置模块类的设计在Yaml提供的数据类型的基础上，我们的配置文件还需要支持对象类型，所以我们需要通过模板，以泛型编程的方式实现对复杂数据类型的解析，除此之外，我们还要，对各个配置变量进行集中管理，综上所述，类的设计如下：\n\nLexicalCast，对象转换类模板（仿函数），使用C++的模板二次封装Boost库的boost::lexical_cast模板函数，实现基础类型和字符串类型间相互转换。然后对LexicalCast类模板进行偏特化，让其支持C++标准库的容器的序列化与反序列化。所以，我们实现的LexicalCast类模板默认支持基础类型和C++容器，想要支持自定义类型的序列化和反序列化，用户需要自己实现全特化LexicalCast。\n\nConfigVarBase，配置变量基类，抽象出配置参数共有属性和方法比如变量名、对变量的描述、互斥锁、toString()、fromString()等，方便ConfigVarManager类使用多态对配置变量进行统一的管理。\n\nConfigVar，配置变量类模板，继承ConfigVarBase类，含有m_value成员变量，利用LexicalCast类模板，实现toString方法将配置变量序列化成Yaml String，实现fromString方法将Yaml String反序列化成配置变量。此外，还支持变更通知，在set方法中调用变更回调，通知引用配置变量的地方更新变量。\n\nConfigVarManager，配置变量管理类，利用std::map容器管理所有ConfigVar变量，std::map以配置变量名作为key，以配置变量基类智能指针作为value，支持配置变量的查询，用户在查询一条配置变量时，会提供该变量的变量名、默认值、变量描述，如果配置变量不存在，ConfigVarManager还会自动通过new 运算符利用默认值创建一个类型相同的配置变量，然后将&lt;变量名，自动创建的配置变量&gt;插到std::map中并返回给用户，因此ConfigVarManage还有自学习的能力。此外，提供了loadFromYaml函数支持对Yaml文件的解析，通过递归的方式解析Yaml文件中的每个map node节点，因为Yaml文件的map类型就是&lt;key, value&gt;对，这里key，value在配置文件中的含义和成员变量std::map中元素的含义是一致的,所以，取解析到的每个map node节点的key，去查该key是否存在于std::map成员中，如果存在，就调用相应配置变量基类的fronString函数，将map node的value作为参数，反序列化成一个配置变量。\n\n\nConfigVarManager的伪代码如下：\n\nyaml配置文件解析的核心代码如下：\n// 递归枚举每一个类型为map的节点。static void __ListAllYamlNode(std::string prefix,    const YAML::Node&amp; node,    std::vector&lt;std::pair&lt;std::string, YAML::Node&gt;&gt;&amp; output)&#123;    output.push_back(std::make_pair(prefix, node));    if(node.IsMap())&#123;        for(auto it = node.begin(); it != node.end(); it++)&#123;            __ListAllYamlNode((prefix.empty() ? prefix :prefix + &quot;.&quot;) + it-&gt;first.Scalar(), it-&gt;second, output);        &#125;    &#125;&#125;void ConfigVarManager::loadFromFile(const std::string&amp; val)&#123;    std::vector&lt;std::pair&lt;std::string, YAML::Node&gt;&gt; nodes;    YAML::Node root = YAML::LoadFile(val);    __ListAllYamlNode(&quot;&quot;, root, nodes);    for(auto it = nodes.begin(); it != nodes.end(); it++)&#123;        ConfigVarBase::ptr var = lookUpConfigVarBaseByName(it-&gt;first);        if(var != nullptr)&#123; // 有配置变量就解析该节点            std::stringstream ss(&quot;&quot;);            ss &lt;&lt; it-&gt;second;            var-&gt;fromString(ss.str());            // for debug            //LUNAR_LOG_DEBUG(LUNAR_LOG_NAME(&quot;system&quot;)) &lt;&lt; var-&gt;toString();        &#125;    &#125;&#125;\n\n关于yaml-cpp的使用可以参考官方文档：https://github.com/jbeder/yaml-cpp/wiki/Tutorial。\n感兴趣的同学，可以阅读一下本文实现的源码：https://github.com/LunarStore/lunar\n\n本章完结\n","tags":["高性能服务器框架"]},{"title":"STM32F103时钟分析","url":"/2024/06/11/stm32/RCC/","content":"SystemInit系统初始化时钟源码分析伪代码如下：\n\n/**  * @brief  Setup the microcontroller system  *         Initialize the Embedded Flash Interface, the PLL and update the   *         SystemCoreClock variable.  * @note   This function should be used only after reset.  * @param  None  * @retval None  */void SystemInit (void)&#123;  /* Reset the RCC clock configuration to the default reset state(for debug purpose) */  /*   *  时钟控制寄存器：   *  将RCC_CR的第0（HSION）位赋1，高速内部时钟使能,其他位不变   */  /* Set HSION bit */  RCC-&gt;CR |= (uint32_t)0x00000001;  /*   *  时钟配置寄存器：   *  将RCC_CFGR的SW[1:0]、SWS[3:2]、HPRE[7:4]、PPRE1[10:8]、PPRE2[13:11]、   *  ADCPRE[15:14]、MCO[26:24]置零，其他位不变。   */  /* Reset SW, HPRE, PPRE1, PPRE2, ADCPRE and MCO bits */  RCC-&gt;CFGR &amp;= (uint32_t)0xF8FF0000;      /*   *  时钟控制寄存器：   *  将RCC_CR的HSEON[16]、CSSON[19]、PLLON[24]置零，其他位不变。   */  /* Reset HSEON, CSSON and PLLON bits */  RCC-&gt;CR &amp;= (uint32_t)0xFEF6FFFF;  /*   *  时钟控制寄存器：   *  将RCC_CR的HSEBYP[18]置零，其他位不变。   */  /* Reset HSEBYP bit */  RCC-&gt;CR &amp;= (uint32_t)0xFFFBFFFF;  /*   *  时钟控制寄存器：   *  将RCC_CR的PLLSRC、PLLXTPRE、PLLMUL、USBPRE置零，其他位不变。[16:23]   */  /* Reset PLLSRC, PLLXTPRE, PLLMUL and USBPRE/OTGFSPRE bits */  RCC-&gt;CFGR &amp;= (uint32_t)0xFF80FFFF;  /*   *  时钟中断寄存器：   *  将RCC_CR的[16:20]位、PLLRDYC[20]、CSSC[23]等置1，其他为0.   */  /* Disable all interrupts and clear pending bits  */  RCC-&gt;CIR = 0x009F0000;  /* Configure the System clock frequency, HCLK, PCLK2 and PCLK1 prescalers */  /* Configure the Flash Latency cycles and enable prefetch buffer */  SetSysClock();  SCB-&gt;VTOR = FLASH_BASE | VECT_TAB_OFFSET; /* Vector Table Relocation in Internal FLASH. */&#125;\n\n结合STM32F1XX中文参考手册，我对上面的代码进行了注释。\n/**  * @brief  Configures the System clock frequency, HCLK, PCLK2 and PCLK1 prescalers.  * @param  None  * @retval None  */static void SetSysClock(void)&#123;#ifdef SYSCLK_FREQ_HSE  SetSysClockToHSE();#elif defined SYSCLK_FREQ_24MHz  //  ...#elif defined SYSCLK_FREQ_72MHz  SetSysClockTo72();#endif  /* If none of the define above is enabled, the HSI is used as System clock    source (default after reset) */ &#125;\n\n系统时钟初始化核心函数：\n/**  * @brief  Sets System clock frequency to 72MHz and configure HCLK, PCLK2   *         and PCLK1 prescalers.   * @note   This function should be used only after reset.  * @param  None  * @retval None  */static void SetSysClockTo72(void)&#123;  __IO uint32_t StartUpCounter = 0, HSEStatus = 0;    /* SYSCLK, HCLK, PCLK2 and PCLK1 configuration ---------------------------*/      // 使HSE时钟源使能。[16]  /* Enable HSE */      RCC-&gt;CR |= ((uint32_t)RCC_CR_HSEON);   // 循环等待RCC_CR寄存器的HSERDY[17]被置为1，此时才代表HSE时钟稳定了。  /* Wait till HSE is ready and if Time out is reached exit */  do  &#123;    HSEStatus = RCC-&gt;CR &amp; RCC_CR_HSERDY;    StartUpCounter++;    &#125; while((HSEStatus == 0) &amp;&amp; (StartUpCounter != HSE_STARTUP_TIMEOUT));  if ((RCC-&gt;CR &amp; RCC_CR_HSERDY) != RESET)  &#123;    HSEStatus = (uint32_t)0x01;  &#125;  else  &#123;    HSEStatus = (uint32_t)0x00;  &#125;    if (HSEStatus == (uint32_t)0x01)  &#123;    /*     *  省略无关代码...     */    // 设置CDGR寄存器的HPRE[7:4]，AHB预分频器分频系数为1。    /* HCLK = SYSCLK */    RCC-&gt;CFGR |= (uint32_t)RCC_CFGR_HPRE_DIV1; // 0000  // 72MHz    // 设置CDGR寄存器的PPRE2[13:11]，APB2预分频器分频系数为1。    /* PCLK2 = HCLK */    RCC-&gt;CFGR |= (uint32_t)RCC_CFGR_PPRE2_DIV1; // 000  // 72MHz        // 设置CDGR寄存器的PPRE1[10:8]，APB1预分频器分频系数为2。    /* PCLK1 = HCLK */    RCC-&gt;CFGR |= (uint32_t)RCC_CFGR_PPRE1_DIV2; // 100  // APB1最高只能36MHz    // 1、先将：PLLSRC[16]、PLLXTPRE[17]、PLLMUL[18:21]全置0，    // 2、PLLSRC[16]置为1，PLLXTPRE[17]保持为0，PLLMUL[21:18]置为0111    // 结果就是将HSE分频器的输出作为PLL输入时钟源，并且PLLMUL的备品系数设为9.    /*  PLL configuration: PLLCLK = HSE * 9 = 72 MHz */    RCC-&gt;CFGR &amp;= (uint32_t)((uint32_t)~(RCC_CFGR_PLLSRC | RCC_CFGR_PLLXTPRE |                                        RCC_CFGR_PLLMULL));    RCC-&gt;CFGR |= (uint32_t)(RCC_CFGR_PLLSRC_HSE | RCC_CFGR_PLLMULL9);    // PLLON[24]置为1，打开PLL时钟    /* Enable PLL */    RCC-&gt;CR |= RCC_CR_PLLON;    // PLLRDY被置为，等待PLL时钟稳定。    /* Wait till PLL is ready */    while((RCC-&gt;CR &amp; RCC_CR_PLLRDY) == 0)    &#123;    &#125;        // 1、SW[1:0]置为 00    // 2、SW[1:0]置为 10，设置sw选择器选择PLL输出作为系统时钟    /* Select PLL as system clock source */    RCC-&gt;CFGR &amp;= (uint32_t)((uint32_t)~(RCC_CFGR_SW));    RCC-&gt;CFGR |= (uint32_t)RCC_CFGR_SW_PLL;        // 等待SWS[3:2]被置为 10，等待时钟稳定。    /* Wait till PLL is used as system clock source */    while ((RCC-&gt;CFGR &amp; (uint32_t)RCC_CFGR_SWS) != (uint32_t)0x08)    &#123;    &#125;  &#125;  else  &#123; /* If HSE fails to start-up, the application will have wrong clock          configuration. User can add here some code to deal with this error */  &#125;&#125;\n\nPLLON为0才能对PLLSRC、PLLXTPRE、PLLMUL设置！！！\n","tags":["STM32"]},{"title":"重写Sylar基于协程的服务器（3、协程模块的设计）","url":"/2024/02/01/sylar/Fiber/","content":"重写Sylar基于协程的服务器系列：\n 重写Sylar基于协程的服务器（0、搭建开发环境以及项目框架 || 下载编译简化版Sylar）\n 重写Sylar基于协程的服务器（1、日志模块的架构）\n重写Sylar基于协程的服务器（2、配置模块的设计）\n重写Sylar基于协程的服务器（3、协程模块的设计）\n重写Sylar基于协程的服务器（4、协程调度模块的设计）\n重写Sylar基于协程的服务器（5、IO协程调度模块的设计）\n重写Sylar基于协程的服务器（6、HOOK模块的设计）\n重写Sylar基于协程的服务器（7、TcpServer &amp; HttpServer的设计与实现）\n前言\n关于线程以及线程并发的封装在此略过该部分比较简单，有兴趣的朋友可以看一下我原来写的Muduo的博客：muduo源码阅读笔记（2、对C语言原生的线程安全以及同步的API的封装）、muduo源码阅读笔记（3、线程和线程池的封装），或者直接阅读本文配套的简化版sylar的源码：https://github.com/LunarStore/lunar。\n协程模块的设计与实现协程的状态定义协程分为：初始化状态、执行状态、阻塞状态、就绪状态、结束状态、异常状态。定义如下：\nenum State&#123;    INIT,   // 初始化    EXEC,   // 执行    HOLD,   // 阻塞    READY,  // 就绪    TERM,   // 结束    EXCE    // 异常&#125;;\n\n协程的状态机任务协程：\n\n一个任务协程运行时，可能的状态机图，如下图:\n\n可能跟着协程调度模块、io协程调度模块走一遍调度任务协程的流程后，才能清晰的理解该图的意义，这里可以先以整体的视角，看一看一个任务协程的状态切换时机即可。\n整体调度流程简述：\n系统中的每个线程最开始会被初始化去执行调度协程，调度协程会向任务队列中取协程任务，如果任务是回调，也会将它封装成一个协程对象。然后调度协程会去swapIn进去执行任务协程。当任务协程调用了IO系统调用或者类sleep系统调用（这些系统调用有被hook重新实现），任务协程做好切回操作后（向epoll注册事件，并将对应的协程和fd绑定；或者向定时器模块注册一下，将协程和定时器绑定。），就swapOut出去，将线程时间片交给调度协程，调度协程继续去调度任务队列的下一个任务协程。当任务队列中没有协程任务后，调度协程会去执行Idle协程，Idle协程会使线程真正阻塞在epoll_wait上，等待IO事件的到来或者定时器超时事件发生，当有事件到来，Idle协程就会被唤醒，然后将被绑定的协程加入到任务队列中，Idle协程就会swapOut出去，将线程时间片交给调度协程继续去调度任务协程。\n调度协程&amp;&amp;Idle协程：\n正常情况下，调度协程和Idle协程的状态机如下图：\n\n协程模块的设计如下总体来讲，协程模块（Fiber类）主要就是对ucontext_t提供的getcontext、makecontext、swapcontext等函数做了一个封装，为了简化后续调度模块使用协程。要讲清楚协程模块，不得不引入类型为Finer::ptr（就是Fiber对象的智能指针）三个重要的线程全局变量，分别是：线程原始协程（t_threadFiber）、线程当前正在运行的协程（t_fiber）、调度协程（t_scheRunFiber），这三个全局变量在每个线程中都会拥有一份，独立于其他线程。由于引入了协程的概念，我们就应该弱化线程的概念，以一切皆是协程的思想去编写代码。所以一个线程在被创建时，它原始的上下文就可以作为一个协程，我们把它的上下文保存在t_threadFiber中，这样可以待线程的其他协程执行完毕，再切换回来。具体类的设计如下：\n\n构造函数，用户在构造一个协程时，会传入协程的回调函数、协程堆栈大小、指明协程在切入去执行时是和哪个协程做切换（t_threadFiber或者t_scheRunFiber），这里额外再解释一下，因为一个协程的切入可以看成是在另一个协程中进行的，所以一个协程在切入执行前，首先要保存先前协程的上下文，因为我们实现的协程服务器是有调度器的，而且每个线程会执行调度协程进行任务协程的调度，所以，一般任务协程在切入执行前，是要先将上下文保存在t_scheRunFiber中，再将上下文修改成任务协程的上下文。待任务协程执行完毕或者阻塞，再反向操作，先将上下文保存到任务协程中，再将上下文还原成t_scheRunFiber去调度其他协程。协程初始化首先会调用getcontext初始化m_ctx成员变量，该成员变量是保存协程的上下文核心数据结构，然后调用makecontext将协程上下文设置成静态成员函数MainFun，在静态成员函数MainFun中，会通过线程局部变量t_fiber（切入时设置）获取到协程当前运行的协程对象，进而回调用户传来的call back函数。\n\n协程默认构造，该构造是私有化的，专门为线程原始的协程打造，内部只调用getcontext函数。\n\nswapIn成员函数，根据用户指定调用swapcontext，和t_threadFiber做切换或者和t_scheRunFiber做切换。\n 协程切入伪代码：\n \n\nswapOut成员函数，协程切换回t_threadFiber或t_scheRunFiber时使用，也是调用swapcontet。\n 协程切出伪代码：\n \n\nyieldToHold&#x2F;yieldToReady，设置协程状态然后调用swapOut。\n\n\n\n下一章将介绍协程调度模块\n感兴趣的同学，可以阅读一下本文实现的源码：https://github.com/LunarStore/lunar\n\n本章完结\n","tags":["高性能服务器框架"]},{"title":"重写Sylar基于协程的服务器（6、HOOK模块的设计）","url":"/2024/02/04/sylar/Hook/","content":"重写Sylar基于协程的服务器系列：\n 重写Sylar基于协程的服务器（0、搭建开发环境以及项目框架 || 下载编译简化版Sylar）\n 重写Sylar基于协程的服务器（1、日志模块的架构）\n重写Sylar基于协程的服务器（2、配置模块的设计）\n重写Sylar基于协程的服务器（3、协程模块的设计）\n重写Sylar基于协程的服务器（4、协程调度模块的设计）\n重写Sylar基于协程的服务器（5、IO协程调度模块的设计）\n重写Sylar基于协程的服务器（6、HOOK模块的设计）\n重写Sylar基于协程的服务器（7、TcpServer &amp; HttpServer的设计与实现）\n简述\nHOOK模块存在的必要性：让IO系统调用，以同步写法展现出异步的性能。\nhook实际上就是对系统调用API进行一次封装，将其封装成一个与原始的系统调用API同名的接口，应用在调用这个接口时，会先执行封装中的操作，再执行原始的系统调用API。本文实现的是一种利用dlsym函数实现的侵入式hook。\nHOOK的实现首先，是hook重新实现accept函数、socket函数，为了高效的cpu利用率，每个被accept函数接受的socketfd、或者socket创建的fd在返回给调用者前，都会使用fcntl函数，将其设置为非阻塞，虽然socketfd被设置成非阻塞的，但是我们的hook机制，能够利用非阻塞的socketfd实现一种在调用者看来是阻塞的socketfd。\n其次，为了获取socketfd的超时时间，所有setsockop也会被hook，如果socketfd之前使用setsockop函数设置超时，其超时时间就会被fdmanager类获取，将超时时间记录在对应的fd上。当然，为了善后，close也会被hook，在调用真正close前，会唤醒相应socketfd上的所有监听协程，让协程退出。\n最后，将socketIO有关的系统调用（如read、write、accept等）抽象出一个统一的接口do_io，用户在调用socketIO有关函数时，底层统一调用do_io。do_io流程如下。\n\n进入do_io函数内部，首先会检查socketfd的合法性。\n\n如果socketfd是合法的，就去调用fun。（fun是通过do_io参数传进来的真正的io系统调用函数地址）。\n\n如果sockefd是非法的，也会去调用一次fun，fun返回什么，do_io就返回什么，完全依赖原始的系统底层调用对非法socketfd的处理。\n\n对于合法的socketfd调用fun，由于socketfd是非阻塞的，不管是否读取到数据，fun都会立刻返回。如果读到数据，do_io整体就返回读到的字节数。如果没有读到数据，fun会返回-1，errno为EAGAIN，但是do_io不会返回。\n\n然后调用线程当前的IOManager的addEvent函数，根据用户调用的socketIO相关接口对socketfd添加事件相应读写事件，在addEvent函数内部会把&lt;当前协程，当前协程的协程调度器&gt;放入socketfd对应的EventContext结构体里面，等待socketfd上io事件到来并由idle协程调用TrigleEvent函数唤醒当前协程。\n\n然后调用yieldToHold将协程变成Hold状态，等待socketfd上io事件到来时idle协程将该协程唤醒。\n\n\n唤醒后，首先查看是怎么被唤醒的，如果是因为超时被唤醒，do_io就返回超时的错误，如果是因为有io事件到来被唤醒，那么回到第4步。\n\n\nHOOK模块核心函数do_io伪代码：\n\ndo_io的使用（也即read、write系统调用的hook）\n\n下一章将介绍TcpServer模块。\n感兴趣的同学，可以阅读一下本文实现的源码：https://github.com/LunarStore/lunar\n\n本章完结\n","tags":["高性能服务器框架"]},{"title":"重写Sylar基于协程的服务器（1、日志模块的架构）","url":"/2024/01/30/sylar/Log/","content":"重写Sylar基于协程的服务器系列：\n 重写Sylar基于协程的服务器（0、搭建开发环境以及项目框架 || 下载编译简化版Sylar）\n 重写Sylar基于协程的服务器（1、日志模块的架构）\n重写Sylar基于协程的服务器（2、配置模块的设计）\n重写Sylar基于协程的服务器（3、协程模块的设计）\n重写Sylar基于协程的服务器（4、协程调度模块的设计）\n重写Sylar基于协程的服务器（5、IO协程调度模块的设计）\n重写Sylar基于协程的服务器（6、HOOK模块的设计）\n重写Sylar基于协程的服务器（7、TcpServer &amp; HttpServer的设计与实现）\n前言\n和Muduo的日志对比，Muduo的同步日志虽然格式固定，但简单高性能。而sylar的日志设计的显得过于冗余，虽然灵活性强、扩展性高，但是性能却不及Muduo。尽管陈硕大大也说过，简单才能保证高性能，日志就没必要设计的那么花里胡哨，但是sylar对日志的设计思想以及设计模式超级超级适合小白去学习。\n日志格式由于用户可能并不需要将日志上下文的每一项都进行输出，而是希望可以自由地选择要输出的日志项。并且，用户还可能需要在每条日志里增加一些指定的字符，比如在文件名和行号之间加上一个冒号的情况。为了实现这项功能，LogFormatter使用了一个模板字符串来指定格式化的方式。模板字符串由普通字符和占位字符构成。在构造LogFormatter对象时会指定一串模板字符，LogFormatter会首先解析该模板字符串，将其中的占位符和普通字符解析出来。在格式化日志上下文时，根据模板字符串，将其中的占位符替换成日志上下文的具体内容，普通字符保持不变。下表是支持的占位符的含义。\n\n\n\n占位符\n含义\n\n\n\n%s\n普通字符串（直接输出的字符串）\n\n\n%d\n时间\n\n\n%t\n线程真实id\n\n\n%N\n线程名\n\n\n%f\n协程id\n\n\n%p\n日志级别\n\n\n%c\n日志器名\n\n\n%F\n文件路径\n\n\n%l\n行号\n\n\n%m\n日志消息\n\n\n%T\nTab缩进\n\n\n%n\n换行\n\n\n以%d&#123;%Y-%m-%d %H:%M:%S&#125;%T%t%T%N%T%f%T[%p]%T[%c]%T%F:%l%T%m%n格式串为例，输出效果如图下：\n\n\n时间占位符%d需要带有格式化参数%Y-%m-%d %H:%M:%S，这使得日志格式器能对日志上下文收集到的时间戳进行格式化，而对于其他占位符，日志格式器只需要从日志上下文中取来做简单的处理再直接拼接即可。\n日志模块架构设计sylar实现的日志中，一条日志数据流向是：日志包装器-&gt;日志器-&gt;数个日志输出地，如图所示。\n\n关于这几个类的设计如下：\n\nLogEvent: 日志上下文，用于记录日志现场，比如该日志的时间，文件路径&#x2F;行号，日志级别，线程&#x2F;协程号，日志消息等。\n\nLogEventWrap: 日志事件包装类，其实就是将日志事件和日志器一起包装到日志上下文中，因为一条日志只会在一个日志器上进行输出。将日志事件和日志器包装到一起后，方便通过宏定义来简化日志模块的使用（这点和Muduo很像）。另外，LogEventWrap还负责在构建时指定日志事件和日志器，在析构时调用日志器的log方法将日志事件进行输出。\n\nLogger: 日志器，负责进行日志输出。一个Logger包含多个LogAppender和一个日志级别，提供log方法，传入日志事件，如果日志事件的级别高于日志器本身的级别就调用每一个LogAppender的log方法将日志进行输出，否则该日志被抛弃。\n 日志包装器以及日志器伪代码：\n \n\nLogAppender: 日志输出器，用于将一个日志上下文输出到对应的输出地。该类内部包含一个LogFormatter成员和一个log方法，日志事件先经过LogFormatter格式化后再输出到对应的输出地。从这个类可以派生出不同的输出器类型，比如StdoutLogAppender和FileLogAppender，分别表示终端和文件的日志输出器。\n 日志输出器伪代码：\n \n\nLogFormatter: 日志格式器，用于格式化一个日志事件。该类构建时可以指定pattern并根据提供的pattern调用init()进行解析。提供format方法，用于将日志事件格式化成字符串。\n 日志格式串的解析：\n void LogFormatter::init()&#123;    std::vector&lt;std::tuple&lt;std::string, std::string&gt;&gt; res;    int len = m_pattern.length();    //state -- 0 普通字符部分/日志修饰字符    //state -- 1 格式化字符部分    //state -- 2 格式化字符参数部分    int pLt = 0, pRt = 0, state = 0 ;    //&#x27;\\0&#x27;看成万能字符    while(pRt &lt;= len)&#123;        if(state == 0)&#123;            //状态升级            if(pRt == len || m_pattern[pRt] == &#x27;%&#x27;)&#123;                if(pLt &lt; pRt)&#123;                    res.push_back(std::make_tuple(&quot;s&quot;, m_pattern.substr(pLt, pRt - pLt)));                &#125;                state = 1;  //升级                pLt = pRt + 1;            &#125;        &#125;else if(state == 1)&#123;            //状态还原 或 状态升级             //或 此时遇到非&#123;，非%，非字母的字符，则隐式代表格式化字符部分结束            if(pRt &lt; len &amp;&amp; m_pattern[pRt] == &#x27;&#123;&#x27;)&#123;                if(pLt &lt; pRt)&#123;                    res.push_back(std::make_tuple(m_pattern.substr(pLt, pRt - pLt), &quot;&quot;));                &#125;else&#123;                    //错误：没有模式字符只有选项参数                    res.push_back(std::make_tuple(&quot;s&quot;, &quot;&lt;parse error&gt; empty format character : &quot;));                &#125;                state = 2;                pLt = pRt + 1;            &#125;            else if(pRt &lt; len &amp;&amp; m_pattern[pRt] == &#x27;%&#x27;)&#123;                if(pLt &lt; pRt)&#123;                    res.push_back(std::make_tuple(m_pattern.substr(pLt, pRt - pLt), &quot;&quot;));                &#125;                state = 0;                pLt = pRt;                continue;            &#125;else if(pRt == len || !isalpha(m_pattern[pRt]))&#123;                if(pLt &lt; pRt)&#123;                    res.push_back(std::make_tuple(m_pattern.substr(pLt, pRt - pLt), &quot;&quot;));                &#125;                state = 0;                pLt = pRt;            &#125;        &#125;else&#123;  //state == 2            //状态还原            //缺少&#125;，结尾（&quot;\\0&quot;）默认为&#x27;&#125;&#x27;            if(pRt == len || m_pattern[pRt] == &#x27;&#125;&#x27;)&#123;                std::get&lt;1&gt;(res.back()) = std::get&lt;1&gt;(res.back()) + m_pattern.substr(pLt, pRt - pLt);                state = 0;                pLt = pRt + 1;            &#125;        &#125;        pRt++;    &#125;    // ... 省略不重要的部分&#125;\n\nLogManager: 日志器管理类，单例模式，用于统一管理所有的日志器，提供日志器的创建与获取方法。LogManager自带一个root Logger，用于为日志模块提供一个初始可用的日志器。\n\n\n简化日志使用的宏定义：\n日志宏：\n#define LUNAR_LOG_LEVEL(logger, level) \\    if((logger)-&gt;getLevel() &lt;= level) \\        (lunar::LogEventWrap(lunar::LogEvent::ptr(new lunar::LogEvent(__FILE__, __LINE__,\\        lunar::GetElapse(), lunar::Thread::GetTid(),\\        lunar::GetFiberId(), ::time(nullptr),\\        lunar::Thread::GetName(), level, (logger))))).getMsg()#define LUNAR_LOG_DEBUG(logger) LUNAR_LOG_LEVEL(logger, lunar::LogLevel::Level::DEBUG)#define LUNAR_LOG_INFO(logger) LUNAR_LOG_LEVEL(logger, lunar::LogLevel::Level::INFO)#define LUNAR_LOG_WRONG(logger) LUNAR_LOG_LEVEL(logger, lunar::LogLevel::Level::WRONG)#define LUNAR_LOG_ERROR(logger) LUNAR_LOG_LEVEL(logger, lunar::LogLevel::Level::ERROR)#define LUNAR_LOG_FATAL(logger) LUNAR_LOG_LEVEL(logger, lunar::LogLevel::Level::FATAL)\n\n其他关于异步日志\n本着基于协程淡化线程的思想，在sylar中，要实现异步日志的前提是，先要基于协程实现一个信号量，然后通过继承LogAppender参考Muduo设计一个异步日志，但是其后台是由一个协程进行落盘。\n异步日志的实现涉及到后面的fiber、hook等模块，本文简略带过。\n异步日志和同步日志对比\n\n\n\n\n优点\n缺点\n\n\n\n异步日志\n执行效率更高，一般应用程序只用将日志输出到一块缓存中，由另起的线程将缓存中的日志输出到磁盘上，减少了系统的IO负担\n当系统崩溃时，容易丢失内存中来不及写入的日志。日志本身的代码实现、调试更复杂\n\n\n同步日志\n事件发生就输出，系统崩溃不会出现丢日志的情况，日志输出顺序可控，代码实现简单\n效率更低，增加系统IO负担，输出日志时会阻塞工作线程\n\n\n感兴趣的同学，可以阅读一下本文实现的源码：https://github.com/LunarStore/lunar\n\n本章完结\n","tags":["高性能服务器框架"]},{"title":"重写Sylar基于协程的服务器（5、IO协程调度模块的设计）","url":"/2024/02/03/sylar/IOManager/","content":"重写Sylar基于协程的服务器系列：\n 重写Sylar基于协程的服务器（0、搭建开发环境以及项目框架 || 下载编译简化版Sylar）\n 重写Sylar基于协程的服务器（1、日志模块的架构）\n重写Sylar基于协程的服务器（2、配置模块的设计）\n重写Sylar基于协程的服务器（3、协程模块的设计）\n重写Sylar基于协程的服务器（4、协程调度模块的设计）\n重写Sylar基于协程的服务器（5、IO协程调度模块的设计）\n重写Sylar基于协程的服务器（6、HOOK模块的设计）\n重写Sylar基于协程的服务器（7、TcpServer &amp; HttpServer的设计与实现）\n简述\nsylar的IOManager模块本质上就是一个事件池，主要负责向epoll中注册事件和回调。实现了Idle协程的回调，回调是一个阻塞在epoll_wait上的事件循环，将有IO事件发生的协程唤醒。\n核心数据结构将socketfd封装成一个结构体FdContext，对于fd上的读写事件封装成EventContext，提供的TrigleEvent函数，在fd有读写事件发生时，将相应读写事件的EventContext::m_fiber成员放到EventContext::m_scheduler调度器中，可以唤醒阻塞的协程。FdContext结构体定义如下。\n\nstruct FdContext &#123;    typedef Mutex MutexType;    struct EventContext&#123;        void reset();        Fiber::ptr m_fiber = nullptr;        std::function&lt;void(void)&gt; m_cb = nullptr;        //记录协程调度器，表示，当事件发生，fiber or callback应该使用哪个协程调度器调度。        Scheduler* m_scheduler = nullptr;       &#125;;    // nolock    void TrigleEvent(Event event);    EventContext&amp; getEventContext(Event event);    void resetEventContext(Event event);    EventContext read;  //读事件Handle    EventContext write; //写事件Handle    Event m_events = NONE;  //记录当前FdContext哪些事件有效    int m_fd = -1;    MutexType m_mutex;&#125;;\n\nIOManager成员变量IOManager的成员变量中有一个类型为std::vector&lt;FdContext*&gt;的数组，以每个socketfd作为数组下标，每个fd都能对应一个FdContext。如下。\nclass IOManager: public Scheduler, public TimerManager&#123;    /*        ...    */private:    std::vector&lt;FdContext*&gt; m_fdContexts;   // fd -&gt; FdContext（和muduo的channel类似）    std::atomic&lt;uint32_t&gt; m_penddingEventCount = &#123; 0 &#125;; // epoll监听的事件数    int m_pipfd[2]; // 管道fd、也称ticklefd，也称wakefd（Muduo）    int m_epollfd;    MutexType m_mutex;  // 全局锁&#125;;\n\nIO协程调度器的设计IO协程调度模块继承自协程调度模块，重新实现了协程调度的idle函数、isStop函数、tickle函数（功能和Muduo中EventLoop的wakeup函数一样）等。因为IO协程调度器继承自协程调度器，所以，现约定以下出现的协程调度器也指IO协程调度器。\n\n构造函数，创建一对个管道和一个epollfd，并将管道的读端注册到epollfd上，作用是：通知和唤醒阻塞在idle协程的epoll_wait函数上的线程，每次在向调度器任务队列中添加任务时，向管道的写端写入一个字符，以唤醒阻塞的线程，开始处理任务。在构造函数里面会自动启动协程调度器。\n\naddEvent，向epollfd注册一个fd的读写事件，并记录到IOManager::m_fdContexts数组里面，等待事件的触发。\n\ndelEvent &#x2F; cancelEvent， 向epollfd取消或删除一个fd的读写事件，并记录到IOManager::m_fdContexts数组里面。\n\nIdle函数（即idle协程的回调函数），既然调度到了idle协程，说明调度器的任务队列里面没任务了，所以，idle函数首先会调用epoll_wait，带超时的阻塞线程一段时间，超时或者有事件发生时，会检查有无定时器超时，如果有定时器超时就会将超时回调函数放到任务队列中去调度，然后去检查有哪些fd发生了哪些事件，将相关的协程唤醒。\n\n\nIdle函数伪代码如图：\n\nIdle函数流程图：\n\n下一章将介绍HOOK模块。\n感兴趣的同学，可以阅读一下本文实现的源码：https://github.com/LunarStore/lunar\n\n本章完结\n","tags":["高性能服务器框架"]},{"title":"重写Sylar基于协程的服务器（0、搭建开发环境以及项目框架 || 下载编译简化版Sylar）","url":"/2024/01/28/sylar/Start/","content":"重写Sylar基于协程的服务器系列：\n 重写Sylar基于协程的服务器（0、搭建开发环境以及项目框架 || 下载编译简化版Sylar）\n 重写Sylar基于协程的服务器（1、日志模块的架构）\n重写Sylar基于协程的服务器（2、配置模块的设计）\n重写Sylar基于协程的服务器（3、协程模块的设计）\n重写Sylar基于协程的服务器（4、协程调度模块的设计）\n重写Sylar基于协程的服务器（5、IO协程调度模块的设计）\n重写Sylar基于协程的服务器（6、HOOK模块的设计）\n重写Sylar基于协程的服务器（7、TcpServer &amp; HttpServer的设计与实现）\n前言\nsylar是一个基于协程的服务器框架。同go语言思想一样，整个框架贯彻协程化的思想，淡化线程的存在。笔者有幸反复阅读sylar数次，并重写过base核心模块。该项目是我真正入门C++的第一个项目，我也将其作为本科毕设，顺利通过答辩。非常感谢sylar的作者能将多年从业经验浓缩在这个项目当中，这真的是为后来者点了一扇关键的灯。\n环境搭建以及下载安装开发环境参考如下表：\n\n\n\n环境\n版本\n\n\n\nLinux操作系统\nCentOS 7.5 64位（2核2G）\n\n\nG++编译器\n4.8.5\n\n\nCMake\n3.14.5\n\n\nC++标准\nC++11\n\n\n项目调试工具\nGDB\n\n\n项目开发工具\nVSCode\n\n\n搭建项目框架：\n本系统的文件结构如图所示。项目根目录有CMakeList文件和Makefile文件，也即有两种编译方式。编译输出的中间文件输出在build目录，二进制文件输出在bin目录下，静态库以及动态库输出在lib目录下，src目录存放项目源码，源码划分为基础模块、网络模块、初始化模块、以及HTTP模块，本文重点集中在基础模块，即src&#x2F;base目录下的文件。bin目录存放的是二进制可执行程序。bin&#x2F;conf以及bin&#x2F;module目录分别存放的是，可执行程序的配置文件以及动态库模块。\n\n下载、编译、试玩重写的简化版sylar：\n\n去除了sylar原来的负载均衡、数据库连接、protobuff、orm、zk等模块。（实际上是太菜了，这些模块没跟下去。）\n#源码下载git clone https://github.com/LunarStore/lunar.git # 安装boost库yum install boost-devel # boost库# 安装yaml-cpp（可能需要升级cmakegit clone https://github.com/jbeder/yaml-cpp.gitcd yaml-cppmkdir build &amp;&amp; cd buildcmake .. &amp;&amp; make -jsudo make install# 创建build目录cd lunarmkdir buildcd build# 在build目录生成makefile文件cmake ..# 编译make -j2# 建立一个必要目录# 否则会报错：# 2024-01-28 05:01:06\t[ERROR]\t[system]\t/root/workspace/lunar/src/init/application.cc:116\topen pidfile /apps/work/lunar/lunar.pid failedmkdir -p /apps/work/lunar# 终端运行，让网站跑起来../bin/test_application -s# 关闭防火墙systemctl stop firewalld\n\n浏览器输入ip:8090出现默认页如下：\n\n\n本章完结\n","tags":["高性能服务器框架"]},{"title":"重写Sylar基于协程的服务器（4、协程调度模块的设计）","url":"/2024/02/02/sylar/Scheduler/","content":"重写Sylar基于协程的服务器系列：\n 重写Sylar基于协程的服务器（0、搭建开发环境以及项目框架 || 下载编译简化版Sylar）\n 重写Sylar基于协程的服务器（1、日志模块的架构）\n重写Sylar基于协程的服务器（2、配置模块的设计）\n重写Sylar基于协程的服务器（3、协程模块的设计）\n重写Sylar基于协程的服务器（4、协程调度模块的设计）\n重写Sylar基于协程的服务器（5、IO协程调度模块的设计）\n重写Sylar基于协程的服务器（6、HOOK模块的设计）\n重写Sylar基于协程的服务器（7、TcpServer &amp; HttpServer的设计与实现）\n简述\n协程调度模块，让线程池里的每个线程都运行调度协程，并不断切换去执行协程任务。\n协程调度器整体架构图sylar实现的协程是非对称协程，虽然就调度器的架构看来，很反人类，一眼看去很像是对称协程。\n问了一下GPT，回答如下：\n实现了 IO Hook 模块的协程通常是非对称协程模型。在异步编程中，IO Hook 通常用于异步 IO 操作，而非对称协程模型更适合处理异步 IO 操作。\n在非对称协程模型中，一个主协程（通常是事件循环或主任务）可以通过 IO Hook 来注册感兴趣的 IO 事件，并在事件发生时启动相应的协程执行。这样的模型更适用于事件驱动的编程，其中主协程负责管理整体的控制流，而子协程负责处理具体的 IO 操作。\n协程调度器模块的设计是基于线程池来完成的，对线程池进行协程的定制化改造，让线程池模型能够适应协程的切换，如图：\n\n协程调度模块设计\n构造函数，用户创建协程调度器主要的参数有，设置参与协程调度的线程数量threadCount、主线程是否参与协程调度等，构造函数首先会为主线程原始的上下文创建一个协程（t_threadFiber），其次，如果用户指定了主协程需要参与协程调度，就会为成员变量m_rootFiber创建一个回调函数是Scheduler::run()的协程，并且指定该协程与t_threadFiber做上下文切换。主线程等待后面延迟将m_rootFiber切入，进入Scheduler::run()函数后，t_threadFiber保存主线程原始上下文，t_schRunFiber赋值为m_rootFiber即运行Scheduler::run()函数的协程。而子线程运行的回调函数就是Scheduler::run()函数，所以，子线程的t_threadFiber和t_schRunFiber是同一个协程对象。并且，因为主线程充当了一个调度协程，所以，创建子线程的时候，会少创建一个线程，即子线程的数量等于threadCount-1。当用户没有要使用主线程充当调度协程时，调度器最后会创建threadCount个子线程。\n\nScheduler::run，协程调度部分，进入调度函数最开始会初始化t_threadFiber和t_schRunFiber变量，然后进入调度循环，在调度循环中，首先到任务队列中取任务，取到任务时，判断任务是协程还是回调，如果是协程，判断协程状态的合法性，只有合法的协程才能切入去执行，对于回调，会被封装成协程，再切入去执行。如果没有任务，就会去执行idle协程，idle协程是IO协程调度模块的重点，主要负责等待事件的到来然后唤醒相应任务协程。\n 协程调度函数Scheduler::run伪代码：\n\n\n\n\n![协程调度函数](./Scheduler/photo/SchedilerRun.png)\n\n\nScheduler::isStop，当用户调用了成员stop函数，且任务队列队列为空，且没有任何线程正在执行协程任务，即可认为整个协程调度器停止了，此时该函数返回true。\n\n下一章将介绍IO协程调度模块。\n感兴趣的同学，可以阅读一下本文实现的源码：https://github.com/LunarStore/lunar\n\n本章完结\n","tags":["高性能服务器框架"]},{"title":"重写Sylar基于协程的服务器（7、TcpServer & HttpServer的设计与实现）","url":"/2024/02/04/sylar/TcpServerAndHttpServer/","content":"重写Sylar基于协程的服务器系列：\n 重写Sylar基于协程的服务器（0、搭建开发环境以及项目框架 || 下载编译简化版Sylar）\n 重写Sylar基于协程的服务器（1、日志模块的架构）\n重写Sylar基于协程的服务器（2、配置模块的设计）\n重写Sylar基于协程的服务器（3、协程模块的设计）\n重写Sylar基于协程的服务器（4、协程调度模块的设计）\n重写Sylar基于协程的服务器（5、IO协程调度模块的设计）\n重写Sylar基于协程的服务器（6、HOOK模块的设计）\n重写Sylar基于协程的服务器（7、TcpServer &amp; HttpServer的设计与实现）\nTcpServer模块架构图将基于线程的主从Reactor模型进行协程的定制化修改，如图所示。\n\nTcpServer实现TcpServer类是一个服务器通用类，TcpServer类的实现是Server端专门用来管理Tcp连接的，主要的成员函数及作用如下：\n\n构造函数，用户在构造一个TcpServer时会传三个类型都为IOManager的参数，参数名以功能命名，分别是：worker、io_worker、accept_worker。\n\nbind函数，因为一台服务器有可能有多个&lt;ip，端口&gt;对，所以用户在调用bind函数时，可能会传入多个地址对，bind函数就是负责为这些ip地址创建套接字，并且将ip地址和固定端口绑定，开始监听这些套接字。\n\nstart函数，创建accept协程，并将accept协程放到accept_worker协程调度器里面去，accept协程实际上就是回调函数是TcpServer::startAccept的协程。\n start函数的伪代码：\n \n\nstartAccept函数，是一个接受客户端连接的回调函数，内部是一个调用accept函数的死循环，在接受到一个socketfd后，将套接字封装成IO协程，并放入io_worker协程调度器中进行调度。IO协程就是回调函数是TcpServer::handleClient函数的协程。\n startAccept函数的伪代码：\n \n\n\n\n\n\nhandleClient函数，该函数是一个虚函数，是专门用来和客户请求对接的协程，在TcpServer中是一个简单打印连接信息的虚函数，想要实现一个实用的服务器（如HTTP服务器、FTP服务器等）只需要对handleClient函数做定制化重写即可。\n handleClient函数伪代码：\n \n\n\nHTTP服务器的实现HttpServer类继承TcpServer类并重写TcpServer::handleClient函数。HttpServer类重写了handleClient函数，内部实现为不断调用recvRequest函数的死循环，recvRequest函数底层会调用read函数读取并解析客户请求。在调用revcRequest函数后会将解析的请求提交给Servlet类处理请求，Servlet层会返回回复报文，然后利用sendResponse函数，将回复报文发送回客户端。sendResponse函数底层调用write函数。\nhandleClient函数重写如下：\n\n于此Sylar基础模块的设计的讲解完结。\n除了基于协程的服务器外，推荐读者也可以去看一下Muduo基于线程的网络库的设计。两者对比着学习才能有更好的理解。sylar相对于Muduo来说，性能确实没那么高，但是sylar设计的初衷是奔着框架去的，所以sylar基础设施做的特别好，比如有Muduo没有的配置模块、动态库模块、daemon模块等，sylar另一大优势是对网络编程友好，可以以同步的方式进程网络业务的编写，同时享受异步的性能。而深入阅读Muduo其实就可以发现Muduo网络库One Loop Per Thread的思想，和Nginx的One Loop Per Process有神奇的相似之处，这种模型，靠着每个线程都有自己的loop和任务队列，将需要跨线程执行的任务添加到各自的队列中去串行执行，加锁的临界区会很小，几乎不需要锁。所以Muduo性能很高。\n我原来写的的Muduo源码阅读笔记如下：\nMuduo源码笔记系列：\nmuduo源码阅读笔记（0、下载编译muduo）\nmuduo源码阅读笔记（1、同步日志）\nmuduo源码阅读笔记（2、对C语言原生的线程安全以及同步的API的封装）\nmuduo源码阅读笔记（3、线程和线程池的封装）\nmuduo源码阅读笔记（4、异步日志）\nmuduo源码阅读笔记（5、Channel和Poller）\nmuduo源码阅读笔记（6、ExevntLoop和Thread）\nmuduo源码阅读笔记（7、EventLoopThreadPool）\nmuduo源码阅读笔记（8、定时器TimerQueue）\nmuduo源码阅读笔记（9、TcpServer）\nmuduo源码阅读笔记（10、TcpConnection）\nmuduo源码阅读笔记（11、TcpClient）\n感兴趣的同学，可以阅读一下本文实现的源码：https://github.com/LunarStore/lunar\n\n本章完结\n","tags":["高性能服务器框架"]},{"title":"Angular快速入门","url":"/2024/08/25/web/start/","content":"本文主要讲解ANgular在Windows平台的环境搭建。\n安装Node.JS\n官网下载稳定版本的node.js：https://nodejs.org/en/\n\n安装对话框一路默认下一步，当然，可以按需修改安装路径。\n\n安装完成后会自动将Node.JS的根目录添加到环境变量，读者可以自行查询。\n\n在nodejs根目录,创建node_global，node_cache文件夹\n\n以管理人员身份运行CMD。输入如下命令：\n\n\n# 根据实际安装路径更改路径！npm config set prefix &quot;D:\\DevelopmentToolRoot\\WebFront\\nodejs\\node_global&quot;npm config set cache &quot;D:\\DevelopmentToolRoot\\WebFront\\nodejs\\node_cache&quot;\n\n\n将node_global、node_cache所在路径添加到path环境变量中\n\n全局安装最常用的 express 模块\n\n\nnpm install express -g\n\n\n配置淘宝镜像并安装cpm ：\n\nnpm config set registry https://registry.npmmirror.com# 1. 注意修改nodejs安装目录的访问权限，让所有用户都能读写执行。# 如果之前下载过nodejs，注意删除系统user目录下的package-lock.json文件# npm install cnpm@7.1.1 -gnpm install -g cnpm\n\n\n可以使用配置查看命令，查看配置是否成功：\n\nnpm config ls\n\n安装Angular\n首先卸载已经安装的版本：\n\n# 老版本npm uninstall -g angular-clinpm uninstall --save-dev angular-cli# 新版本npm uninstall -g @angular/cli# 清空缓存npm cache verify # npm cache clean --force\n\n\n开始安装，报权限不允许的错误就使用管理员的方式打开cmd！\n\n# cnpm install -g @angular/cli@8.2.2cnpm install -g @angular/cli@latest\n\n\n检查是否安装成功\n\nng help\n\n\n项目创建\n\n# 新建一个Angular项目，默认会执行np installng new projectName# 只创建不安装ng new projectName --skip-installcd projectName# npm install、yarn installcnpm install\n\n\n项目编译&amp;启动\n\nng serve --open\n\n\n在浏览器键入：http://localhost:4200/，可看到欢迎页面\n\n\n常见报错：\nangular 6中使用socket.io报错：global is not define在src&#x2F;polyfills.ts中添加\n(window as any).global = window;\n\n创建一个组件在项目根目录下使用如下命令：\nD:\\root\\WorkSpace\\AngularRoot\\Start&gt;ng g component component/newsCREATE src/app/component/news/news.component.html (19 bytes)CREATE src/app/component/news/news.component.spec.ts (614 bytes)CREATE src/app/component/news/news.component.ts (262 bytes)CREATE src/app/component/news/news.component.scss (0 bytes)UPDATE src/app/app.module.ts (398 bytes)\n\nvscode终端执行ng serve报错：\nng : 无法加载文件 C:\\Users\\root\\AppData\\Roaming\\npm\\ng.ps1，因为在此系统上禁止运行脚本。有关详细信息，请参阅 https:/go.microsoft.com/fwlink/?LinkID=135170 中的 about_Execution_Policies。所在位置 行:1 字符: 1+ ng serve+ ~~\n\n在搜索框搜索：Windows PowerShell ISE，执行如下命令并选A（全是）即可解决\nset-ExecutionPolicy RemoteSigned\n\nangular不建议使用dom操作来改变class\n使用Forms模块在src&#x2F;app&#x2F;app.module.ts文件夹中添加：\n// ...import &#123; FormsModule &#125; from &#x27;@angular/forms&#x27;;@NgModule(&#123;  imports: [    // ...    FormsModule  ],&#125;)\n\nhtml 如下：\n&lt;div&gt;    &lt;form class=&quot;form&quot; [formGroup]=&quot;infoForm&quot;&gt;        &lt;ng-container&gt;        &lt;div formArrayName=&quot;infos&quot;&gt;            &lt;div *ngFor=&quot;let info of infos.controls; let i = index&quot;&gt;            &lt;ng-container [formGroupName]=&quot;i&quot;&gt;                &lt;label&gt;&#123;&#123; info.get(&quot;name&quot;).value&#125;&#125;&lt;/label&gt;                &lt;input type=&quot;text&quot; formControlName=&quot;value&quot; [id]=&quot;i&quot;&gt;            &lt;/ng-container&gt;            &lt;/div&gt;        &lt;/div&gt;        &lt;button (click)=&quot;onSubmit()&quot;&gt;            保存        &lt;/button&gt;        &lt;/ng-container&gt;    &lt;/form&gt;&lt;/div&gt;\n\nts代码如下：\nexport class FormComponent implements OnInit &#123;  // fb:FormBuilder;  constructor(private fb:FormBuilder) &#123;     // this.fb = fb;  &#125;  infoForm = this.fb.group (&#123;    infos: this.fb.array([]),  &#125;);  get infos(): FormArray &#123;    return this.infoForm.get(&#x27;infos&#x27;) as FormArray;  &#125;  ngOnInit() &#123;    const fm = this.fb.group(&#123;      id: 1,      name: &quot;aaa&quot;,      value: &quot;aiaiai&quot;,    &#125;);    this.infos.push(fm);    const fm1 = this.fb.group(&#123;      id:2,      name:&quot;bbb&quot;,      value:&quot;bibibi&quot;,    &#125;);    this.infos.push(fm1);    const fm2 = this.fb.group(&#123;      id:3,      name:&quot;ccc&quot;,      value:&quot;cicici&quot;,    &#125;);    this.infos.push(fm2);    var playerInterval_dvp = setInterval(() =&gt; &#123;      console.log(&quot;aaa&quot;)    &#125;);  &#125;  onSubmit()&#123;    console.log(this.infos.at(1).get(&#x27;name&#x27;).value);    console.log((this.infos.at(1)))    console.log((this.infos.at(1).value))    console.log((this.infos.at(1).value as MyInfoInterface))    console.log((this.infos.at(1).value as MyInfoInterface).value)  &#125;&#125;export interface MyInfoInterface &#123;  id: number;  name: string;  value: string;&#125;\n\n点击按钮后，结果如下：\n\n创建一个服务ng g service service/storage\n\n在src&#x2F;app&#x2F;app.module.ts文件夹中添加：\nimport &#123; StorageService &#125; from &#x27;./service/storage.service&#x27;;@NgModule(&#123;  // ...  providers: [StorageService],  // ...&#125;)\n\n在组件文件中添加：\nimport &#123; StorageService &#125; from &#x27;../../service/storage.service&#x27;;export class SearchComponent implements OnInit &#123;  public keyword:string=&quot;电脑&quot;;  public history:any[]=[];  storage:StorageService;  constructor(storage:StorageService) &#123;    this.storage = storage;    var hi = this.storage.get(&quot;history&quot;);    if (hi) &#123;      this.history = hi;    &#125;  &#125;  ngOnInit() &#123;  &#125;&#125;\n\nDOM操作DOM操作最好在ngAfterViewInit函数中进行，ngOnInit中操作的话，可能一些dom还没有加载完成导致获取不到DOM\n在angular中，操作dom的方式有两种：使用原生js和使用ViewChild\n&lt;p #myfirstdom&gt;domopt works!&lt;/p&gt;\n\nimport &#123; Component, OnInit, ViewChild&#125; from &#x27;@angular/core&#x27;;@Component(&#123;  selector: &#x27;app-domopt&#x27;,  templateUrl: &#x27;./domopt.component.html&#x27;,  styleUrls: [&#x27;./domopt.component.scss&#x27;]&#125;)export class DomoptComponent implements OnInit &#123;  @ViewChild(&quot;myfirstdom&quot;, &#123;static:true&#125;) myfirstdom:any;  constructor() &#123; &#125;  ngOnInit() &#123;  &#125;  ngAfterViewInit()&#123;    console.log(this.myfirstdom);    this.myfirstdom.nativeElement.innerHTML = &quot;aaa&quot;;  &#125;&#125;\n\n父子组件传值方式一：数据、函数、父组件本身都能通过该方法传递。\n父组件中定义:\nexport class NewsComponent implements OnInit &#123;  public title:string = &quot;新闻&quot;;  @ViewChild(&quot;myheader&quot;, &#123;static:true&#125;) header:any;  constructor() &#123; &#125;  ngOnInit() &#123;  &#125;&#125;\n\n&lt;app-header #myheader [title]=&quot;title&quot;&gt;&lt;/app-header&gt;\n\n子组件中定义：\nimport &#123; Component, OnInit, Input&#125; from &#x27;@angular/core&#x27;;export class HeaderComponent implements OnInit &#123;  @Input() title:any;  constructor() &#123;  &#125;  ngOnInit() &#123;  &#125;&#125;\n\n方式二：子组件通过事件驱动的方式给父组件发消息。\n子组件配置：\nimport &#123; Component, OnInit, Input, Output, EventEmitter&#125; from &#x27;@angular/core&#x27;;export class HeaderComponent implements OnInit &#123;  @Output() outer = new EventEmitter();  constructor() &#123;    console.log(this.dt)  &#125;  ngOnInit() &#123;    this.mvvm=&quot;初始值&quot;  &#125;  sendParent()&#123;    console.log(&quot;header的sendParent方法&quot;);    this.outer.emit(&quot;header的sendParent&quot;);  &#125;&#125;\n\n父组件配置：\n&lt;app-header (outer)=&quot;run($event)&quot;&gt;&lt;/app-header&gt;\n\nimport &#123; Component, OnInit, ViewChild &#125; from &#x27;@angular/core&#x27;;export class NewsComponent implements OnInit &#123;  constructor() &#123; &#125;  ngOnInit() &#123;  &#125;  run(arg:any)&#123;    alert(&quot;新闻中的run方法，还可以直接把整个新闻组件传给header！！！&quot; + arg);  &#125;&#125;\n\n异步处理有两种：rxjs（observable）或者promise，主要介绍一下rxjs\n使用rxjs：import &#123;map,filter&#125; from &#x27;rxjs/operators&#x27;import &#123; RequestService &#125; from &#x27;../../service/request.service&#x27;export class HomeComponent implements OnInit &#123;  request:RequestService;  constructor(request:RequestService) &#123;  &#125;  ngOnInit() &#123;    var streamNum = this.request.getRxjsIntervalTimer();    streamNum.pipe(      filter(        (value)=&gt;&#123;          if (Number(value)%2 == 0) &#123;            return true;          &#125;          // console.log(Number(value)*Number(value));          return false;      &#125;),      map(        (val)=&gt;&#123;            return Number(val)*Number(val);        &#125;)    ).subscribe((value) =&gt; &#123;      console.log(value);    &#125;);  &#125;&#125;\n\nRequestServer：\nimport &#123; Observable &#125; from &#x27;rxjs&#x27;;export class RequestService &#123;  constructor() &#123; &#125;  getRxjsIntervalTimer()&#123;    var cnt = 0;    return new Observable(      (observe)=&gt;&#123;        setInterval(()=&gt;&#123;          cnt++;          // console.log(cnt);          observe.next(cnt);        &#125;, 1000);      &#125;    );  &#125;&#125;\n\n使用get、post请求app.module.ts:\nimport &#123; HttpClientModule &#125; from &#x27;@angular/common/http&#x27;@NgModule(&#123;  // ...  imports: [    // ...    HttpClientModule,  ]  // ...&#125;)\n\nhttp如下：\n&lt;button (click)=&quot;getRequest()&quot;&gt;get请求&lt;/button&gt;&lt;button (click)=&quot;postRequest()&quot;&gt;post请求&lt;/button&gt;&lt;button (click)=&quot;getJsonpData()&quot;&gt;jsonp请求&lt;/button&gt;\n\n请求逻辑如下：\nimport &#123; HttpClient, HttpHeaders &#125; from &#x27;@angular/common/http&#x27;export class RequesthttpComponent implements OnInit &#123;  http:HttpClient;  constructor(http:HttpClient) &#123;    this.http = http;  &#125;  ngOnInit() &#123;  &#125;  getRequest()&#123;    this.http.get(&quot;http://a.itying.com/api/productlist&quot;).subscribe((value)=&gt;&#123;      console.log(value);    &#125;);  &#125;  postRequest() &#123;    var httpOptions = &#123; headers: new HttpHeaders(&#123;&quot;Content-Type&quot;: &quot;application/json&quot;&#125;) &#125;;    this.http.post(&quot;www.baidu.com&quot;, &#123;&quot;username&quot;:&quot;zhangsan&quot;, &quot;password&quot;:&quot;123&quot;&#125;, httpOptions).subscribe((data)=&gt;&#123;        console.log(data);    &#125;);  &#125;  getJsonpData()&#123;    this.http.jsonp(&quot;http://a.itying.com/api/productlist&quot;, &quot;callback&quot;).subscribe((value)=&gt;&#123;      console.log(value);    &#125;);   &#125;&#125;\n\n路由在app-routing.module.ts可配置静态路由：\nimport &#123; NewsComponent &#125; from &#x27;./components/news/news.component&#x27;;import &#123; MovieComponent &#125; from &#x27;./components/movie/movie.component&#x27;;import &#123; GameComponent &#125; from &#x27;./components/game/game.component&#x27;;import &#123; ContentComponent &#125; from &#x27;./components/content/content.component&#x27;;const routes: Routes = [  &#123;    path:&quot;news&quot;,    component: NewsComponent,  &#125;,&#123;    path:&quot;movie&quot;,    component: MovieComponent,  &#125;,&#123;    path:&quot;game&quot;,    component: GameComponent,  &#125;,&#123;    path:&quot;content&quot;,    component: ContentComponent,  &#125;,&#123;    path:&quot;**&quot;,    component: NewsComponent,  &#125;,];\n\n加routerLinkActive可实现激活\n&lt;a [routerLink]=&quot;&#x27;/news&#x27;&quot; routerLinkActive=&quot;active&quot;&gt;新闻&lt;/a&gt;&lt;a [routerLink]=&quot;&#x27;/movie&#x27;&quot; routerLinkActive=&quot;active&quot;&gt;电影&lt;/a&gt;&lt;a [routerLink]=&quot;&#x27;/game&#x27;&quot; routerLinkActive=&quot;active&quot;&gt;游戏&lt;/a&gt;\n\n组件路由并传参：\nget路由：\n&#123;    path:&quot;content&quot;,    component: ContentComponent,  &#125;\n\n父：\n&lt;p&gt;news works!&lt;/p&gt;&lt;ul&gt;    &lt;li *ngFor=&quot;let item of list; let i=index;&quot;&gt;        &lt;a [routerLink]=&quot;&#x27;/content&#x27;&quot; [queryParams]=&quot;&#123;aid:i&#125;&quot;&gt;&#123;&#123;i&#125;&#125; === &#123;&#123;item&#125;&#125;&lt;/a&gt;    &lt;/li&gt;&lt;/ul&gt;\n\n子：\nimport &#123; ActivatedRoute &#125; from &#x27;@angular/router&#x27;;export class ContentComponent implements OnInit &#123;  router:ActivatedRoute;  constructor(router:ActivatedRoute) &#123;    this.router = router;  &#125;  ngOnInit() &#123;    this.router.queryParams.subscribe((value)=&gt;&#123;      console.log(value);    &#125;);  &#125;&#125;\n\n动态路由：\n&#123;    path:&quot;content/:aid&quot;,    component: ContentComponent,  &#125;\n\n父：\n&lt;ul&gt;    &lt;li *ngFor=&quot;let item of list; let i=index;&quot;&gt;        &lt;a [routerLink]=&quot;[&#x27;/content/&#x27;, i]&quot;&gt;&#123;&#123;i&#125;&#125; === &#123;&#123;item&#125;&#125;&lt;/a&gt;    &lt;/li&gt;&lt;/ul&gt;\n\n子：\nimport &#123; ActivatedRoute &#125; from &#x27;@angular/router&#x27;;export class ContentComponent implements OnInit &#123;  router:ActivatedRoute;  constructor(router:ActivatedRoute) &#123;    this.router = router;  &#125;  ngOnInit() &#123;    this.router.params.subscribe((value)=&gt;&#123;      console.log(value);    &#125;);  &#125;&#125;\n\njs 动态路由：\napp-routing.module.ts:\n&#123;    path:&quot;content/:aid&quot;,    component: ContentComponent,  &#125;\n\n父\nimport &#123; Router &#125; from &#x27;@angular/router&#x27;;export class NewsComponent implements OnInit &#123;  router:Router;  constructor(router:Router) &#123;    this.router = router;  &#125;  ngOnInit() &#123;    this.router.navigate([&quot;/content&quot;, &quot;123&quot;]);  &#125;&#125;\n\n子\nimport &#123; ActivatedRoute &#125; from &#x27;@angular/router&#x27;;export class ContentComponent implements OnInit &#123;  router:ActivatedRoute;  constructor(router:ActivatedRoute) &#123;    this.router = router;  &#125;  ngOnInit() &#123;    this.router.params.subscribe((value)=&gt;&#123;      console.log(value);    &#125;);  &#125;&#125;\n\njs get路由：\n&#123;    path:&quot;content&quot;,    component: ContentComponent,  &#125;\n\n父：\nimport &#123; Router, NavigationExtras &#125; from &#x27;@angular/router&#x27;;export class NewsComponent implements OnInit &#123;  router:Router;  constructor(router:Router) &#123;    this.router = router;  &#125;  ngOnInit() &#123;    let params:NavigationExtras = &#123;      queryParams:&#123;aid:&quot;123&quot;&#125;,      fragment:&quot;anchor&quot;    &#125;    this.router.navigate([&quot;/content&quot;], params);  &#125;&#125;\n\n子：\nimport &#123; ActivatedRoute, Router &#125; from &#x27;@angular/router&#x27;;export class ContentComponent implements OnInit &#123;  router:ActivatedRoute;  constructor(router:ActivatedRoute) &#123;    this.router = router;  &#125;  ngOnInit() &#123;    this.router.queryParams.subscribe((value)=&gt;&#123;      console.log(value);    &#125;);  &#125;&#125;\n\n路由嵌套：\n根：\n&lt;a [routerLink]=&quot;&#x27;/news&#x27;&quot; routerLinkActive=&quot;active&quot;&gt;新闻&lt;/a&gt;&lt;a [routerLink]=&quot;&#x27;/movie&#x27;&quot; routerLinkActive=&quot;active&quot;&gt;电影&lt;/a&gt;&lt;a [routerLink]=&quot;&#x27;/game&#x27;&quot; routerLinkActive=&quot;active&quot;&gt;游戏&lt;/a&gt;&lt;router-outlet&gt;&lt;/router-outlet&gt;\n\napp-routing.module.ts:\nimport &#123; NgModule &#125; from &#x27;@angular/core&#x27;;import &#123; Routes, RouterModule &#125; from &#x27;@angular/router&#x27;;import &#123; NewsComponent &#125; from &#x27;./components/news/news.component&#x27;;import &#123; MovieComponent &#125; from &#x27;./components/movie/movie.component&#x27;;import &#123; GameComponent &#125; from &#x27;./components/game/game.component&#x27;;import &#123; ContentComponent &#125; from &#x27;./components/content/content.component&#x27;;import &#123; SystemsetComponent &#125; from &#x27;./components/movie/systemset/systemset.component&#x27;;import &#123; SystemmaintainComponent &#125; from &#x27;./components/movie/systemmaintain/systemmaintain.component&#x27;;import &#123; GamesetComponent &#125; from &#x27;./components/game/gameset/gameset.component&#x27;;import &#123; GameupdateComponent &#125; from &#x27;./components/game/gameupdate/gameupdate.component&#x27;;const routes: Routes = [  &#123;    path:&quot;news&quot;,    component: NewsComponent  &#125;,&#123;    path:&quot;movie&quot;,    component: MovieComponent,    children:[      &#123;        path:&quot;systemset&quot;,        component:SystemsetComponent,      &#125;,&#123;        path:&quot;systemmaintain&quot;,        component:SystemmaintainComponent,      &#125;,&#123;        path:&quot;**&quot;,        component:SystemsetComponent,      &#125;    ],  &#125;,&#123;    path:&quot;game&quot;,    component: GameComponent,    children:[      &#123;        path:&quot;gameset&quot;,        component:GamesetComponent,      &#125;,&#123;        path:&quot;gameupdate&quot;,        component:GameupdateComponent,      &#125;,&#123;        path:&quot;**&quot;,        component:GamesetComponent,      &#125;    ],  // &#125;,&#123;  //   path:&quot;content&quot;,  //   component: ContentComponent,  // &#125;,&#123;  &#125;,&#123;    path:&quot;content&quot;,    component: ContentComponent,  &#125;,&#123;    path:&quot;**&quot;,    component: NewsComponent,  &#125;,];@NgModule(&#123;  imports: [RouterModule.forRoot(routes)],  exports: [RouterModule]&#125;)export class AppRoutingModule &#123; &#125;\n\ncomponent&#x2F;movie&#x2F;movie.component.html\n&lt;p&gt;movie works!&lt;/p&gt;&lt;div class=&quot;content&quot;&gt;    &lt;div class=&quot;left&quot;&gt;        &lt;a [routerLink]=&quot;&#x27;/movie/systemset&#x27;&quot; routerLinkActive=&quot;active&quot;&gt;系统设置&lt;/a&gt;        &lt;br&gt;        &lt;br&gt;        &lt;a [routerLink]=&quot;&#x27;/movie/systemmaintain&#x27;&quot; routerLinkActive=&quot;active&quot;&gt;系统维护&lt;/a&gt;    &lt;/div&gt;    &lt;div class=&quot;right&quot;&gt;        &lt;router-outlet&gt;&lt;/router-outlet&gt;    &lt;/div&gt;&lt;/div&gt; \n\ncomponent&#x2F;game&#x2F;game.component.html\n&lt;p&gt;game works!&lt;/p&gt;&lt;div class=&quot;content&quot;&gt;    &lt;div class=&quot;left&quot;&gt;        &lt;a [routerLink]=&quot;&#x27;/game/gameset&#x27;&quot; routerLinkActive=&quot;active&quot;&gt;游戏设置&lt;/a&gt;        &lt;br&gt;        &lt;br&gt;        &lt;a [routerLink]=&quot;&#x27;/game/gameupdate&#x27;&quot; routerLinkActive=&quot;active&quot;&gt;游戏更新&lt;/a&gt;    &lt;/div&gt;    &lt;div class=&quot;right&quot;&gt;        &lt;router-outlet&gt;&lt;/router-outlet&gt;    &lt;/div&gt;&lt;/div&gt; \n\n\n\nSubject和Observable的区别Observable是Subject的特例，Subject是典型的发布订阅模式，可以将消息传递给多个订阅者，但是Observable只能一对一。\n关于Renderer2的介绍Renderer2 是 Angular 提供的一个用于操作 DOM 的服务类。它允许你在不直接访问浏览器的 DOM API 的情况下，通过抽象层安全地操作 DOM。Renderer2 是为了增强跨平台兼容性和更好的安全性而设计的，使得你的应用可以在浏览器、服务器端渲染（如 Angular Universal）以及 Web Workers 等不同的环境中运行。\n关于dom操作constructor(  private Re2: Renderer2,  private el: ElementRef,  &#x2F;&#x2F; 其他依赖项…) { }\n直接访问宿主元素：\n\nElementRef 提供了对 Angular 组件、指令或 HTML 元素的宿主元素的直接引用。通过 el.nativeElement，你可以访问并操作该元素。例如，如果这个 ElementRef 是在一个组件中注入的，那么 el.nativeElement 指向该组件的根 DOM 元素。与 Renderer2 结合使用：\n\n在你提供的构造函数中，ElementRef 通常与 Renderer2 一起使用。Renderer2 提供了一组安全的方法来操作 DOM，而 ElementRef 提供了操作的目标元素。例如，在 ngOnInit 或其他方法中，你可能会通过 el.nativeElement 获取 DOM 元素，然后使用 Renderer2 对其应用样式、添加类或处理事件。\n\n\nData URL例如，一个图像的Data URL可能看起来像这样：\ndata:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAAAUAAAAFCAYAAACNbyblAAAAHElEQVQI12P4//8/w38GIAXDIBKE0DHxgljNBAAO9TXL0Y4OHwAAAABJRU5ErkJggg==\n\n在这个例子中：\n\nimage&#x2F;png 指定了MIME类型，表示这是一个PNG图像。\n\nbase64 表示数据是使用Base64编码的。\n\n后面的长字符串是图像数据的Base64编码表示。\n\n\n它允许将小文件直接嵌入到网页或网络请求中，而不需要从服务器上单独请求资源。Data URL 通常用于图像、字体、CSS、JavaScript 和其他小型文件，以减少HTTP请求的数量，从而提高页面加载速度。\n\n本章完结\n","tags":["Angular"]},{"title":"WorkFlow源码剖析——Communicator之TCPServer（上）","url":"/2024/11/02/workflow/TCPServer_1/","content":"WorkFlow源码剖析——GO-Task 源码分析\nWorkFlow源码剖析——Communicator之TCPServer（上）\nWorkFlow源码剖析——Communicator之TCPServer（中）\nWorkFlow源码剖析——Communicator之TCPServer（下）\n前言上一篇博客已经介绍了一下WorkFlow GO-Task的实现原理。本文会介绍一下WorkFlow Tcp Server端的一些实现细节以及有趣的思想。因为这部分涉及的内容有点多，一些有趣的细节也希望能完整的叙述出来，所以我可能会将TCPServer拆分成上中下三个部分。三个部分分别对应：poller的实现（即对IO对路复用事件池的封装，属于最底层），Communicator的实现（对连接对象生命周期的管理，属于中间层），最后就是TCPServer的实现（利用中间层实现一个TCPServer）。文件异步IO相关的内容，后面抽空会补上。\n\n\n这里声明一下：\n关于workflow的线程池，其实最终的目的就是实现一个线程池嘛，知道它是个什么东西，看一下它的接口，其实就差不多了。原理都是一样的，就是实现细节上各有各的特点。workflow的线程池简单描述它的特点就是一种链式的线程池。它所涉及的源码也不多就200多行，如果专门起一篇博客去讲解它，以我的表达能力和理解的深度，估计到时候代码的比例又会占大头。成了纯纯的贴代码的博客是我不愿意看到的。所以如果你对workflow线程池的实现感兴趣，建议你亲自去看看它的源码。我始终认为，如果你想了解一个项目的源码，博客写的再好，也只能作为辅助，它只能在你看源码有疑惑时给你一些指点，或者说作为你在读完源码后，回头巩固的一种手段。最终亲眼看看代码才是正道。\n关于消息队列，和线程池紧密相关的就是任务（&#x2F;消息）队列。我最大的收货就是：我们以往再写线程池任务队列时，读写都是一把锁，而workflow线程的任务队列却有两把锁，读和写分别持有一把，巧妙的降低了锁的竞争。具体细节，感兴趣的读者可以自行阅读源码，这里就不过多赘述。\nWorkflow其实没有TcpServer的概念，相关的其实就是一个CommService，而CommService其实可以理解为TCPServer，尽管CommService其实也是支持udp等其他非tcp的协议。但是我们重点只关注tcp相关的东西。所以，下文提到的TcpServer都代指CommService。\n本文会按照如下顺序讲解TcpServer的实现：\npoller相关的数据结构的介绍 -&gt; 事件循环的设计 -&gt; 对poller_node的增删改 -&gt; 对定时器的增删 -&gt; 对IO事件回调函数的流程分析。\n对IO以及IO多路复用的封装本节内容主要集中在poller.c文件，主要关注workflow对epoll事件池的设计以及linux原生IO相关api的最底层的封装。这里涉及大量的回调。不得不说workflow对回调理解的是真的透彻。\n数据结构的分析首先是数据结构__poller：\ntypedef struct __poller poller_t;   // 对外暴露的名称struct __poller&#123;\tsize_t max_open_files;                              // 该epollfd支持的最大fd\tvoid (*callback)(struct poller_result *, void *);   // 由Communicator设置，Communicator会根据poller_result.poller_data.operation的类型回调不同的函数。\tvoid *context;                                      // 传给callback的参数\tpthread_t tid;                                      // epollfd监听线程的线程id\tint pfd;                                            // epollfd\tint timerfd;\tint pipe_rd;                                        // 管道读端，通知回收某个/些__poller_node\tint pipe_wr;                                        // 管道写端\tint stopped;\tstruct rb_root timeo_tree;                          // 带超时的__poller_node红黑树，以下三个成员都和[定时器]红黑树相关\tstruct rb_node *tree_first;\tstruct rb_node *tree_last;\tstruct list_head timeo_list;                        // 带超时的__poller_node链表\tstruct list_head no_timeo_list;                     // 不带超时的__poller_node链表\tstruct __poller_node **nodes;                       // 可根据fd索引每一个__poller_node\tpthread_mutex_t mutex;\tchar buf[POLLER_BUFSIZE];                           // 共享buffer&#125;;\n\npoller的数据结构每项成员的定义非常恰到好处，我相信比较难理解的就是为啥为定时器搞了两种结构：链表和红黑树？后面在看到定时器的增删查改时再谈谈我的理解。\n这里可以先用定时器类型，简单的对__poller_node进行分类：\n\n带超时的__poller_node：带超时的__poller_node又有两种挂法：\n\n被挂到红黑树timeo_tree上。\n\n按递增顺序，被挂到超时链表timeo_list上。\n\n\n\n不带超时的__poller_node：被挂到no_timeo_list链表上。\n\n\n然后就是数据结构__poller_node：\n它的定义如下：\nstruct __poller_node&#123;\tint state;                  // __poller_node的状态\tint error;\tstruct poller_data data;    // 稍后解释#pragma pack(1)\tunion\t&#123;\t\tstruct list_head list;  // 如果是在链表上使用该成员作为节点\t\tstruct rb_node rb;      // 如果是在红黑树上使用该成员作为节点\t&#125;;#pragma pack()\tchar in_rbtree;             // 是在红黑树上吗？\tchar removed;               // 被从epoll上移除取消监听（链表/红黑树上也会移除该node）\tint event;                  // epoll监听的事件\tstruct timespec timeout;    // fd超时时间\tstruct __poller_node *res;  // 稍后解释，（PS，我也有点忘了）&#125;;\n\n如果你看过Muduo的源码，那么__poller + __poller_node可以看作Muduo当中的EPollPoller，而Muduo当中，定时器同样使用的是timerfd_，区别是Muduo当中直接使用了标准库当中的std::set对定时器进行排序，而workflow使用的是自己造的红黑树的轮子。并且workflow额外使用了一个链表来维护带超时的__poller_node。\n当然在__poller_node结构体当中，我还没有解释data成员是干啥的，这里既然扯到了Muduo，顺便提一嘴：data成员其实可以等价与Muduo当中的Channel。poller_data当中也是有各种来自（Communicator）上层注册的回调函数。\npoller_dataj结构体在workflow当中定义如下：\nstruct poller_data&#123;// poll关心的事件的定义。#define PD_OP_TIMER\t\t\t0#define PD_OP_READ\t\t\t1#define PD_OP_WRITE\t\t\t2#define PD_OP_LISTEN\t\t3#define PD_OP_CONNECT\t\t4#define PD_OP_RECVFROM\t\t5/* ... */#define PD_OP_EVENT\t\t\t9#define PD_OP_NOTIFY\t\t10\tshort operation;\tunsigned short iovcnt;\tint fd;\tSSL *ssl;\tunion\t&#123;\t\tpoller_message_t *(*create_message)(void *);    // 钩爪异步读上下文的回调\t\tint (*partial_written)(size_t, void *);\t\tvoid *(*accept)(const struct sockaddr *, socklen_t, int, void *);\t\tvoid *(*recvfrom)(const struct sockaddr *, socklen_t,\t\t\t\t\t\t  const void *, size_t, void *);\t\tvoid *(*event)(void *);\t\tvoid *(*notify)(void *, void *);\t&#125;;\tvoid *context;\tunion\t&#123;\t\tpoller_message_t *message;      // 异步读上下文，由create_message创建\t\tstruct iovec *write_iov;        // 异步写缓存\t\tvoid *result;\t&#125;;&#125;;\n\n|\t回调函数\t\t|\t作用\t||\t:-:\t\t\t\t|\t:-:\t\t||\tcreate_message\t|\t（fd的read事件）利用该回调可以构造一个msg对象，并且每次在fd可读时，将读到的数据交给该msg去处理（解析），解释完成后返回一个大于0的值，然后通poller的callback通知上层（Communicator）||\tpartial_written\t|\t（fd的write事件）在每次fd可写并且尽力写入一些数据后会调用该回调，以更新写超时时间\t|| accept\t\t\t|\t（fd的read事件）在listen fd接受一条连接并为连接创建一个读写sockfd后，会调用该回调，回调会为该sockfd创建一个CommServiceTarget对象\t||\trecvfrom\t\t|\t（fd的read事件）使用udp协议的读（可以先不管，主要研究tcp，先将workflow的tcp打通）\t||\tevent\t\t\t|\t（fd的read事件）为实现文件fd的异步读写而生，作者暂时还未能去详细了解，在后续有空了再补上，我们重点还是关注网络socket fd的读写事件和回调\t||\tnotify\t|\t同event\t|\n其次就是poller_result结构体，该结构体其实就是__poller_node最前面三个成员。在poller.c当中，poller_result一般是以指针的形式去使用，并且是将__poller_node类型的变量的地址作为其值，如果你C语言基础足够扎实，结合poller_result和__poller_node定义的成员去看，其实很容易的理解到：将__poller_node类型的变量的地址作为poller_result*变量的值的用法就是只使用__poller_node的前三个成员。\nstruct poller_result&#123;#define PR_ST_SUCCESS\t\t0#define PR_ST_FINISHED\t\t1#define PR_ST_ERROR\t\t\t2#define PR_ST_DELETED\t\t3#define PR_ST_MODIFIED\t\t4#define PR_ST_STOPPED\t\t5\tint state;\tint error;\tstruct poller_data data;\t/* In callback, spaces of six pointers are available from here. */&#125;;\n\n在了解了poller当中的数据结构之后，如果你之前有过阅读其他网络库&#x2F;框架相关源码，其实我们就可以猜测，workflow当中的poller一定会有针对epollfd&#x2F;超时链表&#x2F;红黑树进行增删查改的一些函数。\n这里吐槽一下workflow，我觉得muduo、sylar这些网络框架层次其实更加分明，epoll池是epoll池、io读写操作是io读写操作、定时器是定时器。而在workflow当中。这三部分完全混合在了一个文件当中去实现。导致我最开始看poller源码时，感觉很混乱。\n事件循环（Event Loop） — 主干凡是网络框架都会使用epoll&#x2F;poll等IO多路复用机制，不得不说IO多路复用机制也确实挺好用的。而使用了IO多路复用机制的程序肯定避不开事件循环。本小节会逐步讲解从__poller的创建 到 事件循环的启动。\n首先是__poller的创建，其实__poller和__mpoller的创建可以放一起去讲解，__mpoller就是一群__poller的集合，可以将__mpoller理解为__poller的manager，一个__poller在创建时会申请一个epollfd，而__mpoller向外部提供的接口是具有负载均衡的功能的，它会将读写socketfd分散的发给__poller，同样的__poller内部就是一群__poller_node的集合（socket fd），你也可以将__poller看作__poller_node的manager。下面来看看__mpoller是如何构造每一个__poller的。\nstatic int __mpoller_create(const struct poller_params *params,\t\t\t\t\t\t\tmpoller_t *mpoller) &#123;\tvoid **nodes_buf = (void **)calloc(params-&gt;max_open_files, sizeof (void *));\tunsigned int i;\tif (nodes_buf) &#123;\t\tfor (i = 0; i &lt; mpoller-&gt;nthreads; i++) &#123;\t\t\tmpoller-&gt;poller[i] = __poller_create(nodes_buf, params);\t\t\tif (!mpoller-&gt;poller[i])\t\t\t\tbreak;\t\t&#125;\t\tif (i == mpoller-&gt;nthreads) &#123;\t\t\tmpoller-&gt;nodes_buf = nodes_buf;\t\t\treturn 0;\t\t&#125;\t\t// ...\t&#125;\treturn -1;&#125;\n\n代码很简单，就是分配一个max_open_files大小的指针数组（nodes_buf），然后调用__poller_create函数，将nodes_buf作为参数构造__poller对象，__poller_create会根据传入的参数对__poller做一些初始化，而__poller的nodes成员被初始化为指向nodes_buf，前面介绍过，nodes成员可根据socket fd索引__poller_node。 那么问题就来了，多个__poller共用一个nodes_buf，这样不会有问题吗？？？\n（经过深入思考）我可以明确的告诉你，确实不会，而且这种用法还很奇妙。这是因为有 {nodes成员可根据socket fd索引__poller_node} 这句话保底，因为在linux当中，一个进程当中的描述符（fd）绝对不可能重复！所以即使你创建的多个__poller对象，多个__poller对象又共用一个nodes_buf，但是分配给它们的socket fd（__poller_node）绝对互不相同，又因为__poller_node是根据socket fd索引的，故真实的情况是：每个__poller会共用一个nodes_buf，但是各自使用nodes_buf的不同entry。\n然后就是__poller的启动，调用mpoller的mpoller_start函数可以启动所有的__poller，对于每个__poller的启动会调用poller_start函数，它的实现如下：\nint poller_start(poller_t *poller) &#123;\tpthread_t tid;\tint ret;\tpthread_mutex_lock(&amp;poller-&gt;mutex);\tif (__poller_open_pipe(poller) &gt;= 0) &#123;\t\tret = pthread_create(&amp;tid, NULL, __poller_thread_routine, poller);\t\tif (ret == 0) &#123;\t\t\tpoller-&gt;tid = tid;\t\t\tpoller-&gt;stopped = 0;\t\t&#125;\t\telse &#123;\t\t\terrno = ret;\t\t\tclose(poller-&gt;pipe_wr);\t\t\tclose(poller-&gt;pipe_rd);\t\t&#125;\t&#125;\tpthread_mutex_unlock(&amp;poller-&gt;mutex);\treturn -poller-&gt;stopped;&#125;\n\n主要就干了两件事：\n\n调用__poller_open_pipe函数创建pipfd，将pipfd读端注册到epollfd上，pipfd在__poller当中起到内存回收的作用。具体是什么玩法，后面详细解释。（PS：说实话第一次看到用pipfd做内存回收的用法感觉挺不可思议的。真得感叹一句真是艺高人胆大！）\n\n创建并启动事件循环线程。\n\n\n总结一下就是和Muduo很像的One loop per thread，注意这里的加粗，在看完workflow的__poller源码后，感觉和Muduo的还是有很大差别的，我只能说Muduo的One loop per thread的思想的纯净，Muduo的代码阅读起来也很舒服。而workflow的却不同，它可能夹杂着业务优化的考量，所以并不是那么纯粹的One loop per thread，所以我在源码阅读时比较坎坷。（也可能是我功力不够深厚，没能领悟其奥妙。）针对加锁方面尤为明显，两者每个loop线程都会有自己的互斥锁，Muduo仅在对任务队列操作的时候会加锁，而workflow加锁的范围会稍微大点，凡是对__poller内部数据成员的访问都会加锁。\n事件循环的历程函数是__poller_thread_routine，它的实现如下：\nstatic void *__poller_thread_routine(void *arg) &#123;\tpoller_t *poller = (poller_t *)arg;\t__poller_event_t events[POLLER_EVENTS_MAX];\tstruct __poller_node time_node;\tstruct __poller_node *node;\tint has_pipe_event;\tint nevents;\tint i;\twhile (1) &#123;\t\t// 将最早超时的节点的超时时间作为定时器的超时时间\t\t__poller_set_timer(poller);\t\t// 等待时间的到来\t\tnevents = __poller_wait(events, POLLER_EVENTS_MAX, poller);\t\t// 获取当前时间，小于该时间的节点视为超时。\t\tclock_gettime(CLOCK_MONOTONIC, &amp;time_node.timeout);\t\t// pipfd可读？\t\thas_pipe_event = 0;\t\tfor (i = 0; i &lt; nevents; i++) &#123;\t\t\t// 将private转换为__poller_node*\t\t\tnode = (struct __poller_node *)__poller_event_data(&amp;events[i]);\t\t\tif (node &lt;= (struct __poller_node *)1) &#123;\t// 是pipfd？\t\t\t\tif (node == (struct __poller_node *)1)\t\t\t\t\thas_pipe_event = 1;\t\t\t\tcontinue;\t\t\t&#125;\t\t\t// 根据__poller_node.data的operation调用对应的回调函数。\t\t\tswitch (node-&gt;data.operation) &#123;\t\t\tcase PD_OP_READ:\t\t\t\t__poller_handle_read(node, poller);\t\t\t\tbreak;\t\t\tcase PD_OP_WRITE:\t\t\t\t__poller_handle_write(node, poller);\t\t\t\tbreak;\t\t\tcase PD_OP_LISTEN:\t\t\t\t__poller_handle_listen(node, poller);\t\t\t\tbreak;\t\t\tcase PD_OP_CONNECT:\t\t\t\t__poller_handle_connect(node, poller);\t\t\t\tbreak;\t\t\t/* 省略SSL、UDP相关部分... */\t\t\tcase PD_OP_EVENT:\t\t\t\t__poller_handle_event(node, poller);\t\t\t\tbreak;\t\t\tcase PD_OP_NOTIFY:\t\t\t\t__poller_handle_notify(node, poller);\t\t\t\tbreak;\t\t\t&#125;\t\t&#125;\t\tif (has_pipe_event) &#123;\t\t\t// 内存回收\t\t\tif (__poller_handle_pipe(poller))\t\t\t\tbreak;\t\t&#125;\t\t// 处理超时的__poller_node\t\t__poller_handle_timeout(&amp;time_node, poller);\t&#125;\treturn NULL;&#125;\n\n流程事件循环的流程图如下所示：\n\n最后改造了一下之前画的Muduo的one loop per thread的架构图。放在下面，读者可以好好品味一下。\n\n你可以参考一下Muduo的one loop per thread的架构图，以及xv6最后优化其调度器模型的架构图，再来看workflow的__poller的架构图，你一定会吃惊它们是如此的相似！\n对poller_node的增删改poller对外暴露的对对poller_node的增删改的接口包括：poller_add、poller_del、poller_mod。\n首先：\n这些接口在sylar、muduo中其实也有类似的。从函数的名称其实就能推测出它们分别代表：\npoller_add -&gt; 向poller中添加poller_node;poller_del -&gt; 删除poller_node;poller_mod -&gt; 修改poller_node;\n\n进一步的：\n\npoller_add本质上会将poller_node.data.fd连同关心的读&#x2F;写事件添加到epoll当中。如果poller_node.timeout被用户设置过，还会将poller_node挂到超时链表上或者是红黑树上。适当的时候更新timefd的设置。\n\npoller_del本质会将poller_node.data.fd连同关心的读&#x2F;写事件从epoll当中删除。\n\npoller_mod本质上会修改被挂到epoll当中的poller_node.data.fd所关心的读&#x2F;写事件。当然此时如果用户重新设置了poller_node的超时时间，则应该调整poller_node在超时链表&#x2F;红黑树的位置。适当的时候更新timefd的设置。\n\n\n实际上的：\npoller实际源码也正是这样做的。这里有一些比较意思的实践，这里一一列举一下：\n首先是根据poller_node的超时时间，将poller_node挂到超时链表&#x2F;红黑树上的操作。这个操作在poller_add、poller_mod都有需求。所以在poller当中是实现一个专门的函数__poller_insert_node，该函数功能就是将poller_node插入到超时链表&#x2F;红黑树适当位置，并且在poller_node是最早超时的节点时，更新timefd的设置。\npoller_node源码如下：\nstatic void __poller_insert_node(struct __poller_node *node,\t\t\t\t\t\t\t\t poller_t *poller) &#123;\tstruct __poller_node *end;\tend = list_entry(poller-&gt;timeo_list.prev, struct __poller_node, list);\tif (list_empty(&amp;poller-&gt;timeo_list)) &#123;\t\tlist_add(&amp;node-&gt;list, &amp;poller-&gt;timeo_list);\t\tend = rb_entry(poller-&gt;tree_first, struct __poller_node, rb);\t&#125; else if (__timeout_cmp(node, end) &gt;= 0) &#123;\t\tlist_add_tail(&amp;node-&gt;list, &amp;poller-&gt;timeo_list);\t\treturn;\t&#125; else &#123;\t\t__poller_tree_insert(node, poller);\t\tif (&amp;node-&gt;rb != poller-&gt;tree_first)\t\t\treturn;\t\tend = list_entry(poller-&gt;timeo_list.next, struct __poller_node, list);\t&#125;\tif (!poller-&gt;tree_first || __timeout_cmp(node, end) &lt; 0)\t\t__poller_set_timerfd(poller-&gt;timerfd, &amp;node-&gt;timeout, poller);&#125;\n\n第一次看这段代码可能会感觉比较绕，下面我来梳理它的逻辑：\n其实主要是三个分支，假设现在要添加名为node的poller_node：\n\n先获取（按升序排列的）超时链表上的最后一个元素。\n\n如果超时链表为空，直接将node加到超时链表尾部，然后获取红黑树第一个poller_node，如果红黑树为空或者说node比红黑树的第一个poller_node还小。那么将timefd的超时时间设置为node的超时时间。\n\n如果node比超时链表最后一个poller_node超时时间还大。那直接将node添加到超时链表末尾即可。timefd不用改。\n\n如果node超时时间小于超时链表最后一个元素超时时间，就将node添加到红黑树上。此时要是node正好作为红黑树的第一个元素（红黑树当中最小的节点），就比较一下node超时时间和超时链表第一个元素的超时时间。如果node比链表第一个元素超时时间小的话，更新timefd为node的超时时间。\n\n\n仔细梳理一下该函数的逻辑其实也是很清晰的。说白了就是将node添加到链表&#x2F;红黑树当中，同时当node是作为最小超时时间时，更新一下timefd的超时时间。这里使用两种数据结构的原因是：\n\n利用升序链表追加比红黑树快的优点；\n\n利用红黑树随机插入比升序链表快的优点；\n\n\n其次就是poller当中内存管理的做法：\n对于poller_add、poller_mod其实会调用__poller_new_node（间接调用malloc）分配一个新的地址作为poller_node。\n对于poller_del，会将poller_node从超时链表&#x2F;红黑树上移除。同时将poller_node的地址作为数据，写到pipe当中。等待pipe读时间处理函数调用poller-&gt;callback去回收。这种将地址写到管道然后异步回收的做法在我最开始看到的时候是为它捏了一把汗的。像我这种普通人肯定写不出这样的代码的。\n这里可以欣赏一下poller_del的代码\nint poller_del(int fd, poller_t *poller) &#123;\tstruct __poller_node *node;\tint stopped = 0;\tpthread_mutex_lock(&amp;poller-&gt;mutex);\tnode = poller-&gt;nodes[fd];\tif (node) &#123;\t\tpoller-&gt;nodes[fd] = NULL;\t\tif (node-&gt;in_rbtree)\t\t\t__poller_tree_erase(node, poller);\t\telse\t\t\t// 要么是被从超时链表上移除，要么是被从非超时链表上删除。统一都可以使用list_del来删除。\t\t\tlist_del(&amp;node-&gt;list);\t\t__poller_del_fd(fd, node-&gt;event, poller);\t\tnode-&gt;error = 0;\t\tnode-&gt;state = PR_ST_DELETED;\t\tstopped = poller-&gt;stopped;\t\tif (!stopped) &#123;\t\t\tnode-&gt;removed = 1;\t\t\t// 将地址写到pipe当中，异步让pipe读事件去回收\t\t\twrite(poller-&gt;pipe_wr, &amp;node, sizeof (void *));\t\t&#125;\t&#125;\telse\t\terrno = ENOENT;\tpthread_mutex_unlock(&amp;poller-&gt;mutex);\tif (stopped) &#123;\t\t// 事件处理线程停止了，就主动去释放了。其实pipe读事件处理函数也是做下面这两步操作\t\tfree(node-&gt;res);\t\tpoller-&gt;callback((struct poller_result *)node, poller-&gt;context);\t&#125;\treturn -!node;&#125;\n\n定时器的增删poller对外暴露的定时器操作的接口包括：poller_add_timer、poller_del_timer，简单来说就是对定时器进行添加和删除。\n这里可以需要明确一点，超时事件分为两类：定时器超时 和 读写事件的超时。\n\n定时器超时是指：用户需要指定一个回调函数在延迟n ms后才执行。使用场景比如：1分钟向文件写入一条日志，日志内容是系统当前运行状态。\n\n读写事件超时是指：在规定的超时时间内必需进行一次io，否则将操作视为超时，进行对应的超时处理。使用场景比如：HTTP的保活（keepalived）。\n\n\n这两者区别是：定时器超时是一定会发生的，而对于读写事件超时，只要规定时间内有io事件发生，则超时不一定会发生。\npoller当中定时器其实就是fd为（无效）-1的__poller_node，它不会被添加到epoll当中，但是会根据超时时间被挂到超时链表&#x2F;红黑树上。超时后的处理和IO事件超时的处理一模一样。\n这里贴出部分poller_add_timer代码：\nint poller_add_timer(const struct timespec *value, void *context, void **timer,\t\t\t\t\t poller_t *poller) &#123;\tstruct __poller_node *node;\tnode = (struct __poller_node *)malloc(sizeof (struct __poller_node));\tif (node) &#123;\t\tnode-&gt;data.operation = PD_OP_TIMER;\t\tnode-&gt;data.fd = -1;\t\t\t\t\t// 标记为无效\t\tnode-&gt;data.context = context;\t\t/* ... */\t\tclock_gettime(CLOCK_MONOTONIC, &amp;node-&gt;timeout);\t\tnode-&gt;timeout.tv_sec += value-&gt;tv_sec;\t\tnode-&gt;timeout.tv_nsec += value-&gt;tv_nsec;\t\tif (node-&gt;timeout.tv_nsec &gt;= 1000000000) &#123;\t\t\tnode-&gt;timeout.tv_nsec -= 1000000000;\t\t\tnode-&gt;timeout.tv_sec++;\t\t&#125;\t\t*timer = node;\t\tpthread_mutex_lock(&amp;poller-&gt;mutex);\t\t__poller_insert_node(node, poller);\t// 将poller_node添加到超时链表/红黑树上。\t\tpthread_mutex_unlock(&amp;poller-&gt;mutex);\t\treturn 0;\t&#125;\treturn -1;&#125;\n\n__poller_insert_node函数代码上小结已贴出。poller_del_timer函数实现可以参考poller_del，感兴趣的读者可以直接翻看源码，这里不过多赘述。\n事件处理函数下面逐个分析poller当中IO事件的处理函数细节。本文是以workflow的TCPServer为主，这里默认阅读本文的读者是有网络编程的经验的，那么你一定知道一个TCP的服务端最基本的框架如下：\n+-----------+|\tsocket\t|+-----------+\t|\tV+-----------+|\tbind\t|\t最开始三步由Communicator完成，下一篇博客会细讲。+-----------+\t|\tV+-----------+|\tlisten\t|+-----------+\t|\t\t\t______________________________________________\tV+-----------+|\taccept\t|\t从这里开始涉及到的所以函数就是接下来讲解的重点。+-----------+|\t|\t|\t|V\tV\tV\tVfd\tfd\tfd\t...\t/\\read  write\n\n本节就以上图所示顺序逐一介绍各个处理函数实现细节。\n&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;\n首先是accept事件处理函数————__poller_handle_listen，代码如下：\nstatic void __poller_handle_listen(struct __poller_node *node,\t\t\t\t\t\t\t\t   poller_t *poller) &#123;\tstruct __poller_node *res = node-&gt;res;\t/* ... */\twhile (1) &#123;\t\taddrlen = sizeof (struct sockaddr_storage);\t\tsockfd = accept(node-&gt;data.fd, addr, &amp;addrlen);\t\tif (sockfd &lt; 0) &#123;\t\t\tif (errno == EAGAIN || errno == EMFILE || errno == ENFILE)\t\t\t\treturn;\t\t\telse if (errno == ECONNABORTED)\t\t\t\tcontinue;\t\t\telse\t\t\t\tbreak;\t\t&#125;\t\tresult = node-&gt;data.accept(addr, addrlen, sockfd, node-&gt;data.context);\t\tif (!result)\t\t\tbreak;\t\tres-&gt;data = node-&gt;data;\t\tres-&gt;data.result = result;\t\tres-&gt;error = 0;\t\tres-&gt;state = PR_ST_SUCCESS;\t\tpoller-&gt;callback((struct poller_result *)res, poller-&gt;context);\t\tres = (struct __poller_node *)malloc(sizeof (struct __poller_node));\t\tnode-&gt;res = res;\t\tif (!res)\t\t\tbreak;\t&#125;\tif (__poller_remove_node(node, poller))\t// 从epoll当中移除fd\t\treturn;\tnode-&gt;error = errno;\tnode-&gt;state = PR_ST_ERROR;\tfree(node-&gt;res);\tpoller-&gt;callback((struct poller_result *)node, poller-&gt;context);\t// 一般是server在停止或出错才会走到这里。\t&#125;\n\n逻辑是：不断在一个while循环当中执行如下过程：\n\n（非阻塞的）调用accept系统调用接受客户端的连接。EAGAIN、EMFILE、ENFILE、ECONNABORTED分别代表没有可用连接、文件描述符达到上限、系统无句柄可用、连接终止（客户端的原因）。这些错误都是网络编程当中很常规的错误。都是可以容忍的。所以该return的return，该continue就continue。\n\n当接收到一条合法的io连接，就将其scoketfd作为参数回调node-&gt;data.accept，它会回调 Communicator::accept函数，该函数会为连接构造一个CommServiceTarget对象作为返回值，这里并没有将sockfd设置为非阻塞。这里读者可以先不必探究CommServiceTarget是干什么的，在下一章讲解Communicator自然会明了。因为我一开始也不知道CommServiceTarget的作用。可以从它的命名先猜测它其实就是IO连接在服务端的一个对象实体吧。\n\n回调poller-&gt;callback，其实在看完后面的read、write等io事件的处理函数之后，就会发现最早都会调用poller-&gt;callback，该回调也是在下章讲解Communicator后才发挥重要作用，读者先把它看作黑盒即可。可以预料到它里面一定会将接受的sockfd设置为非阻塞，并且最终会将它包装成poller_node加入到poller当中。\n\n\n&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;\n然后是read事件处理函数————__poller_handle_read，（去除了和SSL相关的代码，剩余的）代码如下：\nstatic int __poller_append_message(const void *buf, size_t *n,\t\t\t\t\t\t\t\t   struct __poller_node *node,\t\t\t\t\t\t\t\t   poller_t *poller) &#123;\tpoller_message_t *msg = node-&gt;data.message;\tstruct __poller_node *res;\tint ret;\tif (!msg) &#123;\t// 为连接的每个请求包分配一个msg\t\tres = (struct __poller_node *)malloc(sizeof (struct __poller_node));\t\tif (!res)\t\t\treturn -1;\t\tmsg = node-&gt;data.create_message(node-&gt;data.context);\t\tif (!msg) &#123;\t\t\tfree(res);\t\t\treturn -1;\t\t&#125;\t\tnode-&gt;data.message = msg;\t\tnode-&gt;res = res;\t&#125;\telse\t\tres = node-&gt;res;\tret = msg-&gt;append(buf, n, msg);\t// 通过msg解析请求包（PS，注意同一个请求可能会被拆分成多个tcp报文发送，所以函数可能会被多次回调）\tif (ret &gt; 0) &#123;\t\tres-&gt;data = node-&gt;data;\t\tres-&gt;error = 0;\t\tres-&gt;state = PR_ST_SUCCESS;\t\t// 当msg收到完整的请求包且解析完后同样回调一下poller-&gt;callback\t\tpoller-&gt;callback((struct poller_result *)res, poller-&gt;context);\t\tnode-&gt;data.message = NULL;\t// 清空，为下一个请求做准备\t\tnode-&gt;res = NULL;\t&#125;\treturn ret;&#125;static void __poller_handle_read(struct __poller_node *node,\t\t\t\t\t\t\t\t poller_t *poller) &#123;\tssize_t nleft;\tsize_t n;\tchar *p;\twhile (1) &#123;\t\tp = poller-&gt;buf;\t\tnleft = read(node-&gt;data.fd, p, POLLER_BUFSIZE);\t\tif (nleft &lt; 0) &#123;\t\t\tif (errno == EAGAIN)\t\t\t\treturn;\t\t&#125;\t\tif (nleft &lt;= 0)\t\t\tbreak;\t\tdo &#123;\t\t\tn = nleft;\t\t\tif (__poller_append_message(p, &amp;n, node, poller) &gt;= 0) &#123;\t\t\t\tnleft -= n;\t\t\t\tp += n;\t\t\t&#125;\t\t\telse\t\t\t\tnleft = -1;\t\t&#125; while (nleft &gt; 0);\t\tif (nleft &lt; 0)\t\t\tbreak;\t&#125;\t// 连接断开或则有错误发生\tif (__poller_remove_node(node, poller))\t\treturn;\tif (nleft == 0) &#123;\t\tnode-&gt;error = 0;\t\tnode-&gt;state = PR_ST_FINISHED;\t&#125; else &#123;\t\tnode-&gt;error = errno;\t\tnode-&gt;state = PR_ST_ERROR;\t&#125;\tfree(node-&gt;res);\tpoller-&gt;callback((struct poller_result *)node, poller-&gt;context);&#125;\n\n因为是异步read，所以__poller_handle_read回调多次才能读完一个完整的客户端请求报文。它的逻辑。主要就是两步，（非阻塞的方式）调用原始read系统调用，然后调用__poller_append_message函数去逐步解析收到的数据。当连接断开或则出现错误就跳出循环，回调poller-&gt;callback处理错误。这里我们其实可以大胆猜测__poller_append_message函数当中msg-&gt;append函数肯定是一个（HTTP、DNS等）报文解析器。\n这里有一个比较有意思的点是read系统调用所使用的buffer是poller-&gt;buf，它是poller的数据成员，这里所有的fd在read的时候使用同一个buffer，难道不会出问题吗？read的数据不会乱吗？\n这里简单分析一下，首先考虑线程安全问题，因为前面说过：一个poller对应一个线程。所以在线程处理IO事件的时候，一定是串行的，这就排除并行导致的线程安全问题。\n其次，再考虑异步非阻塞的方式读数据，东读一下西读一下会不会存在数据乱掉的问题，如果你仔细分析过__poller_handle_read其实就不会出现这个疑问，因为__poller_handle_read在通过poller-&gt;buf接受到数据后，不管这个请求是否完整，都会在__poller_append_message函数里面的msg-&gt;append进行异步解析。解析完毕后，poller-&gt;buf上的数据也就失去了它的意义。所以在处理下一个read事件的时候，放心的随他使用poller-&gt;buf吧。\n&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;\n最后write事件处理函数————__poller_handle_write，代码如下：\nstatic void __poller_handle_write(struct __poller_node *node,\t\t\t\t\t\t\t\t  poller_t *poller)&#123;\tstruct iovec *iov = node-&gt;data.write_iov;\tsize_t count = 0;\tssize_t nleft;\tint iovcnt;\tint ret;\twhile (node-&gt;data.iovcnt &gt; 0) &#123;\t\tiovcnt = node-&gt;data.iovcnt;\t\tif (iovcnt &gt; IOV_MAX)\t\t\tiovcnt = IOV_MAX;\t\tnleft = writev(node-&gt;data.fd, iov, iovcnt);\t\tif (nleft &lt; 0) &#123;\t\t\tret = errno == EAGAIN ? 0 : -1;\t\t\tbreak;\t\t&#125;\t\tcount += nleft;\t\tdo &#123;\t\t\t\t\t\t\t// 修正iov\t\t\tif (nleft &gt;= iov-&gt;iov_len) &#123;\t\t\t\tnleft -= iov-&gt;iov_len;\t\t\t\tiov-&gt;iov_base = (char *)iov-&gt;iov_base + iov-&gt;iov_len;\t\t\t\tiov-&gt;iov_len = 0;\t\t\t\tiov++;\t\t\t\tnode-&gt;data.iovcnt--;\t\t\t&#125; else &#123;\t\t\t\tiov-&gt;iov_base = (char *)iov-&gt;iov_base + nleft;\t\t\t\tiov-&gt;iov_len -= nleft;\t\t\t\tbreak;\t\t\t&#125;\t\t&#125; while (node-&gt;data.iovcnt &gt; 0);\t&#125;\tnode-&gt;data.write_iov = iov;\tif (node-&gt;data.iovcnt &gt; 0 &amp;&amp; ret &gt;= 0) &#123;\t\tif (count == 0)\t\t\treturn;\t\tif (node-&gt;data.partial_written(count, node-&gt;data.context) &gt;= 0)\t// 通知一下已发送的数据量\t\t\treturn;\t&#125;\tif (__poller_remove_node(node, poller))\t\treturn;\tif (node-&gt;data.iovcnt == 0) &#123;\t// 为零结束\t\tnode-&gt;error = 0;\t\tnode-&gt;state = PR_ST_FINISHED;\t\t&#125; else &#123;\t\t\t\t\t\t// 否则出错\t\tnode-&gt;error = errno;\t\tnode-&gt;state = PR_ST_ERROR;\t\t\t&#125;\tpoller-&gt;callback((struct poller_result *)node, poller-&gt;context);&#125;\n\n同样，因为是异步写数据，一个客户请求的回复报文可能分多次write才能发送完毕。poller向socketfd写数据采用的是writev接口，writev接口可以很方便的将内存不连续的数据发送到网络上。这在需要向网络写入大量的数据时很方便。因为数据量大所以系统无法一次性分配那么大块内存，所以数据可能被放在内存不连续的离散的buffer当中。writev使用元素类型为struct iovec的数组作为数据缓存，数组当中每个元素存放指向一块缓存的首地址的指针和长度。（非阻塞的）writev会返回实际发送的字节数。从代码中可以看到while循环嵌套了一个do while，因为tcp发送缓存区是有限的，do while作用是根据实际发送的字节数抹除已经发生的buffer，重新计算iov数组等待下一轮while循环继续发送。\n当发送缓存满，会因为EAGAIN跳出while循环，同时因为还存在没有发送完毕的数据，会调用node-&gt;data.partial_written。当数发送完毕，会将poller_node从epoll当中移除（对写事件的监听），最后回调poller-&gt;callback。\n\n本章完结\n","tags":["高性能服务器框架"]},{"title":"WorkFlow源码剖析——Communicator之TCPServer（中）","url":"/2024/11/04/workflow/TCPServer_2/","content":"WorkFlow源码剖析——GO-Task 源码分析\nWorkFlow源码剖析——Communicator之TCPServer（上）\nWorkFlow源码剖析——Communicator之TCPServer（中）\nWorkFlow源码剖析——Communicator之TCPServer（下）\n前言上节博客已经详细介绍了workflow的poller的实现，这节我们来看看Communicator是如何利用poller的，对连接对象生命周期的管理。（PS：与其说Communicator利用的是poller，其实Communicator使用的是mpoller，上节在介绍poller时也提到过mpoller，现场帮读者回忆一下：mpoller是poller的manager类，管理多个poller事件池，对外提供的接口负责将各种poller_node负载均衡的分散给不同的poller。）\n上节在介绍poller时，出现了各种回调，比如poller-&gt;callback()、node-&gt;data.accept()、node-&gt;data.partial_written()、node-&gt;data.create_message()等，当时我们总是一笔带过，没有去深入分析这些回调会做什么？并且每次在IO事件结束都会回调poller-&gt;callback()为什么要这样做？在poller当中，只看到了针对poller_node的malloc函数，而没有看到对应的free函数，哪里调用了free函数去释放poller_node？\n\n\n别着急，本节的源码分析会逐步揭晓这些疑问。\n同样的，注意里还是放在TCPServer上，对于SSL和UDP相关的内容直接忽略。先把TCP给模清楚。\n管理连接对象的实现连接上下文数据结构的分析既然谈到对连接对象的管理，那Communicator必然有一个数据结构来表示一个连接上下文对象，它就是CommConnEntry，代码如下：\nstruct CommConnEntry&#123;\tstruct list_head list;          // 链表节点，\tCommConnection *conn;           // 下一章介绍TCPServer的时候会用到，本文可忽略。\tlong long seq;                  // seq的作用其实可以理解为：一条连接 请求-回复 的轮次。一条连接的服务端和客户端理论上讲seq值是同时递增的，并且一定是保持相同的。\tint sockfd;                     // 连接的句柄#define CONN_STATE_CONNECTING\t0#define CONN_STATE_CONNECTED\t1#define CONN_STATE_RECEIVING\t2#define CONN_STATE_SUCCESS\t\t3#define CONN_STATE_IDLE\t\t\t4#define CONN_STATE_KEEPALIVE\t5#define CONN_STATE_CLOSING\t\t6#define CONN_STATE_ERROR\t\t7\tint state;                      // 连接状态\tint error;\tint ref;                        // 对对象的引用计数\tstruct iovec *write_iov;        // 异步写缓存\tCommSession *session;           // 含义同go-task，每次读写完毕或则出错了都会调用该对象的hanle函数\tCommTarget *target;             // 连接目的地。对于客户端，该成员是服务器的地址；对于服务端，该成员是客户端的地址。\tCommService *service;           // 该成员仅服务端有意义\tmpoller_t *mpoller;\t/* Connection entry&#x27;s mutex is for client session only. */\tpthread_mutex_t mutex;&#125;;\n\nworkflow将客户端、服务端、tcp、udp、ssl的实现都混杂在一个文件当中。在第一次阅读它的源码时有点双眼摸瞎的感觉。如果你有足够丰富的网络编程的经验可能还好。\n需要注意一点的是：该连接上下文在客户端和服务端所使用成员可能是不同的，客户端不会使用service成员，服务端不会使用seq成员。\n对于CommConnEntry::list成员，其实有两种用途：\n\n一是被挂在服务端的CommService::alive_list上。可以理解为服务端的http保活池。\n\n二是被挂在客户端的CommTarget::idle_list上。可以理解为客户端的（对同一个ipaddr:port的）http连接池。\n\n\n广义上讲，服务端的CommServiceTarget::idle_list也是http连接池。只是服务端的idle_list上只可能会有一个连接。\n然后可以预见的是：\n\n随着tcp连接状态的变化，state成员所记录的状态也会随之更新。\n\n当ref成员减为零，CommConnEntry对象将会被free掉。\n\n\n根据以往的经验能大胆猜测到的就是这些信息。\n状态迁移池状态迁移池——没错，类似于事件池，状态迁移池也有一个循环，它的任务是不断根据IO的结果，转换连接上下文的状态，并且根据IO的结果去回调必要的处理函数，最为代表的是：session-&gt;handle，session的概念在go-task源码剖析一节中也是存在。它存在的意义在下一章讲解workflow对TCPServer的时候才适合透露。我们重点集中在communicator如何管理连接上下文的状态的。\n这里其实就引入了一个问题，连接上下文为什么存在状态的迁移？别急，让我一步步道来。\n&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;\n首先是状态池的启动————Communicator::init\n代码如下：\nint Communicator::init(size_t poller_threads, size_t handler_threads) &#123;    /* ... */\tif (this-&gt;create_poller(poller_threads) &gt;= 0)\t&#123;\t\tif (this-&gt;create_handler_threads(handler_threads) &gt;= 0)\t\t&#123;\t\t\tthis-&gt;stop_flag = 0;\t\t\treturn 0;\t\t&#125;\t\tmpoller_stop(this-&gt;mpoller);\t\tmpoller_destroy(this-&gt;mpoller);\t\tmsgqueue_destroy(this-&gt;msgqueue);\t&#125;\treturn -1;&#125;\n\n涉及的代码过多，这里仅挑重点。\n\nCommunicator::init首先会启动mpoller，也就是上章我们所讲的事件池。上节的poller-&gt;callback函数以及它的参数poller-&gt;context，在poller初始化时就是由struct poller_params提供。而该结构体的够着在Communicator::create_poller当中是这样被赋值的：\n void Communicator::callback(struct poller_result *res, void *context) &#123;    msgqueue_t *msgqueue = (msgqueue_t *)context;    msgqueue_put(res, msgqueue);&#125;int Communicator::create_poller(size_t poller_threads) &#123;    struct poller_params params = &#123;        .max_open_files\t\t=\t(size_t)sysconf(_SC_OPEN_MAX),        .callback\t\t\t=\tCommunicator::callback,    &#125;;    this-&gt;msgqueue = msgqueue_create(16 * 1024, sizeof (struct poller_result));    if (this-&gt;msgqueue) &#123;        params.context = this-&gt;msgqueue;        /* ... */    &#125;    return -1;&#125;\n\n 所以，可以看到，上章的poller-&gt;callback回调，会将传进来的poller_result追加到Communicator的状态迁移池的队列当中。\n\nCommunicator::init然后会启动状态迁移池。状态迁移池使用的就是workflow自己造了链式线程池轮子。特别的是，在线程池的每个线程都运行一个routine：Communicator::handler_thread_routine，该函数是一个死循环。在每个线程都分配到一个Communicator::handler_thread_routine后，线程池的队列其实就失去了它的意义。每个Communicator::handler_thread_routine会使用1当中分配的队列。\n\n\n转到Communicator::handler_thread_routine，它的实现如下：\nvoid Communicator::handler_thread_routine(void *context) &#123;\tCommunicator *comm = (Communicator *)context;\tstruct poller_result *res;\twhile (1) &#123;\t\tres = (struct poller_result *)msgqueue_get(comm-&gt;msgqueue);\t\tif (!res)\t\t\tbreak;\t\tswitch (res-&gt;data.operation) &#123;\t\tcase PD_OP_TIMER:\t\t\tcomm-&gt;handle_sleep_result(res);\t\t\tbreak;\t\tcase PD_OP_READ:\t\t\tcomm-&gt;handle_read_result(res);\t\t\tbreak;\t\tcase PD_OP_WRITE:\t\t\tcomm-&gt;handle_write_result(res);\t\t\tbreak;\t\tcase PD_OP_LISTEN:\t\t\tcomm-&gt;handle_listen_result(res);\t\t\tbreak;        /* ... */\t\t&#125;\t\tfree(res);\t&#125;\tif (!comm-&gt;thrdpool) &#123;\t\tmpoller_destroy(comm-&gt;mpoller);\t\tmsgqueue_destroy(comm-&gt;msgqueue);\t&#125;&#125;\n\n阅读过上面的代码后，我们应该惊喜，因为我们看到了free！这里我可以自信的回答这个问题：上节poller当中，只看到了针对poller_node的malloc函数，而没有看到对应的free函数，哪里调用了free函数去释放poller_node？\n没错，poller_node就是在这里释放的。\npoller_node生命周期是这样的链路：\n__poller_new_node &#123; malloc &#125; -&gt; write(addr) -&gt; pipe -&gt; __poller_handle_pipe &#123; addr = read() &#125; -&gt; poller-&gt;callback(addr) -&gt; handler_thread_routine &#123; free &#125;\n\n\n最终在状态迁移池启动完毕后，结合poller的事件池，Communicator最终的系统架构如下图：\n\nIO结果的处理还是贴出上章所讲解的基本tcp服务框架示例：\n+-----------+|\tsocket\t|+-----------+\t|\tV+-----------+|\tbind\t|\t这三步就是由本节的Communicator执行+-----------+\t|\tV+-----------+|\tlisten\t|+-----------+\t|\t\t\t______________________________________________\tV+-----------+|\taccept\t|\t从这里开始涉及到的所有函数是poller负责。+-----------+|\t|\t|\t|V\tV\tV\tVfd\tfd\tfd\t...\t/\\read  write\n\n当然Communicator会使用mpoller暴露的api对sockfd设置所关心的IO事件。间接调用了IO系统调用接口。下面从listen fd入手，逐步揭开communicator的真面目。\n&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;\n创建绑定监听三部曲————Communicator::bind\n函数如下：\nint Communicator::bind(CommService *service) &#123;\tstruct poller_data data;\tint errno_bak = errno;\tint sockfd;\tsockfd = this-&gt;nonblock_listen(service);\tif (sockfd &gt;= 0) &#123;\t\tdata.fd = sockfd;\t\tdata.context = service;\t\t/* ... */\t\tdata.operation = PD_OP_LISTEN;\t\tdata.accept = Communicator::accept;\t\t// 开始接收客户端连接\t\tif (mpoller_add(&amp;data, service-&gt;listen_timeout, this-&gt;mpoller) &gt;= 0) &#123;\t\t\terrno = errno_bak;\t\t\treturn 0;\t\t&#125;\t\tclose(sockfd);\t&#125;\treturn -1;&#125;\n\n注意到，listen套接字被分配了一个Communicator::accept回调，上一章介绍poller时每当listen套接字接收到一个客户端的连接，都会将IO socket作为参数回调一下accept函数，此处代表Communicator::accept，它实际上会为IO socket创建一个CommServiceTarget对象。\n三部曲核心在nonblock_listen，如下：\nint Communicator::nonblock_listen(CommService *service) &#123;\tint sockfd = service-&gt;create_listen_fd();\t\t\t\t// scoket()\tint ret;\tif (sockfd &gt;= 0) &#123;\t\tif (__set_fd_nonblock(sockfd) &gt;= 0)\t&#123;\t\t\t\t// 设置为非阻塞\t\t\tif (__bind_sockaddr(sockfd, service-&gt;bind_addr,\t\t\t\t\t\t\t\tservice-&gt;addrlen) &gt;= 0)\t&#123; \t// 监听socket和addr绑定\t\t\t\tret = listen(sockfd, SOMAXCONN);\t\t\t// 开始监听\t\t\t\tif (ret &gt;= 0 || errno == EOPNOTSUPP) &#123;\t\t\t\t\tservice-&gt;reliable = (ret &gt;= 0);\t\t\t\t\treturn sockfd;\t\t\t\t&#125;\t\t\t&#125;\t\t&#125;\t\tclose(sockfd);\t&#125;\treturn -1;&#125;\n\n所以Communicator::bind绑定并启动一个tcpserver的流程是：\n\n初始化一个监听套接字。\n\n将监听套接字添加到mpoller事件池当中。开始接受来自客户端的连接。\n\n\n&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;\n接收客户端连接————Communicator::handle_listen_result\n了解了前面状态迁移池和bind的流程，结合上一章poller的源码分析，一旦poller的accept接收到一条连接会回调一下Communicator::accept，然后再调用poller-&gt;callback，并将IOsocket填到res，然后res被传回Communicator队列当中。状态迁移线程会从队列当中取res，然后根据res-&gt;data.operation，会去回调Communicator::handle_listen_result，它的实现如下：\nvoid Communicator::handle_listen_result(struct poller_result *res) &#123;\tCommService *service = (CommService *)res-&gt;data.context;\tstruct CommConnEntry *entry;\tCommServiceTarget *target;\tint timeout;\tswitch (res-&gt;state) &#123;\tcase PR_ST_SUCCESS:\t\ttarget = (CommServiceTarget *)res-&gt;data.result;\t\t// Communicator::accept的返回值\t\tentry = Communicator::accept_conn(target, service);\t\tif (entry) &#123;\t\t\tentry-&gt;mpoller = this-&gt;mpoller;\t\t\tres-&gt;data.operation = PD_OP_READ;\t\t\tres-&gt;data.fd = entry-&gt;sockfd;\t\t\tres-&gt;data.create_message = Communicator::create_request;\t\t\tres-&gt;data.context = entry;\t\t\tres-&gt;data.message = NULL;\t\t\ttimeout = target-&gt;response_timeout;\t\t\tif (mpoller_add(&amp;res-&gt;data, timeout, this-&gt;mpoller) &gt;= 0) &#123;\t\t\t\tif (this-&gt;stop_flag)\t\t\t\t\tmpoller_del(res-&gt;data.fd, this-&gt;mpoller);\t\t\t\tbreak;\t\t\t&#125;\t\t\t__release_conn(entry);\t\t&#125;\t\telse\t\t\tclose(target-&gt;sockfd);\t\ttarget-&gt;decref();\t\tbreak;\t&#125;&#125;\n\nCommunicator::accept_conn函数根据res创建出IOsocket的连接上下文CommConnEntry，并且该连接上下文初始的state为CONN_STATE_CONNECTED。然后将该对象加入到mpoller事件池当中开启对IOsocket的读事件进行监听。\n&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;\n边接收边解析————Communicator::create_request + Communicator::append_message\n客户端请求报文解析完毕，状态转移————Communicator::handle_read_result\n因为这两部分涉及的代码过于庞大，详细讲解的话避免不了要贴大量的代码，作者表达水平有限。还是觉得使用图解的方式去呈现比较省事。所以，为避免文章代码比例过高，下一小节将会以图画的形式向读者剖析这部分的源码。\n状态迁移总结服务端监听套接字的绑定就不说了，下面使用一张图来讲解：从服务端接收客户端连接 到 读客户端请求报文（边读边解析），最后向客户端发送回复的一个流程。\n&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;\n接收客户端连接并读取解析客户端发来的报文流程————异步状态机之美\n\n第一步，是poller当中的__poller_handle_listen回调函数：\n\n服务端在接收到一个客户端连接后首先会为其创建一个CommServiceTarget对象。\n\n然后将IO socketfd通过回调poller-&gt;callback（放到队列当中）传回给Communicator。\n\n\n第二步，是Communicator的Communicator::handle_listen_result函数：\n\n状态变迁池拿到res对像后，得知operation为PD_OP_LISTEN的res，所以调用Communicator::handle_listen_result函数来处理res。\n\n在Communicator::handle_listen_result函数当中，首先会构造一个连接上下文entry，它的状态被初始化为CONN_STATE_CONNECTED。\n\n构造一个operation为READ的poller_node。并且data成员的create_message回调填为Communicator::create_request。\n\n将poller_node加入到mpoller，开始对其度事件进行监听。\n\n\n第三步，是poller当中的读事件处理回调__poller_handle_read：\n\n读数据。\n\n__poller_append_message，它里面会创建一个poller_message_t对象（如果不存在的话，一般在一轮请求的最开始会构造一个msg对象）。利用poller_message_t对象对读到的数据进行解析。这是一个边读边解析的过程，中间可能会调用数次。当msg-&gt;append返回值大于0时，说明请求报文读并且解析完了。此时将msg封装在res当中，并回调poller-&gt;callback。create_message和append两个回调分别对应Communicator::create_request和Communicator::append_message。这两个函数核心代码已经在上图③号虚框当中显示，读者可以仔细阅读一下。这里其实涉及到连接上下文entry的两次状态变换。在create_message时，entry-&gt;state会变更为CONN_STATE_RECEIVING，而在数据解析完毕，Communicator::append_message当中的in-&gt;append返回大于0进入到下面的if分支又会将entry-&gt;state变更为CONN_STATE_SUCCESS。\n\n\n第四步，也是读流程的最后一步，Communicator::handle_read_result当中的Communicator::handle_incoming_request函数：\n\n状态变迁池拿到res对像后，得知operation为PD_OP_READ的res，所以调用Communicator::handle_read_result函数来处理res。因为是服务端所以Communicator::handle_read_result函数会调用Communicator::handle_incoming_request函数。\n\n这里会将session的state设置成CS_STATE_TOREPLY。\n\n如果entry-&gt;state &#x3D;&#x3D; CONN_STATE_SUCCESS，则将entry挂到target的idle链表上、entry-&gt;ref++，同时entry-&gt;state修改成CONN_STATE_IDLE。session-&gt;passive 必须赋值为 2。\n\n回调session-&gt;handle，然后entry-&gt;ref–，当entry-&gt;ref为0时，调用__release_conn将连接关闭，并free掉entry连接上下文。\n\n\n所以entry状态变化顺序为：\n[CONN_STATE_CONNECTED] -&gt; [CONN_STATE_RECEIVING] -&gt; [CONN_STATE_SUCCESS] -&gt; [CONN_STATE_IDLE]\n\n&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;\n向客户端发送回复报文————先尽力而为的写，然后再异步写。\n当服务端需要发送一个回复报文时会调用Communicator::reply接口，它的代码如下：\nint Communicator::reply(CommSession *session) &#123;\tstruct CommConnEntry *entry;\tCommServiceTarget *target;\tint errno_bak;\tint ret;\tif (session-&gt;passive != 2) &#123;\t// 处在读完毕的状态\t\terrno = EINVAL;\t\treturn -1;\t&#125;\terrno_bak = errno;\tsession-&gt;passive = 3;\t\t\t// 写状态\ttarget = (CommServiceTarget *)session-&gt;target;\tret = this-&gt;reply_reliable(session, target);\tif (ret == 0) &#123;\t\t\t\t\t// 这里是同步写已经将所有数据发完了。无需异步写\t\tentry = session-&gt;in-&gt;entry;\t\tsession-&gt;handle(CS_STATE_SUCCESS, 0);\t// 再次回调session的handle\t\tif (__sync_sub_and_fetch(&amp;entry-&gt;ref, 1) == 0) &#123;\t\t\t__release_conn(entry);\t\t\ttarget-&gt;decref();\t\t&#125;\t&#125; else if (ret &lt; 0)\t\treturn -1;\terrno = errno_bak;\treturn 0;&#125;int Communicator::reply_reliable(CommSession *session, CommTarget *target) &#123;\tstruct CommConnEntry *entry;\tstruct list_head *pos;\tint ret = -1;\tpthread_mutex_lock(&amp;target-&gt;mutex);\tif (!list_empty(&amp;target-&gt;idle_list)) &#123;\t// 处于CONN_STATE_IDLE状态\t\tpos = target-&gt;idle_list.next;\t\tentry = list_entry(pos, struct CommConnEntry, list);\t\tlist_del(pos);\t\tsession-&gt;out = session-&gt;message_out();\t\tif (session-&gt;out)\t\t\tret = this-&gt;send_message(entry);\t&#125; else\t\terrno = ENOENT;\tpthread_mutex_unlock(&amp;target-&gt;mutex);\treturn ret;&#125;int Communicator::send_message(struct CommConnEntry *entry) &#123;\t/* ... */\tend = vectors + cnt;\tcnt = this-&gt;send_message_sync(vectors, cnt, entry);\t\t// 先尽力而为的同步写\tif (cnt &lt;= 0)\t\treturn cnt;\treturn this-&gt;send_message_async(end - cnt, cnt, entry);\t// 写缓存满了，需要异步写&#125;\n\n写的设计思路和Muduo的很像muduo源码阅读笔记（10、TcpConnection）。这里不过多赘述，只讲一下差别。还是以全面的情况为例子，假设现在需要发送一批（回复）数据，并且同步写无法将所有的数据发送完。那么在同步写一部分我们的数据之后，肯定会触发异步写。\n而异步写呢，就得靠poller层的__poller_handle_write函数。只要tcp的发送缓存区非满，poller_node就会收到通知，然后尽力向发送缓存区写一些数据，这可能也需要花几轮的功夫去写数据。在这期间，每写一部分数据__poller_handle_write函数就会回调node-&gt;data.partial_written，从Communicator::send_message_async函数在构造WRITE类型的poller_node时我们可以得知partial_written就是Communicator::partial_written，而它的实现如下：\nint Communicator::partial_written(size_t n, void *context) &#123;\tstruct CommConnEntry *entry = (struct CommConnEntry *)context;\tCommSession *session = entry-&gt;session;\tint timeout;\ttimeout = Communicator::next_timeout(session);\tmpoller_set_timeout(entry-&gt;sockfd, timeout, entry-&gt;mpoller);\treturn 0;&#125;\n\n在写完部分数据后，为什么需要回调一下partial_written呢？这里其实就得到了合理的解释，既然在规定的写超时时间内，我能向发送缓存写一些数据，那就说明网没断，只是网络状况可能不好。所以，按理来说，在规定的时间内发送了部分数据就应该更新一下发送的超时时间，避免没有必要的超时。\n一旦异步写完成了，和__poller_handle_read不同，**__poller_handle_write会自动将poller_node从epoll上移除**，然后回调poller-&gt;callback。（PS，如果你忘了poller的实现，建议回顾一下WorkFlow源码剖析——Communicator之TCPServer（上））\n然后同读完成类似，在Communicator当中写完成会被Communicator::handle_write_result处理，因为是服务断，所以会调用Communicator::handle_reply_result。该函数逻辑如下：\nvoid Communicator::handle_reply_result(struct poller_result *res) &#123;\tstruct CommConnEntry *entry = (struct CommConnEntry *)res-&gt;data.context;\tCommService *service = entry-&gt;service;\tCommSession *session = entry-&gt;session;\tCommTarget *target = entry-&gt;target;\tint timeout;\tint state;\tswitch (res-&gt;state) &#123;\tcase PR_ST_FINISHED:\t\ttimeout = session-&gt;keep_alive_timeout();\t\tif (timeout != 0) &#123;\t\t\t__sync_add_and_fetch(&amp;entry-&gt;ref, 1);\t\t\t\t\t\t\t// 避免被释放\t\t\tres-&gt;data.operation = PD_OP_READ;\t\t\tres-&gt;data.create_message = Communicator::create_request;\t\t\tres-&gt;data.message = NULL;\t\t\tpthread_mutex_lock(&amp;target-&gt;mutex);\t\t\tif (mpoller_add(&amp;res-&gt;data, timeout, this-&gt;mpoller) &gt;= 0) &#123;\t\t// 以读的方式添加到mpoller当中\t\t\t\tpthread_mutex_lock(&amp;service-&gt;mutex);\t\t\t\tif (!this-&gt;stop_flag &amp;&amp; service-&gt;listen_fd &gt;= 0) &#123;\t\t\t\t\tentry-&gt;state = CONN_STATE_KEEPALIVE;\t\t\t\t\t// entry-&gt;state修改成CONN_STATE_KEEPALIVE\t\t\t\t\tlist_add_tail(&amp;entry-&gt;list, &amp;service-&gt;alive_list);\t\t// 追加到保活链表\t\t\t\t&#125; else &#123;\t\t\t\t\tmpoller_del(res-&gt;data.fd, this-&gt;mpoller);\t\t\t\t\tentry-&gt;state = CONN_STATE_CLOSING;\t\t\t\t&#125;\t\t\t\tpthread_mutex_unlock(&amp;service-&gt;mutex);\t\t\t&#125;\t\t\telse\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t// 出错，该释放了\t\t\t\t__sync_sub_and_fetch(&amp;entry-&gt;ref, 1);\t\t\t\t\tpthread_mutex_unlock(&amp;target-&gt;mutex);\t\t&#125;\t\tif (1)\t\t\tstate = CS_STATE_SUCCESS;\t\tsession-&gt;handle(state, res-&gt;error);\t\t\t\t\t\t\t\t\t// 第二次回调session-&gt;handle\t\tif (__sync_sub_and_fetch(&amp;entry-&gt;ref, 1) == 0) &#123;\t\t\t__release_conn(entry);\t\t\t((CommServiceTarget *)target)-&gt;decref();\t\t&#125;\t\tbreak;\t&#125;&#125;\n\n逻辑分成三部分：\n\n将entry-&gt;ref自增一 &amp;&amp; 以读的方式将poller_node加回到mpoller当中继续监听客户端的读请求 &amp;&amp; 将entry-&gt;state修改成CONN_STATE_KEEPALIVE并且加到CommService的保活链表当中。\n\n将session的state设置成CS_STATE_SUCCESS，再回调session-&gt;handle。\n\nentry-&gt;ref自减一，为零就释放连接以及上下文。\n\n\n所以在经过Communicator::handle_reply_result函数后，entry-&gt;state状态被修改为CONN_STATE_KEEPALIVE。然后再处理下一轮客户端请求。\n最后备忘一下：\n\n对于session-&gt;passive的变化，在（create_request）创建msg（请求报文解析器）session-&gt;passive被置为1，在读取并解析完毕请求报文后，Communicator::handle_incoming_request函数回将session-&gt;passive置为2，在调用Communicator::reply向网络发送回复时session-&gt;passive会被置为3。\n\nCommunicator::send_message_async当中在吧poller_node以WRITE方式加入到mpoller时会走到mpoller_mod分支。\n\n在Communicator当中，TCPServer端的session是个啥？有的人可能会联想可能是HTTPServer当中的Session？我刚开始看源码也是这样认为的，但是实际上并非如此，Communicator当中的session的定义其实和workflow里面的go-task定义很像。HTTPServer当中的Session生命周期同整个HTTP连接一样。而Communicator当中它的生命周期更像仅仅只有一轮（请求-回复），在一轮 请求-回复 过后自动被销毁。而翻阅Communicator源码发现，session会在Communicator::create_request函数中通过service-&gt;new_session接口进行分配，而释放却并不在Communicator当中。结合之前go-task的实现来看，我们可以大胆猜想：Communicator所出现的session对象一定会派生一个子类，然后session对象的释放由子类对象 delete自身而被释放了。\n\n\n到了这里其实就能回答这个问题：连接上下文为什么存在状态的迁移？\n首先一条tcp连接在服务端必定纯在两种状态：接收、发送。而因为追求性能，我们不得不采用异步的方式将socketfd设置成非阻塞的。并且网络传输（不管是否阻塞）会引来一些问题：读不可能一次性读完、写不可能一次性写完，所以读的过程当中和写的过程当中都存在一种中间的状态。所以状态迁移是必然的。\n最后，在了解了workflow的底层架构之后，其实就能感觉到异步编程就是在实现一个状体机的过程。\n\n本章完结\n","tags":["高性能服务器框架"]},{"title":"WorkFlow源码剖析——Communicator之TCPServer（下）","url":"/2024/11/07/workflow/TCPServer_3/","content":"前言系列链接如下：\nWorkFlow源码剖析——GO-Task 源码分析\nWorkFlow源码剖析——Communicator之TCPServer（上）\nWorkFlow源码剖析——Communicator之TCPServer（中）\nWorkFlow源码剖析——Communicator之TCPServer（下）\n终于来到TCPServer最后一部分，前面两篇博客已经深入分析了WorkFlow底层poller和Communicator的实现细节，本篇博客将会从整体视角，整合前面所讲的poller以及Communicator形成最终的TCPServer。\n同样放上workflow开源项目的Github地址：https://github.com/sogou/workflow\n和GO-Task的实现类似，尤其需要注意对基类SubTask、CommSession虚函数的重写。如果你看过GO-Task的实现，本文最终所讲的TCPServer任务其实差不多。因为TCPServer的继承树和GO-Task的继承树不能说相似，只能说一模一样。对称性对框架的设计真的很重要，我认为对称思想（也可以说成抽象思想）是优雅的象征。并且对称性可以帮我们减少出BUG的风险。如果你刷过的LeetCode，你一定会发现，在解答那些对边界条件要求很高的题目时，如果你能给各种情况抽象出一套统一的逻辑说词，大概率就不会wa。\n\n\n重申一下，本系列暂时集中分析workflow的TCPServer端的架构。对于客户端，后面有时间了会另起一个系列进行讲解。像CommSchedGroup、CommSchedTarget、CommSchedObject等属于客户端独有功能。CommSchedGroup主要功能是对客户端的连接按负载（引用数量）进行一个堆排序管理。读者可先忽略掉这些内容。并且因为有些类的设计是同时兼顾客户端和服务端的（如：CommRequest、等），这点在阅读源码的时候需要有自己的判断能力。不要被绕进去了！\n正文我们就顺从WorkFlow GO-Task 源码分析的方式，以workflow给的http_echo_server的示例作为本文的切入点：\n用法go-task的用法示例如下：\n#include &lt;stdio.h&gt;#include &lt;utility&gt;#include &quot;workflow/HttpMessage.h&quot;#include &quot;workflow/HttpUtil.h&quot;#include &quot;workflow/WFServer.h&quot;#include &quot;workflow/WFHttpServer.h&quot;#include &quot;workflow/WFFacilities.h&quot;void process(WFHttpTask *server_task) &#123;\tprotocol::HttpRequest *req = server_task-&gt;get_req();\tprotocol::HttpResponse *resp = server_task-&gt;get_resp();    /* 根据http请求进行一些业务处理，然后构造出回复报文。 */    /* ... */&#125;int main(int argc, char *argv[]) &#123;\tunsigned short port;\tif (argc != 2) &#123;\t\tfprintf(stderr, &quot;USAGE: %s &lt;port&gt;\\n&quot;, argv[0]);\t\texit(1);\t&#125;\tsignal(SIGINT, sig_handler);\tWFHttpServer server(process);\tport = atoi(argv[1]);\tif (server.start(port) == 0) &#123;\t\twait_group.wait();\t\tserver.stop();\t&#125; else &#123;\t\tperror(&quot;Cannot start server&quot;);\t\texit(1);\t&#125;\treturn 0;&#125;\n\n从workflow的httpserver的使用demo当中可以了解到，核心框架有三步：\n\n将http处理回调函数作为参数，构造一个server对象。\n\n调用start接口，启动server。\n\nwait_group.wait()阻塞，等待服务的结束。\n\n\n看到这三步流程，我们其实应该是一脸蒙的，根本无法猜到它底层是如何起服务的；当连接来到时又是如何回调上面的处理函数的。别着急我们先结合前面两篇博客，尽力而为的猜：\n\n看到了process回调函数当中开头定义的两个指针变量req和resp都是来自server_task。结合tcp服务端在读取来自客户端的请求报文并解析前会调用Communicator::create_request函数创建一个in对象作为报文解析器，而in又是由session创建，而在服务端session又是由CommService创建。同时Communicator::reply接口是以session作为参数，最终tcpserver在发送回复时会取出session当中的out并发送给客户端，很明显的是：out显然是服务端对客户端请求的回复报文。所以种种迹象都表明server_task当中req、resp和session的in和out有着紧密联系。\n\nserver.start接口一定会调用创建socket，绑定socket、监听sokcet。而这些流程在Communicator当中有提供接口，对应：Communicator::bind。Communicator::bind函数只有一个唯一的参数：CommService，但综合CommService头文件的定义来看，因为它里面有一个纯虚函数：new_session，显然CommService是一个虚基类，这意味着它无法实例化对象。所以一定有继承CommService的子类。\n\n\n综上，1、2两点都指向了一个关键词————CommService。\n探究WFHttpServer根据上小节得到的线索，我们深入跟到WFHttpServer当中去，它的继承树如下：\n&#123; WFHttpServer == WFServer&lt;protocol::HttpRequest, protocol::HttpResponse&gt; &#125; -&gt; WFServerBase -&gt; CommService\n\n所以，WFHttpServer实际上是模板类WFServer的一个成员函数全特化实现。下面集中分析一下WFServer和WFServerBase：\n首先是WFServer模板类：\ntemplate&lt;class REQ, class RESP&gt;class WFServer : public WFServerBase &#123;public:\tWFServer(const struct WFServerParams *params,\t\t\t std::function&lt;void (WFNetworkTask&lt;REQ, RESP&gt; *)&gt; proc) :\t\tWFServerBase(params),\t\tprocess(std::move(proc)) &#123;  &#125;\tWFServer(std::function&lt;void (WFNetworkTask&lt;REQ, RESP&gt; *)&gt; proc) :\t\tWFServerBase(&amp;SERVER_PARAMS_DEFAULT),\t\tprocess(std::move(proc)) &#123;  &#125;protected:\tvirtual CommSession *new_session(long long seq, CommConnection *conn);protected:\tstd::function&lt;void (WFNetworkTask&lt;REQ, RESP&gt; *)&gt; process;&#125;;template&lt;class REQ, class RESP&gt;CommSession *WFServer&lt;REQ, RESP&gt;::new_session(long long seq, CommConnection *conn) &#123;\tusing factory = WFNetworkTaskFactory&lt;REQ, RESP&gt;;\tWFNetworkTask&lt;REQ, RESP&gt; *task;\ttask = factory::create_server_task(this, this-&gt;process);\ttask-&gt;set_keep_alive(this-&gt;params.keep_alive_timeout);\ttask-&gt;set_receive_timeout(this-&gt;params.receive_timeout);\ttask-&gt;get_req()-&gt;set_size_limit(this-&gt;params.request_size_limit);\treturn task;&#125;\n\n（PS，代码量很少，读者表示狂喜。）\n如代码所写的那样，WFServer就是继承了一下WFServerBase，并重写了new_session函数。究其根本这里的new_session实际上重写的是CommService当中所定义的纯虚函数。如果你认为应该仔细去阅读这里重写的虚函数，那你就错了，实际上WFHttpServer又将new_session函数进行全特化实现。所以WFServer的new_session看看就好。无需深入理解。\nWFServer重点就是将示例在创建server时传入的process回调，保存到了成员变量当中，以供new_session时将任务回调传给Task。下面重点研究一下WFServerBase。\n从上面的分析了解到WFServerBase继承自CommService。WFServerBase实现如下：\nclass WFServerBase : protected CommService &#123;public:\tWFServerBase(const struct WFServerParams *params) :\t\tconn_count(0) &#123;\t\tthis-&gt;params = *params;\t\tthis-&gt;unbind_finish = false;\t\tthis-&gt;listen_fd = -1;\t&#125;public:\t/* To start a TCP server */\t/* ... */\t/* Start with binding address. The only necessary start function. */\tint start(const struct sockaddr *bind_addr, socklen_t addrlen);\t/* stop() is a blocking operation. */\tvoid stop() &#123;\t\tthis-&gt;shutdown();\t\tthis-&gt;wait_finish();\t&#125;\t/* Nonblocking terminating the server. For stopping multiple servers.\t * Typically, call shutdown() and then wait_finish().\t * But indeed wait_finish() can be called before shutdown(), even before\t * start() in another thread. */\tvoid shutdown();\tvoid wait_finish();public:\tsize_t get_conn_count() const &#123; return this-&gt;conn_count; &#125;protected:\tWFServerParams params;protected:\tvirtual int create_listen_fd();\tvirtual WFConnection *new_connection(int accept_fd);\tvoid delete_connection(WFConnection *conn);private:\tint init(const struct sockaddr *bind_addr, socklen_t addrlen);\tvirtual void handle_unbound();protected:\tstd::atomic&lt;size_t&gt; conn_count;private:\tint listen_fd;\tbool unbind_finish;\tstd::mutex mutex;\tstd::condition_variable cond;\tclass CommScheduler *scheduler;&#125;;\n\n首先，我们看到WFServerBase当中有一个类型为CommScheduler的成员变量scheduler。我们应该感到惊喜，因为CommScheduler不就是对Communicator做了一层浅浅的封装吗？这里出现的scheduler不就意味着WFServerBase和Communicator联系起来了吗？那server的启动必定是调用了Communicator::bind接口来创建、绑定、监听listen socket。下面重点研究一下start函数的函数的实现：\nint WFServerBase::start(const struct sockaddr *bind_addr, socklen_t addrlen) &#123;\tif (this-&gt;init(bind_addr, addrlen) &gt;= 0) &#123;\t\tif (this-&gt;scheduler-&gt;bind(this) &gt;= 0)\t\t\treturn 0;\t\tthis-&gt;deinit();\t&#125;\tthis-&gt;listen_fd = -1;\treturn -1;&#125;\n\ninit函数伪代码如下：\nint WFServerBase::init(const struct sockaddr *bind_addr, socklen_t addrlen) &#123;\t/* ... */\tif (this-&gt;CommService::init(bind_addr, addrlen, -1, timeout) &lt; 0)\t// 调用基类CommService的初始化函数，就是将listen fd所绑定的地址拷贝一份到基类。\t\treturn -1;\tthis-&gt;scheduler = WFGlobal::get_scheduler();\t\t\t\t\t\t// 全局的单例CommScheduler对象。\treturn 0;&#125;\n\n主要干了两件事：调用基类的init，将绑定的地址拷贝一份到基类的成员变量当中。然后通过__CommManager获取全局的单例CommScheduler对象。\n特别的是，这里有个重要的时间点，在__CommManager被构造时，会初始化CommScheduler对象，如果你看过上一篇博客，你一定知道为什么这个时刻重要。因为CommScheduler::init函数会启动workflow底层的事件池和状态迁移池。具体的架构模型图可以参考：WorkFlow源码剖析——Communicator之TCPServer（中）。\n在WFServerBase::start实现中，调用init函数过后，立马调用CommScheduler::bind（实际上就是Communicator::bind），该函数里面会做网络编程三部曲：创建、绑定、监听。至此我们的TCPServer服务器就在这里启动，等待客户端的连接。\n关于WFServerBase其实还有两个有趣的知识点：new_connection 和 服务停止。\n\nnew_connection：该函数和WFServerBase::conn_count强相关。new_connection所创建的对象共用WFServerBase::conn_count。每当有客户端连接到来，都会创建一个CommConnection对象，同时会使WFServerBase::conn_count自增一。每当连接断开，Communicator当中就会调用__release_conn释放连接上下文，并且CommConnection对象也随之释放，其构造函数当中，会将WFServerBase::conn_count变量自减一。所以说，每次在连接到来创建的CommConnection对象可以视为连接计数器。（PS，因为目前只了解workflow的部分源码，所以连接计数器存在的具体意义，我目前还未能领悟。后面有时间的话，再去深究吧。）\n\n服务停止：如代码注释那样，WFServerBase所提供的stop接口是阻塞的，它其实连续调用了两个函数：shutdown、wait_finish。其中shutdown会调用Communicator::unbind函数，它会直接将listen fd从mpoller当中删除。当调用shutdown函数时，整体停止的链路是这样的：\n\n\nWFServerBase::shutdown -&gt; CommScheduler::unbind -&gt; Communicator::unbind -&gt; mpoller_del(listen_fd) - - -&gt; Communicator::handle_listen_result -&gt; Communicator::shutdown_service -&gt; while (直到CommService的ref减为0) &#123; CommService::decref() &#125; -&gt; WFServerBase::handle_unbound\n\nCommunicator::shutdown_service代码如下：\nvoid Communicator::shutdown_service(CommService *service) &#123;\tclose(service-&gt;listen_fd);\tservice-&gt;listen_fd = -1;\tservice-&gt;drain(-1);\tservice-&gt;decref();&#125;\n\n这里的service-&gt;drain(-1)会将server端目前所有的连接都从mpoller当中移除。然后等待所有连接上下文回调CommServiceTarget::decref将server对象的引用计数减为0后，调用WFServerBase::handle_unbound函数\ninline void CommService::decref() &#123;\tif (__sync_sub_and_fetch(&amp;this-&gt;ref, 1) == 0)\t\tthis-&gt;handle_unbound();\t\t\t\t\t\t// 最终被重写成：WFServerBase::handle_unbound&#125;\n\nWFServerBase::stop的注释说明了该函数是阻塞的，其阻塞主要原因就在wait_finish，它会等待所有的连接被释放然后释放WFServerBase的引用计数后才会跳出等待条件变量的循环。\nvoid WFServerBase::handle_unbound() &#123;\tthis-&gt;mutex.lock();\tthis-&gt;unbind_finish = true;\tthis-&gt;cond.notify_one();\tthis-&gt;mutex.unlock();&#125;void WFServerBase::wait_finish() &#123;\tstd::unique_lock&lt;std::mutex&gt; lock(this-&gt;mutex);\twhile (!this-&gt;unbind_finish)\t\tthis-&gt;cond.wait(lock);\tthis-&gt;deinit();\tthis-&gt;unbind_finish = false;\tlock.unlock();&#125;\n\n探究WFHttpServerTask好了，tcpserver的启动流程基本流程已经分析完毕，下面我们重点看看WFHttpServer::new_session的实现。该函数在每轮读取客户端请求时会被调用一次。返回值是类型为CommSession的对象。\ntemplate&lt;&gt; inlineCommSession *WFHttpServer::new_session(long long seq, CommConnection *conn) &#123;\tWFHttpTask *task;\ttask = WFServerTaskFactory::create_http_task(this, this-&gt;process);\ttask-&gt;set_keep_alive(this-&gt;params.keep_alive_timeout);\ttask-&gt;set_receive_timeout(this-&gt;params.receive_timeout);\ttask-&gt;get_req()-&gt;set_size_limit(this-&gt;params.request_size_limit);\treturn task;&#125;\n\n可以看到出现了一个新的类——WFHttpTask，我可以明确告诉你，WFHttpTask只是一个基类，我们应该从final类开始深入分析。\n对于WFHttpTask，它的定义如下：\nusing WFHttpTask = WFNetworkTask&lt;protocol::HttpRequest,\t\t\t\t\t\t\t\t protocol::HttpResponse&gt;;\n\n那么WFNetworkTask是啥呢？先别急，后面再来揭晓它的源码。通过WFServerTaskFactory::create_http_task我们可以找到我们所需要的final类——WFHttpServerTask它的定义如下：\nclass WFHttpServerTask : public WFServerTask&lt;protocol::HttpRequest,\t\t\t\t\t\t\t\t\t\t\t protocol::HttpResponse&gt; &#123;private:\tusing TASK = WFNetworkTask&lt;protocol::HttpRequest, protocol::HttpResponse&gt;;public:\tWFHttpServerTask(CommService *service, std::function&lt;void (TASK *)&gt;&amp; proc) :\t\tWFServerTask(service, WFGlobal::get_scheduler(), proc),\t\treq_is_alive_(false),\t\treq_has_keep_alive_header_(false) &#123;  &#125;protected:\tvirtual void handle(int state, int error);\tvirtual CommMessageOut *message_out();protected:\tbool req_is_alive_;\tbool req_has_keep_alive_header_;\tstd::string req_keep_alive_;&#125;;\n\n从构造函数当可以看到，再一次对全局单例的CommScheduler的引用。类的成员函数包括hanlde、message_out最终实现，我们重点关注handle的实现：\nvoid WFHttpServerTask::handle(int state, int error) &#123;\tif (state == WFT_STATE_TOREPLY) &#123;\t\t/* 设置fianl类的成员变量... */\t&#125;\tthis-&gt;WFServerTask::handle(state, error);&#125;\n\n在服务端收完并解析完客户端发来的请求报文之后（在Communicator::handle_incoming_request函数当中）会进入该函数，从WFT_STATE_TOREPLY宏的命名也可以推测到，它代表准备回复的状态。在做完final类一些设置后，最终会调用父类的handle，所以下面深入看看WFServerTask模板类的实现。\ntcpserver任务部分最烧脑的就在WFServerTask模板类的实现，对于WFNetworkTask模板类，它本身的成员函数对我们理解tcpserver本身来说并不重要。但需要注意的是WFNetworkTask继承自CommRequest。\n简单用字符画了一下WFHttpServerTask的继承树。如下：\nSubTask\t\tCommSession\t\t\\/\tCommRequest\t\t|\t\tVWFNetworkTask&lt;REQ, RESP&gt;\t# 该类的实现在对我们理解tcpserver不是特别重要，读者可以跳过该类。\t\t|\t\tVWFServerTask&lt;REQ, RESP&gt;\t\t|\t\tVWFHttpServerTask\n\n在正式讲解WFServerTask前，先学习几个关键知识点：\n首先回顾一下，SubTask::subtask_done函数实现：\nvoid SubTask::subtask_done() &#123;\tSubTask *cur = this;\tcur = cur-&gt;done();\tif (cur) &#123;\t\tcur-&gt;dispatch();\t\t// 下一个任务的dispatch\t&#125;\treturn;&#125;\n\n更简单点描述，调done后调dispatch触发任务队列的下一个任务。（关键点一：）其中done函数实现最后都会调用series_of(this)-&gt;pop()，这行代码是获取SeriesWork串行队列的下一个任务，当队列中（没有任何任务了）山穷水尽了会返回nullptr，并且SeriesWork会delete this（SeriesWork对象本身）。\n然后了解一下两个WFServerTask当中的内嵌类的定义：\nclass Processor : public SubTask &#123;public:\tProcessor(WFServerTask&lt;REQ, RESP&gt; *task,\t\t\t\tstd::function&lt;void (WFNetworkTask&lt;REQ, RESP&gt; *)&gt;&amp; proc) :\t\tprocess(proc) &#123;\t\tthis-&gt;task = task;\t&#125;\tvirtual void dispatch() &#123;\t\tthis-&gt;process(this-&gt;task);\t\t// 调用\t\tthis-&gt;task = NULL;\t/* As a flag. get_conneciton() disabled. */\t\tthis-&gt;subtask_done();\t&#125;\tvirtual SubTask *done() &#123;\t\treturn series_of(this)-&gt;pop();\t// 获取串行队列下一个任务\t&#125;\tstd::function&lt;void (WFNetworkTask&lt;REQ, RESP&gt; *)&gt;&amp; process;\tWFServerTask&lt;REQ, RESP&gt; *task;&#125; processor;class Series : public SeriesWork &#123;public:\tSeries(WFServerTask&lt;REQ, RESP&gt; *task) :\t\tSeriesWork(&amp;task-&gt;processor, nullptr) &#123;\t\tthis-&gt;set_last_task(task);\t\tthis-&gt;task = task;\t&#125;\tvirtual ~Series() &#123;\t\tdelete this-&gt;task;\t&#125;\tWFServerTask&lt;REQ, RESP&gt; *task;&#125;;\n\n\nProcessor::dispatch函数首先调用了一下构造传进来的回调函数process，然后调用subtask_done，结合上面的分析，它会调用串行队列当中的下一个任务的dispatch函数。\n\n对于Series，只有析构和构造函数，从构造函数当中可以看出来，它本质上就是 （关键点二）只有两个任务的串行队列。并且在该串行队列被delete时，顺带会在析构函数当中delete掉二号任务。此外，（关键点三）在每个任务被加到串行队列当中时，会将任务的SubTask::pointer指针指向串行队列对象。\n\n\n好了，下面从WFServerTask&lt;REQ, RESP&gt;::handle函数开始分析其中的奥妙。源代码如下：\ntemplate&lt;class REQ, class RESP&gt;void WFServerTask&lt;REQ, RESP&gt;::handle(int state, int error) &#123;\tif (state == WFT_STATE_TOREPLY) &#123;\t\tthis-&gt;state = WFT_STATE_TOREPLY;\t\tthis-&gt;target = this-&gt;get_target();\t\tnew Series(this);\t\tthis-&gt;processor.dispatch();\t&#125;\telse if (this-&gt;state == WFT_STATE_TOREPLY) &#123;\t\tthis-&gt;state = state;\t\tthis-&gt;error = error;\t\tif (error == ETIMEDOUT)\t\t\tthis-&gt;timeout_reason = TOR_TRANSMIT_TIMEOUT;\t\tthis-&gt;subtask_done();\t&#125;\telse\t\tdelete this;&#125;\n\n因为WFServerTask顶层的基类包括：SubTask + CommSession，对这里着重强调CommSession，因为CommSession当中的handle最终被重写成如上所示的代码。服务端在每次读完（并解析完）客户端发来的数据后，状态迁移池都会回调Communicator::handle_incoming_request函数，在每次写完客户端请求的回复后，状态迁移池都会回调Communicator::handle_reply_result函数。这两函数当最后都会调用session-&gt;handle，所不同的是每次传入的state参数有所不同。正常情况下，在读完后回调session-&gt;handle传入的state为CS_STATE_TOREPLY，而在写完后回调的session-&gt;handle传入的state为CS_STATE_SUCCESS。\n\n所以在解析完客户端请求后所调用的handle会进入第一个if分支，从代码当中可以看到，它先是将WFServerTask::state数据成员设置成了WFT_STATE_TOREPLY，这为发送完回复再次回到handle进入第二个if分支做准备。然后最关键的是new Series(this);这行代码，如果你第一次看workflow的源码这样的写法一定会让你蒙。什么鬼？new的成员没用什么变量去接？这不典型内存泄漏了吗？，但是深入研究下去，这样写好像也没问题。结合上面Series的定义，所以最终new的Series串行队列当中第一个任务是WFServerTask::processor，第二个任务是WFServerTask本身。继续分析第一个if分支的代码，接下来调用了this-&gt;processor.dispatch();函数这也是整个业务代码的起始点。深入分析Processor::dispatch实现可知，它首先调用process回调，我可以直接告诉你此回调正是我们在示例当中创建server时传进来的process函数。在调用process处理完业务代码，然后调用了this-&gt;subtask_done();函数，根据前面提到的关键点一，我们可以知道这将返回串行队列的第二个任务即WFServerTask。我想这里一定会有读者有疑惑，怎么就返回第二个任务了？不是应该返回第一个任务吗？如果你有这样的疑惑，我建议你仔细阅读一下SeriesWork这部分的源码，所谓的两个任务实际上首个任务是不会入队列的，需要人手动触发，而在WFServerTask&lt;REQ, RESP&gt;::handle当中其实已经手动触发了一号任务————Processor，所以第一次调用subtask_done实际上返回的是第二个任务。好了回到正题，进入二号任务的dispatch（WFServerTask::dispatch），因为我们在前面已经将WFServerTask::state设置成了WFT_STATE_TOREPLY所以，进入if分支，这里应该开个香槟了，因为这里调用了this-&gt;scheduler-&gt;reply，这意味着服务端向客户端发送回复了！！！正常情况下会返回大于零的值，然后直接返回。这里罗列一下调用和返回流程：\n  首先是调用流程：\n  WFServerTask&lt;REQ, RESP&gt;::handle -&gt;Processor::dispatch -&gt; Processor::subtask_done -&gt; WFServerTask::dispatch\n\n  然后是返回流程：\n  从第二个任务的WFServerTask的：dispatch函数当中 返回到 -&gt; 第一个任务Processor的基类函数：subtask_done -&gt; 返回到第一个任务Processor的dispatch函数 -&gt; 返回到WFServerTask&lt;REQ, RESP&gt;::handle函数。\n\n  我这里想表达的是：WFServerTask&lt;REQ, RESP&gt;::handle第一个if分支的一次dispatch实际上嵌套执行了两个任务。，注意是嵌套，这点很重要！\n\n然后在写完回复后，会再次回到WFServerTask&lt;REQ, RESP&gt;::handle，此时会进入第二个if分支，并且根据关键点一，因为此时第一个if分支new的串行队列已经为空，所以WFServerTask::subtask_done操作会将第一个if分支new的串行队列给释放掉，同时因为Series的释放，它的析构函数又会将WFServerTask给释放掉！\n\n\n下面贴出WFServerTask关键的代码：\ntemplate&lt;class REQ, class RESP&gt;class WFServerTask : public WFNetworkTask&lt;REQ, RESP&gt; &#123;protected:\tvirtual CommMessageOut *message_out() &#123; return &amp;this-&gt;resp; &#125;\tvirtual CommMessageIn *message_in() &#123; return &amp;this-&gt;req; &#125;\tvirtual void handle(int state, int error);protected:\tvirtual void dispatch() &#123;\t\tif (this-&gt;state == WFT_STATE_TOREPLY) &#123;\t\t\t/* Enable get_connection() again if the reply() call is success. */\t\t\tthis-&gt;processor.task = this;\t\t\tif (this-&gt;scheduler-&gt;reply(this) &gt;= 0)\t// 发生回复\t\t\t\treturn;\t\t\tthis-&gt;state = WFT_STATE_SYS_ERROR;\t\t\tthis-&gt;error = errno;\t\t\tthis-&gt;processor.task = NULL;\t\t&#125;\t\telse\t\t\tthis-&gt;scheduler-&gt;shutdown(this);\t\tthis-&gt;subtask_done();\t&#125;\tvirtual SubTask *done() &#123;\t\tSeriesWork *series = series_of(this);\t\tif (this-&gt;callback)\t\t\tthis-&gt;callback(this);\t\t/* Defer deleting the task. */\t\treturn series-&gt;pop();\t&#125;public:\tWFServerTask(CommService *service, CommScheduler *scheduler,\t\t\t\t std::function&lt;void (WFNetworkTask&lt;REQ, RESP&gt; *)&gt;&amp; proc) :\t\tWFNetworkTask&lt;REQ, RESP&gt;(NULL, scheduler, nullptr),\t\tprocessor(this, proc)\t&#123; &#125;&#125;;\n\n好了基本的HTTPServer端处理客户端请求的流程已经梳理完毕，最后贴出我在看源码的过程当中，所梳理流程笔记，可以给读者提供一些思路。同时也作为我个人的备忘笔记：\n# Workflow服务端处理连接流程分析server:    entry:         accept_conn: CONN_STATE_CONNECTED -&gt;             create_request: CONN_STATE_RECEIVING -&gt;                             // tag1                append_message: 当http请求接受完毕时：CONN_STATE_SUCCESS -&gt;                    handle_incoming_request:                         ==&gt;     CONN_STATE_IDLE &amp;&amp; entry被追加到target-&gt;idle_list上;                        ==&gt;    session-&gt;passive = 2;                        ==&gt;    state = CS_STATE_TOREPLY    |   |   |    V   V   V    WFHttpServerTask::handle -&gt;        WFServerTask&lt;REQ, RESP&gt;::handle -&gt;            ```cpp                template&lt;class REQ, class RESP&gt;                void WFServerTask&lt;REQ, RESP&gt;::handle(int state, int error)                &#123;                    if (state == WFT_STATE_TOREPLY)         // √                    &#123;                        this-&gt;state = WFT_STATE_TOREPLY;                        this-&gt;target = this-&gt;get_target();                        new Series(this);                        this-&gt;processor.dispatch();                    &#125;                    else if (this-&gt;state == WFT_STATE_TOREPLY)                    &#123;                        this-&gt;state = state;                        this-&gt;error = error;                        if (error == ETIMEDOUT)                            this-&gt;timeout_reason = TOR_TRANSMIT_TIMEOUT;                        this-&gt;subtask_done();                    &#125;                    else                        delete this;                &#125;\n\n    WFServerTask&lt;REQ, RESP&gt;::dispatch -&gt;\n        this-&gt;scheduler-&gt;reply(this) -&gt; 以写方式将entry添加到epoll上\n            session-&gt;passive = 3;\n\n|   |   |       \nV   V   V\nhandle_reply_result -&gt;\n    ===&gt; entry-&gt;state = CONN_STATE_KEEPALIVE &amp;&amp; entry 被追加到service-&gt;alive_list &amp;&amp; 以读方式将entry添加到epoll上（如果保活的话）\n    ===&gt; state = CS_STATE_SUCCESS\n\n|   |   |\nV   V   V\nWFHttpServerTask::handle -&gt;\n    WFServerTask&lt;REQ, RESP&gt;::handle -&gt;\n        template&lt;class REQ, class RESP&gt;void WFServerTask&lt;REQ, RESP&gt;::handle(int state, int error)&#123;    if (state == WFT_STATE_TOREPLY)    &#123;        this-&gt;state = WFT_STATE_TOREPLY;        this-&gt;target = this-&gt;get_target();        new Series(this);        this-&gt;processor.dispatch();    &#125;    else if (this-&gt;state == WFT_STATE_TOREPLY)  // √    &#123;        this-&gt;state = state;        this-&gt;error = error;        if (error == ETIMEDOUT)            this-&gt;timeout_reason = TOR_TRANSMIT_TIMEOUT;        this-&gt;subtask_done();       // will delete Series for first &#x27;if&#x27; branch malloc    &#125;    else        delete this;&#125;\n\n|   |   |\nV   V   V\ngo to tag1.\n\n\n---\n\n**本章完结**\n\n","tags":["高性能服务器框架"]},{"title":"WorkFlow GO-Task 源码分析","url":"/2024/10/13/workflow/go_task/","content":"WorkFlow GO-Task 源码分析\nWorkFlow源码剖析——Communicator之TCPServer（上）\nWorkFlow源码剖析——Communicator之TCPServer（中）\nWorkFlow源码剖析——Communicator之TCPServer（下）\n前言任何好的框架的设计都是围绕着一个核心思想去展开，sylar的一切皆协程、muduo的one loop per thread等。一切皆是任务流就是workflow的精髓。（PS，目前作者功力尚浅，许多设计细节还未能悟透其用意，目前也只能尽力将我的理解呈现出来，有错误非常欢迎指出。\n也是尝试着阅读过许多开源优秀的代码，这里记录一下我个人在阅读一份源码时的习惯：适可而止的自低向上。因为我在阅读一份完全不了解的源码时，迫不及待的想去知道每个每个模块、每个函数的实现细节，我也曾尝试以自顶向下去阅读一份源码，但是无法克制自己钻牛角尖的心，并且在经验尚浅，完全不了解设计背景的境况下，自顶向下去阅读一份源码，某一个函数的实现你只能去猜，由于经验尚浅，你大概率猜的也是错误的。所以，兜兜转转，我还是遵循我个人的习惯，自低向上去阅读一份源码。当然，应该：适可而止的自低向上，一些你完全知道起什么作用的模块其实就不必去深究了，比如：链表、红黑树、编码器等。深入细节的同时，也不要忘了我们的初心：框架的设计思想。\n\n\n网络框架（包括库）的模块设计其实有很多相似的地方，比如都会有的：线程池、对epoll的封装、对io接口的封装、对tcpserver以及tcpclient的封装等。在阅读网络并发相关的源码时可以以这些方面入手。\n在深入阅读workflow的源码之后，特别是在kernel文件夹下对一些基础模块的封装中感受到了对c++的克制使用。因为kernel下基础模块的实现大多都是以c语言为主。这点大家要有一个心理准备。\n这里建议读者在阅读workflow，go-task源码时，以如下顺序阅读：\nExecQueue -&gt; ExecSession -&gt; Executor-&gt; ExecRequest -&gt; SubTask -&gt; __ExecManager -&gt; __WFGoTask -&gt; WFGoTask -&gt; SeriesWork\n正文下面直接以workflow给的gotask的示例作为本文的切入点：\n用法go-task的用法示例如下：\n#include &lt;stdio.h&gt;#include &lt;utility&gt;#include &quot;workflow/WFTaskFactory.h&quot;#include &quot;workflow/WFFacilities.h&quot;void add(int a, int b, int&amp; res) &#123;    res = a + b;&#125;int main(void) &#123;    WFFacilities::WaitGroup wait_group(1);    int a = 1;    int b = 1;    int res;    WFGoTask *task = WFTaskFactory::create_go_task(&quot;test&quot;, add, a, b, std::ref(res));   // cb1    task-&gt;set_callback([&amp;](WFGoTask *task) &#123;    // cb2        printf(&quot;%d + %d = %d\\n&quot;, a, b, res);        wait_group.done();    &#125;);     task-&gt;start();    wait_group.wait();    return 0;&#125;\n\n如果你有一定网络编程的基础，应该很容易看懂这段小daemo。我们可以这段代码猜测：\n第一行声明了一个WaitGroup变量，从后面的代码可以知道wait_group的作用是：阻塞主线程等待计算完成。在创建wait_group后，将计算过程add函数封装在一个回调函数（cb1）当中，cb1作为一个参数再来构造一个任务–WFGoTask，然后调用WFGoTask::set_callback函数又设置了一个回调函数（cb2），从代码上可以看到，该cb2的作用是：打印计算结果并通知主线程计算完毕。\n所以经过上面的分析，我们可以知道：\n\nWaitGroup的实现一定是基于条件变量&#x2F;信号量。\n\n作为WFGoTask构造参数cb1，一定某一时刻被线程池里面的某个线程给调用了，并且该线程在调用add函数返回之后，一定是直接或者间接调用了一下cb2。\n\n\n源码简析示例代码中create_go_task的第一个参数其实是kernel目录下的ExecQueue队列对应的队列名。ExecQueue具体的用法以及作用稍后讲解，只需知道它是一个队列即可。\ncreate_go_task实现很简单，它里面就是依赖一个全局的单例__ExecManager，通过这个单例拿到队列名对应的队列指针以及Executor对象。然后将队列和Executor对象作为__WFGoTask的构造参数，创建出了继承自WFGoTask的__WFGoTask对象。\n这里备注一下：__ExecManager单例管理从队列名到队列指针的映射。并且在__ExecManager初始化时，会创建一个Executor对象。\n目前为止，出现了几个新的类：ExecQueue、Executor、__WFGoTask。\n对于ExecQueue从kernel目录下可以看到它的源码，单纯就是一个链表，使用的还是linux原生链表。它的每一个节点都是ExecSessionEntry类型，如下定义：\nstruct ExecSessionEntry &#123;\tstruct list_head list;\tExecSession *session;\tthrdpool_t *thrdpool;&#125;;\n\n单独看ExecQueue、ExecSession、ExecSessionEntry的源码一定会蒙（我就是），所以这里直接讲解Executor的实现，前面的三个类就是被它所使用。\nvoid Executor::executor_thread_routine(void *context) &#123;\tExecQueue *queue = (ExecQueue *)context;\tstruct ExecSessionEntry *entry;\tExecSession *session;\tint empty;\tentry = list_entry(queue-&gt;session_list.next, struct ExecSessionEntry, list);\tpthread_mutex_lock(&amp;queue-&gt;mutex);\tlist_del(&amp;entry-&gt;list);\tempty = list_empty(&amp;queue-&gt;session_list);\tpthread_mutex_unlock(&amp;queue-&gt;mutex);\tsession = entry-&gt;session;\tif (!empty) &#123;\t\tstruct thrdpool_task task = &#123;\t\t\t.routine\t=\tExecutor::executor_thread_routine,\t\t\t.context\t=\tqueue\t\t&#125;;\t\t__thrdpool_schedule(&amp;task, entry, entry-&gt;thrdpool);\t&#125;\telse\t\tfree(entry);\tsession-&gt;execute();\tsession-&gt;handle(ES_STATE_FINISHED, 0);&#125;\n\n流程如下：\n\n从队列中取ExecSessionEntry。\n\n队列非空的话，将ExecSessionEntry中的session包装成thrdpool_task，并且将ExecSessionEntry的地址复用成线程池的__thrdpool_task_entry（PS：线程池在拿到__thrdpool_task_entry时用完后会自动free掉）。\n\n队列为非空的话，直接free掉ExecSessionEntry。\n\n最后执行ExecSession的execute、handle。\n\n\n这里的execute函数其实暗示着会调用cb1，handle其实就暗示里面会调用cb2。这下前后不就连起来了？（恍然大悟！）别着急，我们继续去剖析源码。\n细心的读者应该会发现这句代码没被放在锁里面：\nentry = list_entry(queue-&gt;session_list.next, struct ExecSessionEntry, list);\n\n为什么可以不放在锁里面？如果线程2，在线程1执行完list_del之前，拿到了同一个entry，这样不会有野指针的问题吗？\n这里放出我的猜测：Executor::executor_thread_routine本身就已经保证了一个时刻只会有一个线程访问队列头部。这个函数的执行逻辑是这样的：当前Executor::executor_thread_routine的回调是靠上一个Executor::executor_thread_routine回调访问完链表头部之后触发的，也即下一个队列头部访问的回调还得靠上一个回调来封装。这里其实有点并行任务串行化的味道了。\nstruct thrdpool_task task = &#123;    .routine\t=\tExecutor::executor_thread_routine,    .context\t=\tqueue&#125;;__thrdpool_schedule(&amp;task, entry, entry-&gt;thrdpool);\n\n最后是ExecQueue队列的start点，如下：\nint Executor::request(ExecSession *session, ExecQueue *queue) &#123;\tstruct ExecSessionEntry *entry;\tsession-&gt;queue = queue;\tentry = (struct ExecSessionEntry *)malloc(sizeof (struct ExecSessionEntry));\tif (entry) &#123;\t\tentry-&gt;session = session;\t\tentry-&gt;thrdpool = this-&gt;thrdpool;\t\tpthread_mutex_lock(&amp;queue-&gt;mutex);\t\tlist_add_tail(&amp;entry-&gt;list, &amp;queue-&gt;session_list);\t\tif (queue-&gt;session_list.next == &amp;entry-&gt;list) &#123;\t\t\tstruct thrdpool_task task = &#123;\t\t\t\t.routine\t=\tExecutor::executor_thread_routine,\t\t\t\t.context\t=\tqueue\t\t\t&#125;;\t\t\tif (thrdpool_schedule(&amp;task, this-&gt;thrdpool) &lt; 0) &#123;\t\t\t\tlist_del(&amp;entry-&gt;list);\t\t\t\tfree(entry);\t\t\t\tentry = NULL;\t\t\t&#125;\t\t&#125;\t\tpthread_mutex_unlock(&amp;queue-&gt;mutex);\t&#125;\treturn -!entry;&#125;\n\n从源码中可以看到，就是使用malloc分配一块内存，将session封装成ExecSessionEntry，然后将其添加到队列尾部，如果队列原来为空（意味着ExecQueue没有开始执行），就启动第一个Executor::executor_thread_routine，这样它会自动链式触发执行队列当中的每一个任务的回调。\n这里malloc分配的ExecSessionEntry由两个地方去释放：\n\n这里malloc分配的ExecSessionEntry会被复用为线程池的__thrdpool_task_entry，最后被线程池调用free释放掉。\n\n在函数Executor::executor_thread_routine中，由ExecQueue最后一个任务调用free释放。\n\n\n从这里可以看到，workflow针对内存的释放也是极其晦涩（反正我在阅读源码时就是这样感觉）。为了性能，根本没使用智能指针，完全靠malloc和free。内存池也没有，这点我是无法理解的。\n经过上面的分析我们了解了ExecSession、ExecQueue、Executor的作用，接下来我们分析一下，__WFGoTask是怎么使用这些类的。\n从本段开头了解到ExecQueue、Executor是作为__WFGoTask的构造参数，所以下面我们以__WFGoTask为主去看看它是怎么实现的\nclass __WFGoTask : public WFGoTask &#123;    // ...protected:\tvirtual void execute() &#123;\t\tthis-&gt;go();\t&#125;protected:\tstd::function&lt;void ()&gt; go;public:\t__WFGoTask(ExecQueue *queue, Executor *executor,\t\t\t   std::function&lt;void ()&gt;&amp;&amp; func) :\t\tWFGoTask(queue, executor),\t\tgo(std::move(func)) &#123; /* ... */ &#125;&#125;;\n\n使用了virtual关键字声明的execute函数！，并且调用了go也即cb1！（衔接起来了！）\n继续看它基类的实现：\nclass WFGoTask : public ExecRequest &#123;public:\tvoid start() &#123;\t\tassert(!series_of(this));\t\tWorkflow::start_series_work(this, nullptr);\t&#125;public:\tvoid *user_data;public:\tvoid set_callback(std::function&lt;void (WFGoTask *)&gt; cb) &#123;\t\tthis-&gt;callback = std::move(cb);\t&#125;protected:\tvirtual SubTask *done() &#123;\t\tSeriesWork *series = series_of(this);\t\tif (this-&gt;callback)\t\t\tthis-&gt;callback(this);\t\tdelete this;\t\treturn series-&gt;pop();\t&#125;protected:\tstd::function&lt;void (WFGoTask *)&gt; callback;public:\tWFGoTask(ExecQueue *queue, Executor *executor) :\t\tExecRequest(queue, executor) &#123; /* ... */ &#125;&#125;;\n\nWFGoTask::start()正是示例当中调用的start函数，set_callback正是设置的cb2回调。我可以明确的说，start_series_work会创建一个SeriesWork对象，并且将SeriesWork对象的指针赋值给WFGoTask祖父类SubTask的user_data成员，并且SeriesWork其实也是一个队列，它是串行队列，队列当中的任务是有先后执行顺序的。这里串行队列的设计是为特定的有先后依赖顺序的计算场景所设计的。\n深入查看ExecRequest的实现：\nclass ExecRequest : public SubTask, public ExecSession &#123;public:\tExecRequest(ExecQueue *queue, Executor *executor) &#123; /* ... */ &#125;public:\tvirtual void dispatch() &#123;\t\tif (this-&gt;executor-&gt;request(this, this-&gt;queue) &lt; 0)\t\t\tthis-&gt;handle(ES_STATE_ERROR, errno);\t&#125;protected:\tExecQueue *queue;\tExecutor *executor;protected:\tvirtual void handle(int state, int error) &#123;\t\tthis-&gt;state = state;\t\tthis-&gt;error = error;\t\tthis-&gt;subtask_done();\t&#125;&#125;;\n\nSubTask类和ExecSession类非常简单，由于篇幅有限这只列出我们关心的函数。\nSubTask有三个关键函数：\n虚函数：dispatch、done\n普通成员函数：subtask_done。\n而\nSubTask::dispatch 最终被重写为：ExecRequest::dispatch\nSubTask::done 最终被重写为：WFGoTask::done\n其中subtask_done实现如下：\nvoid SubTask::subtask_done() &#123;\tSubTask *cur = this;\twhile (1) &#123;\t\tcur = cur-&gt;done();\t\tif (cur) &#123;\t\t\tcur-&gt;dispatch();\t\t&#125;        /* ... */\t\tbreak;\t&#125;&#125;\n\ndone的实现落实到了WFGoTask::done上，作用是销毁当前的task对象并且返回串行队列当中的下一个task，然后由subtask_done调用ExecRequest::dispatch将task挂到ExecQueue的链表上等待线程池的消费。\nExecSession有两个我们比较关心的纯虚函数：execute、handle。这两函数一路继承体系下来最终分别被重写为__WFGoTask::execute和ExecRequest::handle。\n所以在Executor::executor_thread_routine函数中调用的execute、handle函数最终被重写为：__WFGoTask::execute、ExecRequest::handle()。\n最后总结一下go-task执行的流程：\n\n构造一个go-task对象 &amp;&amp; 调用start函数。\n\nstart函数会new一个first为go-task，last为nullptr的SeriesWork对象 &amp;&amp; 调用first的dispatch也即ExecRequest::dispatch。\n\nexecutor的request函数，将go-task挂到ExecQueue链表的尾部上，由线程池去消费。当然，如果ExecQueue原来是为空的，就创建第一个Executor::executor_thread_routine。\n\nExecutor::executor_thread_routine会链式触发让线程池处理ExecQueue每一个任务。\n\n调用任务的__WFGoTask::execute。\n\n调用任务的ExecRequest::handle。\n\n调用SubTask::subtask_done &amp;&amp; （如果存在的话）调用SeriesWork对象的下一个task的dispatch（PS，可能不是ExecRequest::dispatch这个重载函数）\n\n调用WFGoTask::done。删除当前task对象并且返回串行队列的下一个串行任务。\n\n\n最后要还要提醒的一句是：Executor::executor_thread_routine在向ExecQueue的链表取任务时是保证非并发的，但是在执行任务的execute时，是有可能是并发执行的！ 有人可能会注意到那为什么在向链表取任务时要加锁？因为这把锁可能防止Executor::executor_thread_routine和Executor::request之间的竞争问题，而Executor::executor_thread_routine和Executor::executor_thread_routine之间并不存在竞争问题。\n\n本章完结\n","tags":["高性能服务器框架"]},{"title":"这才是计科之 Onix & XV6 源码分析（1、XV6-x86的启动）","url":"/2024/06/07/xv6/Boot/","content":"前言Onix是一款相对于XV6来说功能更为健全的单核OS，由于功能更加完善，Onix也更加复杂。代码阅读起来会比较绕。\nXV6是一款简单易读的多核操作系统，但其功能相对Onix来说更为简陋，比如Onix对物理内存的管理采用了位图、内核内存和进程相关的内存进行了分开管理，页目录使用单独的内核内存，没有和页表、页框混用等。而XV6显得非常简陋。尽管XV6的实验可以弥补部分缺陷。\nOnix操作系统也实现了bootloader，对于将OS加载到内存的操作，Onix是采用汇编进行内核加载，并且在加载内核前，还会进行一个内存探测的操作，所以Onix的bootloader稍微有些复杂。而XV6操作系统的启动操作写的非常简洁，加载内核的操作采用的是C语言的形式，非常利于阅读学习，但是XV6不会进行内存探测。为求方便，本文主要叙述XV6的启动流程。\nOnix相关链接：\n\ngithub仓库链接\n\nB站配套视频链接\n\n\nXV6-x86的github链接：\n\n链接\n\n\n\nMakefile &amp; kernel.ld文件的分析首先科普一个概念 —— 引导扇区\n引导扇区：是计算机启动过程中 BIOS 读取的第一个磁盘扇区（通常为 512 字节），用于加载操作系统或更复杂的引导程序（如 GRUB）。\nBISO规定引导扇区有如下要求：\n\n引导扇区的代码被加载到内存的0x7c00位置执行\n引导扇区有且仅有512字节。\n引导扇区的魔数在最后两个字节的位置，值0x55aa。\n\n在叙述os启动前，必须要了解其Makefile是怎么写的。同时，在了解bootmain从镜像中加载os的代码到内存中前，因为os是elf格式，所以我们需要了解os的link脚本是怎么布局的。以便我们能更好掌握os的内存布局。\nMakefile这里贴出makefile比较关键的代码：\n# 利用dd命令制作OS镜像，依赖bootblock和kernel，# 首先划分了10000个扇区# 然后将bootblock写到了第0号扇区# 最后从1号扇区开始，填入OS的代码文件。xv6.img: bootblock kernel\tdd if=/dev/zero of=xv6.img count=10000\tdd if=bootblock of=xv6.img conv=notrunc\tdd if=kernel of=xv6.img seek=1 conv=notrunc# 单独产生os的bootloader模块，并且该模块是使用$(OBJCOPY)产生，# 所以没有elf文件头信息，只是单纯的二进制可执行文件。并且$(LD)规定# 代码的入口点是start、并且从地址0x7C00（物理地址）开始，最终的文件名是bootblock# 最后一步，调用pl脚本对引导扇区进行签名，在512字节引导扇区最后两字节写入0x55aabootblock: bootasm.S bootmain.c\t$(CC) $(CFLAGS) -fno-pic -O -nostdinc -I. -c bootmain.c\t$(CC) $(CFLAGS) -fno-pic -nostdinc -I. -c bootasm.S\t$(LD) $(LDFLAGS) -N -e start -Ttext 0x7C00 -o bootblock.o bootasm.o bootmain.o\t$(OBJDUMP) -S bootblock.o &gt; bootblock.asm\t$(OBJCOPY) -S -O binary -j .text bootblock.o bootblock\t./sign.pl bootblock# 产生AP cpu的boot代码# 指定程序的加载地址是0x7000（物理地址）# 可执行代码的格式和bootblock相同，纯粹的二进制程序，没有elf的头部信息# 但最终输出的entryother会和kernel合并entryother: entryother.S\t$(CC) $(CFLAGS) -fno-pic -nostdinc -I. -c entryother.S\t$(LD) $(LDFLAGS) -N -e start -Ttext 0x7000 -o bootblockother.o entryother.o\t$(OBJCOPY) -S -O binary -j .text bootblockother.o entryother\t$(OBJDUMP) -S bootblockother.o &gt; entryother.asm# 产生第一个进程，init进程的boot代码# 同样以$(OBJCOPY)提取纯粹的可执行代码，没有elf头部信息，输出文件名为initcode# 指定程序的加载地址是0（虚拟地址）initcode: initcode.S\t$(CC) $(CFLAGS) -nostdinc -I. -c initcode.S\t$(LD) $(LDFLAGS) -N -e start -Ttext 0 -o initcode.out initcode.o\t$(OBJCOPY) -S -O binary initcode.out initcode\t$(OBJDUMP) -S initcode.o &gt; initcode.asm# 产生内核的elf可执行文件kernel: $(OBJS) entry.o entryother initcode kernel.ld\t$(LD) $(LDFLAGS) -T kernel.ld -o kernel entry.o $(OBJS) -b binary initcode entryother\t$(OBJDUMP) -S kernel &gt; kernel.asm\t$(OBJDUMP) -t kernel | sed &#x27;1,/SYMBOL TABLE/d; s/ .* / /; /^$$/d&#x27; &gt; kernel.sym\n\n这里编译器、链接器的选项具体作用读者可以自行百度，这里只阐述比较关键的部分。\n首先是利用dd命令制作xv6.img镜像，从代码中可以很清楚的看到，bootblock填到了第0号扇区、kernel填到了1号以及之后的扇区。bootblock作用就是使cpu从实时模式转换为保护模式，然后将kernel从磁盘上加载到内存。这里要注意一个特殊的数字0x7C00，它出现在生成bootblock二进制文件的$(LD)阶段，这里暗示了bootblock代码在加载进内存时应该被放在0x7C00的位置。事实也是如此，在BIOS完成硬件初始化后，就会将第0号扇区（一个扇区一般就是512字节）的512字节代码加载到内存的0x7C00的位置，然后BIOS就会让eip指向0x7C00的位置，去执行bootasm.S里面的汇编代码。\n这里有个关键点：在使用$(LD)命令生成bootblock.o时，命令参数部分bootasm.o放在bootmain.o前面，会导致链接时，bootasm.o代码就会靠前，这样在eip执行0x7C00位置的代码时，一定是从start开始。\nbootblock是由$(OBJCOPY)生成，$(OBJCOPY)的作用就是去除elf文件中的各种头部，因为BIOS只负责从第0号扇区加载bootblock，不会解析elf文件，所以，$(OBJCOPY)去提取纯粹的二进制是非常有必要的！\nbootblock具体细节下面会详细探讨\nkernel.ld这里贴出kernel.ld比较关键的代码：\nOUTPUT_FORMAT(&quot;elf32-i386&quot;, &quot;elf32-i386&quot;, &quot;elf32-i386&quot;)OUTPUT_ARCH(i386)ENTRY(_start)SECTIONS&#123;\t/* Link the kernel at this address: &quot;.&quot; means the current address */        /* Must be equal to KERNLINK */\t. = 0x80100000; // 定义代码起始的虚拟地址\t.text : AT(0x100000) &#123;  // 定义了起始加载地址\t\t*(.text .stub .text.* .gnu.linkonce.t.*)\t&#125;  /*    省略...  */&#125;\n\n从kernel的链接脚本我们可以看到，kernel的起始虚拟内存地址是0x8010 0000，内核实际加载的物理地址是x100000，由AT定义。这里我反复标注了 虚拟地址 &#x2F; 物理地址 ，这两者一定要分清！\n从实时模式到保护模式本段代码实现位于xv6的bootasm.S文件。具体细节如下，xv6使用的AT&amp;T的汇编，还是比较好懂的，读者有疑问的话，可以百度去搜相关指令的作用。英文注释已经非常详细，我就直接引用了。\n#include &quot;asm.h&quot;#include &quot;memlayout.h&quot;#include &quot;mmu.h&quot;# Start the first CPU: switch to 32-bit protected mode, jump into C.# The BIOS loads this code from the first sector of the hard disk into# memory at physical address 0x7c00 and starts executing in real mode# with %cs=0 %ip=7c00..code16                       # Assemble for 16-bit mode.globl startstart:  cli                         # BIOS enabled interrupts; disable  # Zero data segment registers DS, ES, and SS.  xorw    %ax,%ax             # Set %ax to zero  movw    %ax,%ds             # -&gt; Data Segment  movw    %ax,%es             # -&gt; Extra Segment  movw    %ax,%ss             # -&gt; Stack Segment  # 硬件相关，主线是OS的启动，该部分不深究也没影响，其实就是一个固定步骤。  # Physical address line A20 is tied to zero so that the first PCs   # with 2 MB would run software that assumed 1 MB.  Undo that.seta20.1:  inb     $0x64,%al               # Wait for not busy  testb   $0x2,%al  jnz     seta20.1  movb    $0xd1,%al               # 0xd1 -&gt; port 0x64  outb    %al,$0x64seta20.2:  inb     $0x64,%al               # Wait for not busy  testb   $0x2,%al  jnz     seta20.2  movb    $0xdf,%al               # 0xdf -&gt; port 0x60  outb    %al,$0x60  # Switch from real to protected mode.  Use a bootstrap GDT that makes  # virtual addresses map directly to physical addresses so that the  # effective memory map doesn&#x27;t change during the transition.  lgdt    gdtdesc  movl    %cr0, %eax  orl     $CR0_PE, %eax  movl    %eax, %cr0 ########################### 以下正式进入32位保护模式//PAGEBREAK!  # Complete the transition to 32-bit protected mode by using a long jmp  # to reload %cs and %eip.  The segment descriptors are set up with no  # translation, so that the mapping is still the identity mapping.  ljmp    $(SEG_KCODE&lt;&lt;3), $start32.code32  # Tell assembler to generate 32-bit code now.start32:  # Set up the protected-mode data segment registers  movw    $(SEG_KDATA&lt;&lt;3), %ax    # Our data segment selector  movw    %ax, %ds                # -&gt; DS: Data Segment  movw    %ax, %es                # -&gt; ES: Extra Segment  movw    %ax, %ss                # -&gt; SS: Stack Segment  movw    $0, %ax                 # Zero segments not ready for use  movw    %ax, %fs                # -&gt; FS  movw    %ax, %gs                # -&gt; GS  # Set up the stack pointer and call into C.  movl    $start, %esp            # 这里将esp栈设置到了start，由于栈向低地址处增长，所以刚好和bootasm文件的代码背道而驰。  call    bootmain  # If bootmain returns (it shouldn&#x27;t), trigger a Bochs  # breakpoint if running under Bochs, then loop.  movw    $0x8a00, %ax            # 0x8a00 -&gt; port 0x8a00  movw    %ax, %dx  outw    %ax, %dx  movw    $0x8ae0, %ax            # 0x8ae0 -&gt; port 0x8a00  outw    %ax, %dxspin:  jmp     spin# Bootstrap GDT.p2align 2                                # force 4 byte alignmentgdt:  SEG_NULLASM                             # null seg  SEG_ASM(STA_X|STA_R, 0x0, 0xffffffff)   # code seg  SEG_ASM(STA_W, 0x0, 0xffffffff)         # data seggdtdesc:  .word   (gdtdesc - gdt - 1)             # sizeof(gdt) - 1  .long   gdt                             # address gdt\n\n这里先普及一下实时模式和保护模式的区别：\n\n实时模式：为兼容以前的PC。特点是：寄存器都是16位。寻址方式：16位的段寄存器 + 16位的偏移寄存器，最大寻址范围是20位。\n\n保护模式：现代CPU的寻址方式。特点是：寄存器有16位、32为、64位。寻址方式：段描述符 + 32位偏移寄存器。最大寻址范围4G+。\n\n\nbootasm.S是操作系统被加载内存前，最先开始执行的代码。BIOS是运行在实时模式下的程序，只拥有1M的寻址空间（boot代码被加载到0x7C00 &lt; 1M 就能证明存在1M的限制），所以在cpu拥有4G寻址空间前，还需要进行一些初始化操作，\n从实时模式 -&gt; 保护模式的转变流程非常固定，在xv6的bootasm中实现如下：\n\n对于多核处理器，最先启动的CPU（我们称为BSP（bootstrap processor）），BSP一上电就是实时模式。其余的从处理器（我们称为AP）在后面的内核初始化阶段会被BSP依次唤醒并初始化。\n\n关中断。清空各种段寄存器，包括ds、es、ss。\n\n打开A20地址线.\n\n加载全局描述符表。即使用lgdt指令将表的地址和大小放在GDTR中。（这里的全局描述符表是临时的，后面内核初始化会更换一张gdt表，那张表更加完善。\n\n将CR0寄存器第0位设置为1（PE位），此时正式转换成保护模式。\n\n使用ljmp（长跳转指令）指令刷新段寄存器，跳到start32。\n # xv6对ljmp的注释如下：# Complete the transition to 32-bit protected mode by using a long jmp# to reload %cs and %eip. \n\n初始化数据段、栈段寄存器，将esp设置到0x7C00处，跳到bootmain函数中，该函数会将XV6 OS加载到内存。\n\n\n相关的结构如下：\n段描述符（Descriptor），描述一段地址的特性，包括基地址、范围、权限、粒度（范围以字节为单位还是以4K为单位）、类型（代码&#x2F;数据）等信息：\n\n全局描述符表，由许多个8字节段描述符组成的表：\n\n全局描述符表寄存器，其基地址的内容是全局描述符表的首地址，界限是全局描述符表的大小：\n\n段选择子，【代码、数据、栈等】段使用了哪个段描述符，索引号指使用段描述符在全局描述符表的偏移（8字节为单位），第2位指明是全局描述符还是局部描述符，0~1位指示段的特权级：\n\n从低特权级的段空间 跳到 高特权级的段空间就会发生cpu特权级的切换，cpu就是通过全局描述符表来确定一个段的特权级。最典型的就是用户进程调用系统调用产生的特权级切换，这中间涉及查tss段、切栈等复杂操作，我们后面在进行详细的讨论。\n对于GDT的详细描述，这里推荐两篇博客，这两篇博客写的真的非常好！相信阅读之后，对全局描述符会有一个清晰的认识，全局描述符的几幅图片也是取自这两篇文章，如有侵权，可告知删除：\n详细介绍了段描述符各个位的作用\n可以作为扩展，里面介绍了局部描述符的作用\n正式将XV6加载到内存分析该部分细节前，先需要了解一下elf文件的格式，我们重点关注elf文件的ELF header和Program header table。直接上各个字段的描述：\n引用自博客https://zhuanlan.zhihu.com/p/165336511如有侵权，可告知删除：\n#define ELF_MAGIC 0x464C457FU  // &quot;\\x7FELF&quot; in little endian// ELF 文件的头部struct elfhdr &#123;  uint magic;       // 4 字节，为 0x464C457FU（大端模式）或 0x7felf（小端模式）                      // 表明该文件是个 ELF 格式文件  uchar elf[12];    // 12 字节，每字节对应意义如下：                    //     0 : 1 = 32 位程序；2 = 64 位程序                    //     1 : 数据编码方式，0 = 无效；1 = 小端模式；2 = 大端模式                    //     2 : 只是版本，固定为 0x1                    //     3 : 目标操作系统架构                    //     4 : 目标操作系统版本                    //     5 ~ 11 : 固定为 0  ushort type;      // 2 字节，表明该文件类型，意义如下：                    //     0x0 : 未知目标文件格式                    //     0x1 : 可重定位文件                    //     0x2 : 可执行文件                    //     0x3 : 共享目标文件                    //     0x4 : 转储文件                    //     0xff00 : 特定处理器文件                    //     0xffff : 特定处理器文件  ushort machine;   // 2 字节，表明运行该程序需要的计算机体系架构，                    // 这里我们只需要知道 0x0 为未指定；0x3 为 x86 架构  uint version;     // 4 字节，表示该文件的版本号  uint entry;       // 4 字节，该文件的入口地址，没有入口（非可执行文件）则为 0  uint phoff;       // 4 字节，表示该文件的“程序头部表”相对于文件的位置，单位是字节  uint shoff;       // 4 字节，表示该文件的“节区头部表”相对于文件的位置，单位是字节  uint flags;       // 4 字节，特定处理器标志  ushort ehsize;    // 2 字节，ELF文件头部的大小，单位是字节  ushort phentsize; // 2 字节，表示程序头部表中一个入口的大小，单位是字节  ushort phnum;     // 2 字节，表示程序头部表的入口个数，                    // phnum * phentsize = 程序头部表大小（单位是字节）  ushort shentsize; // 2 字节，节区头部表入口大小，单位是字节  ushort shnum;     // 2 字节，节区头部表入口个数，                    // shnum * shentsize = 节区头部表大小（单位是字节）  ushort shstrndx;  // 2 字节，表示字符表相关入口的节区头部表索引&#125;;// 程序头表struct proghdr &#123;  uint type;        // 4 字节， 段类型                    //         1 PT_LOAD : 可载入的段                    //         2 PT_DYNAMIC : 动态链接信息                    //         3 PT_INTERP : 指定要作为解释程序调用的以空字符结尾的路径名的位置和大小                    //         4 PT_NOTE : 指定辅助信息的位置和大小                    //         5 PT_SHLIB : 保留类型，但具有未指定的语义                    //         6 PT_PHDR : 指定程序头表在文件及程序内存映像中的位置和大小                    //         7 PT_TLS : 指定线程局部存储模板  uint off;         // 4 字节， 段的第一个字节在文件中的偏移  uint vaddr;       // 4 字节， 段的第一个字节在内存中的虚拟地址  uint paddr;       // 4 字节， 段的第一个字节在内存中的物理地址(适用于物理内存定位型的系统)  uint filesz;      // 4 字节， 段在文件中的长度  uint memsz;       // 4 字节， 段在内存中的长度  uint flags;       // 4 字节， 段标志                    //         1 : 可执行                    //         2 : 可写入                    //         4 : 可读取  uint align;       // 4 字节， 段在文件及内存中如何对齐&#125;;\n\n如果感兴趣的话，可以参考文章：ELF文件格式的详解，这篇文章讲解的更为详细，但是对于现阶段来说，可以不用了解太仔细，知道elf文件的ELF header和Program header table各个字段的作用就足够你继续学习XV6操作系统。\n对kernel elf文件解析的主流程：\nvoidbootmain(void)&#123;  struct elfhdr *elf;  struct proghdr *ph, *eph;  void (*entry)(void);  uchar* pa;  // 预留足够空间  // 46K  elf = (struct elfhdr*)0x10000;  // scratch space  // 以0为偏移读4096个字节，读elf文件头  // Read 1st page off disk  readseg((uchar*)elf, 4096, 0);  // 判断elf文件魔数。  // Is this an ELF executable?  if(elf-&gt;magic != ELF_MAGIC)    return;  // let bootasm.S handle error  // 定位到Program header table  // Load each program segment (ignores ph flags).  ph = (struct proghdr*)((uchar*)elf + elf-&gt;phoff);  // 程序头部表个数  eph = ph + elf-&gt;phnum;  // 一个段一个段的读  for(; ph &lt; eph; ph++)&#123;  // 以struct proghdr为单位自增。    // 应该加载到的物理内存，xv6中是0x100000    pa = (uchar*)ph-&gt;paddr;    // 读取整个段到pa中    readseg(pa, ph-&gt;filesz, ph-&gt;off);    if(ph-&gt;memsz &gt; ph-&gt;filesz)      // mem大小比file大小大，多余的补零      stosb(pa + ph-&gt;filesz, 0, ph-&gt;memsz - ph-&gt;filesz);  &#125;  // Call the entry point from the ELF header.  // Does not return!  entry = (void(*)(void))(elf-&gt;entry);  // 在xv6的kernel.ld中描述为_start  entry();&#125;\n\n前半部分做的操作就是不断从kernel的elf文件中按照程序头将各个段读取到内存。后面通过elf头中的entry，该字段保存内核入口点_start（这可以通过阅读kernel.ld文件来证明），去执行_start开始的代码，这里要提醒读者的是，到目前为止，对cpu来说状态是：保护模式 &amp; 已经装填了全局描述符 &amp; 一切地址皆是物理地址。而kernel的elf文件中代码都是使用的（0x8010 0000开始的）虚拟地址，所以我们后面在entry.S中会看到，在给_start赋值前，会通过一个宏将真正的入口地址的虚拟地址转换为XV6加载到内存的物理地址。从而我们在物理地址寻址模式下，可以利用_start准确跳转到xv6的入口代码。\n需要注意的是：在执行bootasm汇编代码时，esp的位置是不确定的，唯一能确定的是esp在1M空间之内。在bootasm汇编最后才会将esp挪到0x7c00的位置！\n在bootmain执行完毕后，内存布局如下：\n\n接下来深入分析一下从磁盘读取文件的细节，这些内容都是和硬件相关的，通过操作硬件寄存器来实现读写磁盘，这部分深入下去也是挺让人头大的，作者也是菜鸟一个，也就力所能及的叙述一些自己明白的东西吧。\nvoidwaitdisk(void)&#123;  // Wait for disk ready.  while((inb(0x1F7) &amp; 0xC0) != 0x40)    ;&#125;// readsect也引用自https://zhuanlan.zhihu.com/p/165336511，如有侵权，可联系我删除// Read a single sector at offset into dst.voidreadsect(void *dst, uint offset)&#123;  // Issue command.  waitdisk();  outb(0x1F2, 1);   // count = 1          // 要读取的扇区数量 count = 1  outb(0x1F3, offset);                    // 扇区 LBA 地址的 0-7 位  outb(0x1F4, offset &gt;&gt; 8);               // 扇区 LBA 地址的 8-15 位  outb(0x1F5, offset &gt;&gt; 16);              // 扇区 LBA 地址的 16-23 位  outb(0x1F6, (offset &gt;&gt; 24) | 0xE0);     // offset | 11100000 保证高三位恒为 1                                          //         第7位     恒为1                                          //         第6位     LBA模式的开关，置1为LBA模式                                          //         第5位     恒为1                                          //         第4位     为0代表主硬盘、为1代表从硬盘                                          //         第3~0位   扇区 LBA 地址的 24-27 位  outb(0x1F7, 0x20);  // cmd 0x20 - read sectors  // 20h为读，30h为写  // Read data.  waitdisk();  insl(0x1F0, dst, SECTSIZE/4); // 读的时候以4字节位单位读，所以需要扇区除以4，代表要读的次数&#125;// Read &#x27;count&#x27; bytes at &#x27;offset&#x27; from kernel into physical address &#x27;pa&#x27;.// Might copy more than asked.voidreadseg(uchar* pa, uint count, uint offset)&#123;  uchar* epa; // end phy addr  epa = pa + count; // 结束地址  // 这里将起始物理地址回退了offset的SECTSIZE（512）余数个Byte  // 因为在从磁盘上读数据的时候，以512字节进行读取。所以offset会以512为单位向下取整，  // 被换算成扇区号的偏移。如果offset原来不是SECTSIZE的整数倍，向下取整会导致offset截断，  // 记换算后的offset为ofs，此时如果直接读取ofs扇区会多读offset % SECTSIZE个字节，  // 所以需要提前将pa的地址减去offset % SECTSIZE个字节来排除offset被截断的多余的字节。  // Round down to sector boundary.  pa -= offset % SECTSIZE;      // 将offset换算成扇区号  // Translate from bytes to sectors; kernel starts at sector 1.  offset = (offset / SECTSIZE) + 1;   // 依次读取每个扇区，最后一个扇区多读了也没关系。  for(; pa &lt; epa; pa += SECTSIZE, offset++)    readsect(pa, offset);&#125;\n\n仔细推敲readseg函数中pa回滚的操作，总感觉这样贸然回滚，会覆盖之前已经被加载到内存中的代码。既然xv6敢这样写，八成说明是没有问题的，具体的原因，作者实在琢磨不透，如果有了解的朋友，可以在评论区交流一下。\n跳到_start，正式进入内核前的预初始化相关代码文件是entry.S\n#include &quot;asm.h&quot;#include &quot;memlayout.h&quot;#include &quot;mmu.h&quot;#include &quot;param.h&quot;# Multiboot header.  Data to direct multiboot loader..p2align 2.text.globl multiboot_headermultiboot_header:  #define magic 0x1badb002  #define flags 0  .long magic  .long flags  .long (-magic-flags)  # 这里就能看到，因为elf描述的一些标签是使用的虚拟地址，# 而在进入entry开启分页前都是使用的物理地址，所以使用了# V2P_WO宏将entry转换成了物理地址。方便bootmain跳转到entry.# V2P_WO展开就是将输入的地址减去 0x8000 0000 的偏移。.globl _start_start = V2P_WO(entry)# Entering xv6 on boot processor, with paging off..globl entryentry:  # 打开4M big page开关  # Turn on page size extension for 4Mbyte pages  movl    %cr4, %eax  orl     $(CR4_PSE), %eax  movl    %eax, %cr4  # 将页目录设置为entrypgdir，同样由于在启用虚拟内存前，  # 需要将虚拟地址entrypgdir转换为物理地址，有关entrypgdir  # 的定义在内存管理章节进行详细叙述。  # Set page directory  movl    $(V2P_WO(entrypgdir)), %eax  movl    %eax, %cr3  # 开启分页  # Turn on paging.  movl    %cr0, %eax  orl     $(CR0_PG|CR0_WP), %eax  movl    %eax, %cr0  ########################### 以下正式进入分页模式，地址皆是虚拟地址  # 再一次修改esp指针，将esp移到内核代码范围中  # Set up the stack pointer.  movl $(stack + KSTACKSIZE), %esp  # 真正进入内核main函数，开始各种初始化。  mov $main, %eax  jmp *%eax.comm stack, KSTACKSIZE\n\n该部分代码负责进入main函数前的初始化，主要工作如下：\n\n打开4M big page分页开关，让cpu支持4M大页。entrypgdir会将内核区域与映射为物理地址低4M的大页，我们后面会详细进行讨论，entrypgdir的生命周期非常短，在main函数中初始化过程中，会另外产生一个粒度更小的页表kpgdir（4K为一页），该页表会一直作为xv6的内核页表。\n\n设置entrypgdir为BSP（booststrap processor）的页目录。\n\n开启分页。\n\n修改esp。指向内核自己分配的4K大小的栈上。\n\n进入main。\n\n\n在entry.S代码执行完后，cpu开启分页模式，所有的地址都将以虚拟地址的形式存在。此时内存布局如下：\n\n至此，cpu的预初始化进行完毕。下面几章将围绕内核的初始化去讲解类unix操作系统的内存管理、进程调度、文件系统子模块。\n\n本章完结\n","tags":["类Unix源码剖析"]},{"title":"这才是计科之 Onix & XV6 源码分析（4、文件系统）","url":"/2025/09/28/xv6/FileSystem/","content":"前言拖了这么久，终于到了最后一篇关于类unix源码分析的文章————文件系统。\n（偷偷告诉你，本篇博客起草于我即将从毕业以来第一家公司离职前一个星期（2025-9-16），忍着牙痛ing ing ing）。\n如你所见，文件系统完全值得另开一篇博文单独去记录它。这里为了方便，同样将Onix的文件系统和XV6的文件系统放一块进行记录。并且，Onix的文件系统确实更加规范和完善，而XV6的文件系统相对来说更加简洁。但值得一提的是，XV6文件系统实现了一个简单而确实又很重要的东西————基于写时日志的事务系统。这是Onix所没有的。\nlinux的一切皆文件的伟大思想，将所有的IO行为（包括IO设备读写、pipe读写、进程读写）都抽象成文件。在本篇文章当中，我们将通过两个小型操作系统源码一一揭开linux文件系统的面纱。\n\n\n首先贴上XV6文件系统的模块方图：\n\n其次是Onix文件系统的模块方图：\n\n诈一看，你一定会认为XV6实现的文件系统比Onix的更复杂，实际上恰恰相反。尽管XV6实现了更多的模块，但是因为它教学系统的本质，每个模块的实现都以简单且精简为目的，没有为扩展性做任何多余操作。： ）读过的人表示非常干脆利落。\n反观Onix，为了适配 终端、串口、键盘、磁盘、软盘以及以后可能有的各种IO设备，根据他们的特点，抽象出了 struct device_t 结构体，分离出虚拟设备抽象层，将共有的io操作提取出来。为了统一文件系统的管理，抽象出了 fs_op_t 结构体。更贴近真实Linux操作系统的做法。具体细节将逐一解释。\n依据我阅读源码的习惯，下面就来自底向上逐步介绍文件系统的实现。以Onix文件系统为主。\n磁盘IO驱动层 &amp; 虚拟设备实现层\n磁盘IO驱动层: 一句话概括该模块的作用：所谓磁盘IO驱动层，实际上就是操作系统当中，最底层的直接和磁盘设备打交道的代码。使用厂商规定的指令对磁盘设备按一定规则进行读写操作。\n\n虚拟设备实现层: 对IO设备抽象出统一接口，并且针对磁盘设备访问，Onix还实现了经典的电梯算法。XV6当中虚拟设备实现层过于简单，这里以Onix为主进行深入讲解。\n\n\n对于XV6XV6的磁盘IO驱动层可以分为两块：IO请求队列 和 磁盘IO驱动。实现可以参考文件：ide.c，IO请求队列的实现非常巧妙，其思想为：多个进程需要并发对一块磁盘进行IO，为了保证正确性，我们必须对并行操作串行化处理。让请求IO的进程入队。仅让位于队头的进行执行磁盘IO，其他进程休眠等待。当队头的进程执行完IO请求后出队列唤醒进程，下一个队头再进行IO操作。 如果你有看过LevelDB的源码，应该对这种思想非常熟悉，因为LevelDB的Write方法实现运用了同样的手法。参考：LevelDB源码阅读笔记（1、整体架构）\n框图图如下：\n\n流程如下：\n首先对外暴露iderw接口。\niderw函数的逻辑：\n\n\n上层调用对外暴露的磁盘操作接口 iderw(struct buf *b)。\n将请求push到IO队列尾部。\n如果本身就是队头，直接执行IO函数 idestart。\n非队头，阻塞直到 IO任务 被中断处理函数消费。\n\n\n磁盘中断处理函数ideintr的逻辑：（一旦磁盘准备好，就会触发中断）\n\n\nIO队列为空直接返回。\n移除并唤醒队头。\n如果IO队列还非空，为 IO任务 执行IO函数 idestart。\n\n\n实现了简单的先来先服务(FCFS)调度\n对于Onix而Onix的磁盘IO驱动极其复杂，需要深入了解 IDE（Integrated Drive Electronics）硬盘控制器（IDE是一种常见的硬盘接口标准，也称为ATA（Advanced Technology Attachment））。感兴趣的可以先阅读一下XV6当中idestart函数，该函数使用几行代码实现了对磁盘设备的读写操作，然后深入去阅读Onix的磁盘IO驱动，代码路径：kernel&#x2F;ide.c，我的重心在文件系统的架构，所以Onix最底层的磁盘IO驱动就跳过了。笔者重点要讲的是Onix的虚拟设备实现层。\nOnix对虚拟设备的实现定义了如下虚拟设备控制结构体：\ntypedef struct device_t&#123;    char name[NAMELEN];  // 设备名    int type;            // 设备类型    int subtype;         // 设备子类型    dev_t dev;           // 设备号    dev_t parent;        // 父设备号    void *ptr;           // 设备指针    list_t request_list; // 块设备请求链表    bool direct;         // 磁盘寻道方向    // 设备控制    int (*ioctl)(void *dev, int cmd, void *args, int flags);    // 读设备    int (*read)(void *dev, void *buf, size_t count, idx_t idx, int flags);    // 写设备    int (*write)(void *dev, void *buf, size_t count, idx_t idx, int flags);&#125; device_t;\n\n针对IO设备，抽象出了ioctl、read、write接口，这样，每种IO设备只需根据实际情况实现回调函数，任何IO设备都能被实例化成该结构体，并且我们即将讲解的minix（onix文件系统类型）文件系统也能更好的对IO设备做适配。\nOnix当中对IO设备的设备类型按读取的粒度分成了两类：字符设备（单字节）和块设备（1024为一快）。\n// 设备类型enum device_type_t&#123;    DEV_NULL,  // 空设备    DEV_CHAR,  // 字符设备    DEV_BLOCK, // 块设备&#125;;\n\n设备子类型（也即具体的设备）就多了，比如：控制台、键盘、串口、磁盘、磁盘分区、软盘等。设备子类型定义在枚举类型 enum device_subtype_t 当中。\nOnix的设备抽象层，为了保证磁盘IO的正确性，其实也存在类似XV6的IO请求队列的设计，它的核心代码如下：\n// 块设备请求err_t device_request(dev_t dev, void *buf, u8 count, idx_t idx, int flags, u32 type) &#123;    device_t *device = device_get(dev);    request_t *req = kmalloc(sizeof(request_t));    // ...    // 判断列表是否为空    bool empty = list_empty(&amp;device-&gt;request_list);    // 将请求插入链表    list_insert_sort(&amp;device-&gt;request_list, &amp;req-&gt;node, element_node_offset(request_t, node, idx));    // 如果列表不为空，则阻塞，因为已经有请求在处理了，等待处理完成；    if (!empty)    &#123;        req-&gt;task = running_task();        assert(task_block(req-&gt;task, NULL, TASK_BLOCKED, TIMELESS) == EOK);    &#125;    // do io    err_t ret = do_request(req);    // 跑一轮电梯算法，获取紧挨着req的下一个任务执行IO    request_t *nextreq = request_nextreq(device, req);    list_remove(&amp;req-&gt;node);    kfree(req);    // 唤醒新队头    if (nextreq)    &#123;        assert(nextreq-&gt;task-&gt;magic == ONIX_MAGIC);        task_unblock(nextreq-&gt;task, EOK);    &#125;    return ret;&#125;\n\nOnix IO请求队列的逻辑更加清晰易懂：\n\n流程如下：\n\n\n进程调用磁盘IO接口 device_request。\n为IO请求分配一个句柄req。\n将req利用插入排序，升序插入请求队列。\n如果在该任务插入到队列前，队列非空，就当前阻塞进程。说明已经有进程在执行IO任务。\n如果在该任务插入到队列前，队列为空，由当前进程执行IO操作。\n依据电梯算法，获取到紧挨着req的下一个IO请求任务reqnext。\n从请求队列当中移除req。\n唤醒reqnext进程。注意可以确定的是，在请求队列当中只有一个进程是活跃状态其他进程都处于阻塞状态，所以在reqnext此前必定为阻塞状态，在这里我们需要唤醒它。\n\n\nOnix实现了比XV6更高效的磁盘调度算法——电梯算法。电梯算法能有效避免饥饿现象，比简单的先来先服务（FCFS）更高效，减少了磁头移动距离，能够有效平衡响应时间和吞吐量。\n电梯算法的工作原理：磁头（电梯）沿一个方向移动（比如向上），处理该方向上所有的请求。到达该方向的最后一个请求后，改变方向（向下），处理反方向上的请求。如此往复循环，就像电梯在楼层间上下运行一样。\nOnix电梯算法实现函数request_nextreq如下：\n// 获得下一个请求static request_t *request_nextreq(device_t *device, request_t *req) &#123;    list_t *list = &amp;device-&gt;request_list;    if (device-&gt;direct == DIRECT_UP &amp;&amp; req-&gt;node.next == &amp;list-&gt;tail) &#123;        device-&gt;direct = DIRECT_DOWN;    &#125; else if (device-&gt;direct == DIRECT_DOWN &amp;&amp; req-&gt;node.prev == &amp;list-&gt;head) &#123;        device-&gt;direct = DIRECT_UP;    &#125;    void *next = NULL;    if (device-&gt;direct == DIRECT_UP) &#123;        next = req-&gt;node.next;    &#125; else &#123;        next = req-&gt;node.prev;    &#125;    if (next == &amp;list-&gt;head || next == &amp;list-&gt;tail) &#123;        return NULL;    &#125;    return element_entry(request_t, node, next);&#125;\n\n磁盘块缓存层OS对磁盘的任何访问都是基于块的，XV6当中，一个磁盘块的大小为512字节，Onix当中，默认为1024字节。如果我们每次访问文件都直接从磁盘上去读取，显然大部分时间都被浪费在磁盘IO上。所以，为了提高效率，内存（高速设备）和磁盘（低速设备）之间必然需要缓存层。本小节以Onix的代码为主。代码参考：onix-dev&#x2F;src&#x2F;kernel&#x2F;buffer.c\n缓存层主要作用是：在在有限的内存当中，缓存访问频率最高的部分磁盘块。那么我们如何定性一个磁盘块的访问频率是高还是低呢？这就得靠大名鼎鼎的LRU算法。在CMU15445实验1当中可以学习到更高级的LRU算法——LRU-K算法、在LevelDB当中.sst文件结构的缓存也是用到了LRU算法。Onix实现的磁盘块缓存层使用的是最朴素的LRU算法。\n本节简单介绍一下Onix当中LRU算法运行逻辑。\n首先假设LRU相关数据结构的初始状态如下图：\n\n如图所示，LRU算法涉及的基本数据结构包括：哈希表和链表。\n\n空闲链表 idle_list:  一旦OS对缓存块的引用为0，缓存块就会被放入到空闲链表。当OS需要读取一块没有缓存到内存的磁盘块时，就会触发LRU替换策略，LRU替换策略会将空闲链表当中最久未被访问过的缓存块写回磁盘，并且将缓存块的内容覆盖成新的磁盘块。空闲链表的插入和删除的逻辑非常重要，LRU替换策略实现的关键。\n哈希表: 记录已经缓存的磁盘块。用于快速查找缓存的磁盘块。\n\n磁盘块的读取：\n\n有被缓存到内存（这里假设要读 block1）：\n\n根据哈希算法  (设备号 ^ 块号) % 哈希表长度 获取block1的所在哈希数组的位置，这里block1计算所得哈希数组索引为0。\n遍历链表，得到buffer1。\n将buffer1的引用计数加一。\n从idle_list当中移除buffer1，防止缓存块被替换出磁盘。\n\n  最终，结构如下：  \n\n没有被缓存到内存（这里假设要读 block4）：\n\n基于idle_list执行LRU替换策略。从idle_list可以了解到，idle_list的最后一个元素即为我们需要的 最久未被访问过的缓存块。也就是本节最开始所放图片当中的buffer3。\n将buffer3写回磁盘（如果标记为脏）。并从哈希表当中去除buffer3。\n将block4读到buffer3。\n根据哈希算法，计算block4所在哈希表的位置。这里假设计算block4的哈希值为1，并将block4插入到哈希表。\n\n  最终，结构如下：  \n\n\n磁盘块的释放：\n\n假设，现在要释放buffer4，将缓存块的引用计数减一。\n\n当缓存块引用计数变为0时，将缓存块挂到idle_list开头。这样可以确保idle_list当中缓冲块从左到右，最后一次访问时间越来越长。\n 最终，结构如下： \n\n\n事务层（日志层）日志层简介事务层是XV6所拥有的模块，在Onix当中并不存在，所以，本节会以XV6为主进行叙述。在XV6的文档当中，将事务层称为日志层。不管是日志层还是事务层，作用都是一样的。旨在实现错误恢复 的机制，保证磁盘上文件系统的一致性。\n依我的理解，其实linux文件系统本身就是一个通用数据库。像mysql、oracle、leveldb等数据库存在一些共有问题（包括：一致性、错误恢复等问题），解决问题的思想或多或少都借鉴linux的文件系统的实现。\n文件系统设计中最有趣的问题之一就是错误恢复，产生这样的问题是因为大多数的文件系统都涉及到对磁盘多次的写操作，如果在写操作的过程当中系统崩溃了，就会使得磁盘上的文件系统处于不一致的状态中。举例来说，根据写的顺序的不同，上述错误可能会导致一个目录项指向一个空闲的 i 节点，或者产生一个已被分配但是未被引用的块。后一种情况相对来说好一些，但在前一种情况中，目录项指向了一个空闲的 i 节点，重启之后就会导致非常严重的问题。\nxv6 通过简单的日志系统来解决文件操作过程当中崩溃所导致的问题。一个系统调用并不直接导致对磁盘上文件系统的写操作，相反，他会把一个对磁盘写操作的描述包装成一个日志写在磁盘中。当系统调用把所有的写操作都写入了日志，它就会写一个特殊的提交记录到磁盘上，代表一次完整的操作。从那时起，系统调用就会把日志中的数据写入磁盘文件系统的数据结构中。在那些写操作都成功完成后，系统调用就会删除磁盘上的日志文件。\n为什么日志可以解决文件系统操作中出现的崩溃呢？如果崩溃发生在操作提交之前，那么磁盘上的日志文件就不会被标记为已完成，恢复系统的代码就会忽视它，磁盘的状态正如这个操作从未执行过一样。如果是在操作提交之后崩溃的，恢复程序会重演所有的写操作，可能会重复之前已经进行了的对磁盘文件系统的写操作。在任何一种情况下，日志文件都使得磁盘操作对于系统崩溃来说是原子操作：在恢复之后，要么所有的写操作都完成了，要么一个写操作都没有完成。\n日志层设计——日志的磁盘结构首先，这里可以放上一张XV6当中文件系统在磁盘上的结构图：\n\n可以直观看到，我们的日志信息被放在磁盘分区最末尾的位置。\n然后，你需要感到疑惑的是：那么末尾的日志信息具体的内部结构是啥？要弄清这个问题，就需要我们深入查看xv6的代码了，日志层实现代码位于log.c文件，我们直接来到initlog函数的实现：\nstruct logheader &#123;  int n;  int block[LOGSIZE];&#125;;struct log &#123;  struct spinlock lock;  int start;  int size;  int outstanding; // how many FS sys calls are executing.  int committing;  // in commit(), please wait.  int dev;  struct logheader lh;&#125;;struct log log;voidinitlog(int dev)&#123;  struct superblock sb;  initlock(&amp;log.lock, &quot;log&quot;);  readsb(dev, &amp;sb);  log.start = sb.logstart;  log.size = sb.nlog;  log.dev = dev;  recover_from_log();&#125;\n\n\n首先可以看到初始化函数从超级快当中获取了日志的起始块 sb.logstart 和 日志磁盘块数限制 sb.nlog。\n\n然后调用了recover_from_log函数，在系统初始化阶段，尝试进行磁盘错误恢复，recover_from_log函数实现如下：\n\n调用read_head函数，读取前面拿到的日志起始块，日志起始块结构如下： struct logheader &#123;    int n;    int block[LOGSIZE];&#125;;\n 简单来说，成员n代表block数组的有效长度，而block数组，其实表示日志起始块之后的日志块 和 文件（磁盘）的数据块的映射关系。这点在介绍install_trans函数时会更有体会。\n调用install_trans函数，进行差错恢复，将上一次关机记录在日志块的没有提交完的数据重新提交。确保文件系统的一致性： // Copy committed blocks from log to their home locationstatic voidinstall_trans(void)&#123;int tail;for (tail = 0; tail &lt; log.lh.n; tail++) &#123;    struct buf *lbuf = bread(log.dev, log.start+tail+1); // read log block    struct buf *dbuf = bread(log.dev, log.lh.block[tail]); // read dst    memmove(dbuf-&gt;data, lbuf-&gt;data, BSIZE);  // copy block to dst    bwrite(dbuf);  // write dst to disk    brelse(lbuf);    brelse(dbuf);&#125;&#125;\n 仔细阅读代码可以了解到，就是顺序将日志起始块后面的日志块读出，然后根据起始块提供的文件（磁盘）数据块映射数组，将日志块拷贝到文件（磁盘）的数据块当中。并且同步到磁盘\n将日志起始块的logheader::n改为0，意味着日志的清空，标志着事务的完成。这一步也就是本节最开头所谓的“特殊的提交记录”。\n\n\n\n综上所述，磁盘当中日志的结构如下图：\n\n日志层设计——日志的封装首先是日志写操作——log_write函数，xv6规定，所有文件的写操作，都使用log_write函数进行，并且log_write函数必须在begin_op函数和end_op函数之间调用。在进程每次写一些缓存块后都会调用log_write函数。该函数只做两件事：\n\n将被修改的缓存块的块号记录到logheader.block数组当中。\n将缓存块标记为脏。\n\n需要明确的是：进程对文件任何写操作都是基于被缓存到内存的缓存块。写操作开始并未同步到磁盘块。真正的同步操作是在commit函数当中进行。\n事务包装函数如下：\n// called at the start of each FS system call.voidbegin_op(void)&#123;  acquire(&amp;log.lock);  while(1)&#123;    if(log.committing)&#123;      sleep(&amp;log, &amp;log.lock);    &#125; else if(log.lh.n + (log.outstanding+1)*MAXOPBLOCKS &gt; LOGSIZE)&#123;      // this op might exhaust log space; wait for commit.      sleep(&amp;log, &amp;log.lock);    &#125; else &#123;      log.outstanding += 1;      release(&amp;log.lock);      break;    &#125;  &#125;&#125;// called at the end of each FS system call.// commits if this was the last outstanding operation.voidend_op(void)&#123;  int do_commit = 0;  acquire(&amp;log.lock);  log.outstanding -= 1;  if(log.committing)    panic(&quot;log.committing&quot;);  if(log.outstanding == 0)&#123;    do_commit = 1;    log.committing = 1;  &#125; else &#123;    // begin_op() may be waiting for log space,    // and decrementing log.outstanding has decreased    // the amount of reserved space.    wakeup(&amp;log);  &#125;  release(&amp;log.lock);  if(do_commit)&#123;    // call commit w/o holding locks, since not allowed    // to sleep with locks.    commit();    acquire(&amp;log.lock);    log.committing = 0;    wakeup(&amp;log);    release(&amp;log.lock);  &#125;&#125;\n\n仔细阅读begin_op和end_op函数的源码，可以了解到，xv6允许多个进程在begin_op和end_op之间执行事务。但是，提交操作只能由最后一个执行完的进程执行，并且提交期间，其他要执行事务的进程都将被阻塞在begin_op函数。假设存在A、B、C三个进程要执行一组写操作。ABC都调用begin_op函数，然后对缓存块进行修改。此时 log.outstanding 为3。而A、B的写操作很快执行完调用end_op因为log.outstanding非0而直接退出，只有C执行完后，log.outstanding 为0，才能执行提交操作。并且在执行提交操作期间，其他任何准备执行事务的进程都将阻塞在begin_op函数。这里体现了xv6文档当中提到的：任何时候只能有一个进程在一个会话之中，其他进程必须等待当前会话中的进程结束。因此同一时刻日志最多只记录一次会话。\nxv6 不允许并发会话，目的是为了避免下面几种问题。假设会话 X 把一个对 i 节点的修改写入了会话中。并发的会话 Y 从同一块中读出了另一个 i 节点，更新了它，把 i 节点块写入了日志并且提交。这就会导致可怕的后果：Y 的提交导致被 X 修改过的 i 节点块被写入磁盘，而 X 此时并没有提交它的修改。如果这时候发生崩溃会使得 X 的修改只应用了一部分而不是全部，从而打破会话是原子的这一性质。有一些复杂的办法可以解决这个问题，但 xv6 直接通过不允许并行的会话来回避这个问题。\ncommit函数实现如下：\nstatic voidcommit()&#123;  if (log.lh.n &gt; 0) &#123;    write_log();     // Write modified blocks from cache to log    write_head();    // Write header to disk -- the real commit    install_trans(); // Now install writes to home locations    log.lh.n = 0;    write_head();    // Erase the transaction from the log  &#125;&#125;\n\n从注释当中可以直观了解到，干了如下4件事：\n\n\n将被改动的文件缓存块同步追加到日志（磁盘）块当中。\n更新日志（磁盘）起始块logheader磁盘结构。\n将日志块同步到适合的文件（磁盘）数据块当中。\n清空日志块。\n\n\nxv6 使用固定量的磁盘空间来保存日志。系统调用写入日志的块的总大小不能大于日志的总大小。所以源码当中，会有很多预防日志过大导致空间不足的处理措施。比如限制每个进程每次事务操作最大写数量，或者在空间不足时，阻塞进程，直到有足够可用空间被唤醒。\n虚拟文件系统实现层本小节将借Onix Minix文件系统的实现，解答虚拟文件神秘的面纱。在此之前，你一定对linux的文件系统有许多疑惑：\n\n为什么一个int类型的fd能代表文件？\n都说fd代表进程内核文件描述数组的下标，那么文件描述数组类型本身是什么？\n一块256G的磁盘，linux怎么做到让每个文件打开，能从0开始顺序读写，就好像每个文件都占用了独立的磁盘一样？\nIO设备、网络套接字、管道等怎么被抽象成fd，可以使用统一的系统调用的？\n\ni节点初步介绍这里从i节点作为切入。\ni 节点这个术语可以有两个的意思。（对于常规文件系统）它可以指的是磁盘上的记录文件类型、文件大小、数据块扇区号的数据结构。也可以指内存中的一个 i 节点代理，它包含了i节点（实体）描述符、文件系统类型、文件操作回调等。\n简单来说，一个i节点对应一个虚拟文件。并且，对于常规的文件系统，i节点分：内存结构、磁盘结构，这两种结构并不等同，内存i节点代理会引用磁盘上的i节点。\n超级快代理和i节点代理的内存结构，如下：\ntypedef struct super_t&#123;    void *desc;           // 超级块描述符    struct buffer_t *buf; // 超级块描述符 buffer    dev_t dev;            // 设备号    u32 count;            // 引用计数    int type;             // 文件系统类型    size_t sector_size;   // 扇区大小    size_t block_size;    // 块大小    list_t inode_list;    // 使用中 inode 链表    inode_t *iroot;       // 根目录 inode    inode_t *imount;      // 安装到的 inode&#125; super_t;typedef struct inode_t&#123;    list_node_t node; // 链表结点    void *desc; // inode 描述符（可能是 磁盘i节点、套接字结构体、管道结构体等。    union    &#123;        struct buffer_t *buf; // inode 描述符对应 buffer        void *addr;           // pipe 缓冲地址    &#125;;    dev_t dev;  // 设备号    dev_t rdev; // 虚拟设备号    idx_t nr;     // i 节点号    size_t count; // 引用计数    time_t atime; // 访问时间    time_t mtime; // 修改时间    time_t ctime; // 创建时间    dev_t mount; // 安装设备    mode_t mode; // 文件模式    size_t size; // 文件大小    int type;    // 文件系统类型    int uid; // 用户 id    int gid; // 组 id    struct super_t *super;   // 超级块    struct fs_op_t *op;      // 文件系统操作    struct task_t *rxwaiter; // 读等待进程    struct task_t *txwaiter; // 写等待进程&#125; inode_t;\n\n对于inode_t::type字段，onix当中有如下可选值：\nenum&#123;    FS_TYPE_NONE = 0,    FS_TYPE_PIPE,    FS_TYPE_SOCKET,    FS_TYPE_MINIX,    FS_TYPE_NUM,&#125;;\n\n虚拟文件系统将所有的IO操作都抽象成了这样一个i节点——inode_t。这里主要提一下 inode_t::desc 这个成员，如果i节点对应文件的话（FS_TYPE_MINIX），inode_t::desc会被赋值成 minix_inode_t 也就是i节点在磁盘上的数据结构；如果i节点对应网络套接字（FS_TYPE_SOCKET），那么inode_t::desc会被赋值成 socket_t。如果i节点对应管道（FS_TYPE_PIPE），那么inode_t::desc会被赋值成 fifo_t。\n本文主要介绍FS_TYPE_MINIX类型的i节点。minix文件系统初始化函数会向数组 fs_ops 当中注册一组minix文件系统专用回调函数。\nstatic fs_op_t minix_op = &#123;    minix_mkfs,    minix_super,    minix_open,    minix_close,    minix_read,    minix_write,    minix_truncate,    minix_stat,    minix_permission,    minix_namei,    minix_mkdir,    minix_rmdir,    minix_link,    minix_unlink,    minix_mknod,    minix_readdir,&#125;;void minix_init()&#123;    fs_register_op(FS_TYPE_MINIX, &amp;minix_op);&#125;\n\n如果需要实现新的文件系统，仿照minix，只需实现上面所列的必要回调函数即可。使用minix文件系统创建的i节点inode_t::op都会被赋值成minix_op。\n下面可以先了解一下minix文件系统在磁盘上的结构：\n\n各字段起始位置和作用如下表：\n\n\n\n\n\n起始位置\n作用\n\n\n\n超级块\n块1\n记录i节点位图所占块数、（磁盘数据块）逻辑块位图所占块数、第一个逻辑块号等解析文件系统的关键信息\n\n\ni节点位图\n块2\n管理磁盘i节点分配情况\n\n\n（磁盘数据块）逻辑块位图\n块2 + super-&gt;imap_blocks\n管理磁盘数据块分配情况\n\n\ni节点\n块2 + super-&gt;imap_blocks + super-&gt;zmap_blocks\n磁盘i节点数组\n\n\n（磁盘数据块）逻辑块\nminix_super_t::firstdatazone\n磁盘数据块数组\n\n\n\ni节点的分配与释放上面提到，由于i节点在虚拟文件系统当中有两种存在形式，所以本小节i节点的分配与释放也是分为两种。当需要创建文件时我们需要分配一个i节点，当文件需要被删除（文件磁盘i节点引用计数减为零）时我们需要释放这一个i节点。\n首先对于磁盘i节点的分配与释放，磁盘i节点分配&#x2F;释放的管理由磁盘上 i节点位图 和 磁盘i节点数组 共同管理，它们的起始位置在上小结已给出。所有的磁盘上的 i 节点都被统一存放在一个称为 i 节点数组的连续区域中。每一个 i 节点的大小都是一样的，所以对于一个给定的数字n，很容易找到磁盘上对应的 i 节点。事实上这个给定的数字就是操作系统中 i 节点的编号。\n磁盘上的 i 节点由结构体 minix_inode_t 定义。mode 域用来区分文件、目录和特殊文件。nlink 域用来记录指向了这一个 i 节点的目录项，这是用于判断一个 i 节点是否应该被释放的。size 域记录了文件的字节数。zone 数组用于这个文件的数据块的块号。在后面 虚拟文件的打开&#x2F;读写&#x2F;关闭 小结就会看到，每个文件采用 zone 数组管理文件数据的方式非常巧妙，该数组就是每个文件都能从0偏移开始读取和写入，看起来像多块磁盘的原因。\ni节点和超级快磁盘结构如下：\ntypedef struct minix_inode_t&#123;    u16 mode;    // 文件类型和属性(rwx 位)    u16 uid;     // 用户id（文件拥有者标识符）    u32 size;    // 文件大小（字节数）    u32 mtime;   // 修改时间戳 这个时间戳应该用 UTC 时间，不然有瑕疵    u8 gid;      // 组id(文件拥有者所在的组)    u8 nlinks;   // 链接数（多少个文件目录项指向该i 节点）    u16 zone[9]; // 直接 (0-6)、间接(7)或双重间接 (8) 逻辑块号&#125; minix_inode_t;typedef struct minix_super_t&#123;    u16 inodes;        // 节点数    u16 zones;         // 逻辑块数    u16 imap_blocks;   // i 节点位图所占用的数据块数    u16 zmap_blocks;   // 逻辑块位图所占用的数据块数    u16 firstdatazone; // 第一个数据逻辑块号    u16 log_zone_size; // log2(每逻辑块数据块数)    u32 max_size;      // 文件最大长度    u16 magic;         // 文件系统魔数&#125; minix_super_t;\n\n磁盘i节点分配&#x2F;释放的核心函数是 minix_ialloc 和 minix_ifree。分配函数首先会从超级块当中定位i节点位图的起始位置和长度。然后遍历i节点位图，找到一个可用i节点编号，将位图置为true（占用）并返回。minix_ialloc只负责从磁盘上找到一个可用的i节点编号返回给调用者，其他的什么也不做。minix_ifree 就是 minix_ialloc 逆过程，将分配的i节点编号对应的位图置为false（可分配）。两函数代码如下：\n// 分配一个文件系统 inodeidx_t minix_ialloc(super_t *super)&#123;    buffer_t *buf = NULL;    idx_t bit = EOF;    bitmap_t map;    minix_super_t *desc = (minix_super_t *)super-&gt;desc;    idx_t bidx = 2;    for (size_t i = 0; i &lt; desc-&gt;imap_blocks; i++)    &#123;        buf = bread(super-&gt;dev, bidx + i);        assert(buf);        bitmap_make(&amp;map, buf-&gt;data, BLOCK_BITS, i * BLOCK_BITS);        bit = bitmap_scan(&amp;map, 1);        if (bit != EOF)        &#123;            assert(bit &lt; desc-&gt;inodes);            buf-&gt;dirty = true;            break;        &#125;    &#125;    brelse(buf); // todo 调试期间强同步    return bit;&#125;// 释放一个文件系统 inodevoid minix_ifree(super_t *super, idx_t idx)&#123;    minix_super_t *desc = (minix_super_t *)super-&gt;desc;    assert(idx &lt; desc-&gt;inodes);    buffer_t *buf;    bitmap_t map;    idx_t bidx = 2;    for (size_t i = 0; i &lt; desc-&gt;imap_blocks; i++)    &#123;        if (idx &gt; BLOCK_BITS * (i + 1))        &#123;            continue;        &#125;        buf = bread(super-&gt;dev, bidx + i);        assert(buf);        bitmap_make(&amp;map, buf-&gt;data, BLOCK_BITS, i * BLOCK_BITS);        assert(bitmap_test(&amp;map, idx));        bitmap_set(&amp;map, idx, 0);        buf-&gt;dirty = true;        break;    &#125;    brelse(buf); // todo 调试期间强同步&#125;\n\n调用者得到i节点编号后，可以通过 iget 函数将i节点编号转为i节点代理。它的逻辑是这样的，先尝试在现有内存i节点代理当中根据i节点编号查找i节点，如果查到了，就直接返回，否则调用 get_free_inode 函数先从空闲的i节点代理缓存当中获取一个空闲i节点代理，再将i节点编号转为磁盘i节点所在块号，然后将磁盘i节点所在磁盘块缓存到内存获取磁盘上的i节点，然后对空闲的i节点代理进行初始化。\ni节点代理的核心初始化步骤包括：\n\n\n将inode_t::desc填充成 磁盘i节点缓存地址。\ninode::buf填充成 磁盘i节点所在块缓存地址，inode::buf指向 磁盘i节点所在块缓存地址 这样能保证 磁盘i节点所在块缓存 的引用计数非零，防止inode_t::desc所指向的磁盘i节点缓存地址 因磁盘块的lru算法被置换出内存而失效。\n如果i节点对应的是IO设备，而非文件，将inode_t::rdev填充为minix_inode_t::zone[0]，对于设备文件，minix_inode_t::zone[0]表示IO设备的虚拟设备号。\ninode_t::super填充成所在设备的超级快。同一设备上的文件系统，超级快是相同的！\ninode_t::type设置成FS_TYPE_MINIX。\n并且inode_t::op安装成FS_TYPE_MINIX对应的回调。\n\n\n核心代码如下：\n#define BLOCK_INODES (BLOCK_SIZE / sizeof(minix_inode_t))    // 1个磁盘块能容纳 inode的数量// 计算 inode nr 对应的块号static inline idx_t inode_block(minix_super_t *desc, idx_t nr) &#123;    // inode 编号 从 1 开始    return 2 + desc-&gt;imap_blocks + desc-&gt;zmap_blocks + (nr - 1) / BLOCK_INODES;&#125;// 将i节点编号转为i节点代理static inode_t *iget(dev_t dev, idx_t nr) &#123;    inode_t *inode = find_inode(dev, nr);    if (inode)    &#123;        inode-&gt;count++;        inode-&gt;atime = time();        return fit_inode(inode);    &#125;    super_t *super = get_super(dev);    assert(super);    minix_super_t *desc = (minix_super_t *)super-&gt;desc;    assert(nr &lt;= desc-&gt;inodes);    inode = get_free_inode();    inode-&gt;dev = dev;    inode-&gt;nr = nr;    inode-&gt;count++;    // 加入超级块 inode 链表    list_push(&amp;super-&gt;inode_list, &amp;inode-&gt;node);    idx_t block = inode_block(desc, inode-&gt;nr);    buffer_t *buf = bread(inode-&gt;dev, block);    inode-&gt;buf = buf;    // 将缓冲视为一个 inode 描述符数组，获取对应的指针；    inode-&gt;desc = &amp;((minix_inode_t *)buf-&gt;data)[(inode-&gt;nr - 1) % BLOCK_INODES];    minix_inode_t *minode = (minix_inode_t *)inode-&gt;desc;    inode-&gt;rdev = minode-&gt;zone[0];    inode-&gt;mode = minode-&gt;mode;    inode-&gt;size = minode-&gt;size;    inode-&gt;super = super;    inode-&gt;type = FS_TYPE_MINIX;    inode-&gt;op = fs_get_op(FS_TYPE_MINIX);    return fit_inode(inode);&#125;\n\n与iget对立的是iput函数，该函数定义在onix-dev&#x2F;src&#x2F;fs&#x2F;inode.c，函数iput用于将i节点代理进行释放，iput会借助 minix_close 函数，一旦inode_t::count成员减为零，就将inode::buf（磁盘i节点所在块缓存）释放，然后将i节点代理放回空闲缓存区。\n磁盘数据块的分配与释放onix当中，文件系统文件的数据存在于分散的磁盘数据块当中。文件使用磁盘上的i节点minix_inode_t::zone数组管理磁盘数据块。文件创建后，向里面不断顺序写入数据，一旦当前磁盘数据块写满了，文件系统就会调用 minix_balloc 函数给文件分配一块磁盘数据块。相反的，如果一个文件需要被删除，调用 minix_bfree 函数释放所有分配的磁盘数据块。磁盘块同样采用位图的方式去管理。\n磁盘数据块分配释放&#x2F;器的原理和 磁盘i节点分配&#x2F;释放器（minix_ialloc 和 minix_ifree）极其类似。磁盘数据块分配器首先会从超级块当中定位磁盘数据块位图的起始位置和长度。然后遍历磁盘数据块位图，找到一个可用磁盘数据块号，将位图置为true（占用）并返回。minix_ialloc只负责从磁盘上找到一个可用的磁盘数据块号返回给调用者，其他的什么也不做。minix_bfree 就是 minix_balloc 逆过程，将分配的磁盘数据块号对应的位图置为false（可分配）。\n代码如下：\n// 分配一个文件块idx_t minix_balloc(super_t *super)&#123;    buffer_t *buf = NULL;    idx_t bit = EOF;    bitmap_t map;    minix_super_t *desc = (minix_super_t *)super-&gt;desc;    idx_t bidx = 2 + desc-&gt;imap_blocks;    for (size_t i = 0; i &lt; desc-&gt;zmap_blocks; i++)    &#123;        buf = bread(super-&gt;dev, bidx + i);        assert(buf);        // 将整个缓冲区作为位图        bitmap_make(&amp;map, buf-&gt;data, BLOCK_SIZE, i * BLOCK_BITS + desc-&gt;firstdatazone - 1);        // 从位图中扫描一位        bit = bitmap_scan(&amp;map, 1);        if (bit != EOF)        &#123;            // 如果扫描成功，则 标记缓冲区脏，中止查找            assert(bit &lt; desc-&gt;zones);            buf-&gt;dirty = true;            break;        &#125;    &#125;    brelse(buf); // todo 调试期间强同步    return bit;&#125;// 释放一个文件块void minix_bfree(super_t *super, idx_t idx)&#123;    minix_super_t *desc = (minix_super_t *)super-&gt;desc;    assert(idx &lt; desc-&gt;zones);    buffer_t *buf;    bitmap_t map;    idx_t bidx = 2 + desc-&gt;imap_blocks;    for (size_t i = 0; i &lt; desc-&gt;zmap_blocks; i++)    &#123;        // 跳过开始的块        if (idx &gt; BLOCK_BITS * (i + 1))        &#123;            continue;        &#125;        buf = bread(super-&gt;dev, bidx + i);        assert(buf);        // 将整个缓冲区作为位图        bitmap_make(&amp;map, buf-&gt;data, BLOCK_SIZE, BLOCK_BITS * i + desc-&gt;firstdatazone - 1);        // 将 idx 对应的位图置位 0        assert(bitmap_test(&amp;map, idx));        bitmap_set(&amp;map, idx, 0);        // 标记缓冲区脏        buf-&gt;dirty = true;        break;    &#125;    brelse(buf); // todo 调试期间强同步&#125;\n\n根文件系统的挂载在onix初始化阶段在使用文件系统之前，必须先对根目录进行挂载。我们使用系统调用 open 函数打开的文件时，需要传入路径作为参数，而想要通过路径找到最终文件的i节点，必经过根目录的i节点。任何路径的解析，都以根目录为基础的。所以，分析onix根目录的挂载流程是很重要的。\n根文件系统挂载触发点位于super_init函数：\n\nsuper_init函数首先会初始化全局超级块表，为后续文件系统挂载做准备，每个超级块维护一个inode链表，用于管理该文件系统打开的文件节点，超级块表初始化完毕后，会调用mount_root函数。\nmount_root函数首先获取磁盘第一个分区的虚拟设备控制句柄，然后从设备控制句柄当中获取设备号dev（方便读写），然后调用read_super函数获取dev设备的超级快。最后将超级块和根目录i节点代理做绑定，方便将路径解析为i节点。  // 挂载根文件系统static void mount_root() &#123;    LOGK(&quot;Mount root file system...\\n&quot;);    // 假设主硬盘第一个分区是根文件系统    device_t *device = device_find(DEV_IDE_PART, 0);    assert(device);    // 读根文件系统超级块    root = read_super(device-&gt;dev);    // 超级快和根目录i节点代理做绑定。    root-&gt;imount = root-&gt;iroot;    root-&gt;imount-&gt;count++;    root-&gt;iroot-&gt;mount = device-&gt;dev;&#125;\nread_super函数首先会尝试使用设备号dev从超级快列表当中直接获取超级快，但由于系统刚启动，正处在初始化阶段，所以，必然失败。然后read_super函数会尝试从超级块列表当中获取一个空闲的超级快，然后调用minix_super函数对空闲超级快进行初始化。  // 读设备 dev 的超级块super_t *read_super(dev_t dev) &#123;    super_t *super = get_super(dev);    if (super) &#123;        super-&gt;count++;        return super;    &#125;    LOGK(&quot;Reading super block of device %d\\n&quot;, dev);    // 获得空闲超级块    super = get_free_super();    super-&gt;count++;    for (size_t i = 1; i &lt; FS_TYPE_NUM; i++) &#123;        fs_op_t *op = fs_get_op(i);        if (!op)            continue;        // minix_super        if (op-&gt;super(dev, super) == EOK) &#123;            return super;        &#125;    &#125;    put_super(super);    return NULL;&#125;\nminix_super函数实现较为简单，通过读取磁盘的 磁盘块1 （在前面的列出的磁盘分区结构图当中有列出，超级块位于1号磁盘块），将 磁盘超级快 读到内存当中，然后将磁盘超级块缓存填充到super_t::desc，当然为了避免磁盘超级块缓存被替换出磁盘，将磁盘超级快所在的磁盘块缓存填充给super_t::buf，最后调用 iget 函数获取根目录的i节点代理（按规定，根目录磁盘i节点的编号为1）。  static int minix_super(dev_t dev, super_t *super) &#123;    // 读取超级块    buffer_t *buf = bread(dev, 1);    if (!buf)        return -EFSUNK;    assert(buf);    minix_super_t *desc = (minix_super_t *)buf-&gt;data;    if (desc-&gt;magic != MINIX1_MAGIC) &#123;        brelse(buf);        return -EFSUNK;    &#125;    super-&gt;buf = buf;    super-&gt;desc = desc;    super-&gt;dev = dev;    super-&gt;type = FS_TYPE_MINIX;    super-&gt;block_size = BLOCK_SIZE;    super-&gt;sector_size = SECTOR_SIZE;    super-&gt;iroot = iget(dev, 1);    return EOK;&#125;\n\n注意 super-&gt;iroot 成员的初始化被放在了最后，这里是一定要放在最后。如果你深入阅读 iget 函数的代码就会发现 iget函数 里面需要用到超级块如下：\nstatic inode_t *iget(dev_t dev, idx_t nr) &#123;    inode_t *inode = find_inode(dev, nr);    if (inode) &#123;        inode-&gt;count++;        inode-&gt;atime = time();        return fit_inode(inode);    &#125;    super_t *super = get_super(dev);    assert(super);    // ...    return fit_inode(inode);&#125;\n\n等等，我们正在初始化超级快，但是 iget 函数又用到了超级快，这样对吗？我的回答是没有任何毛病，因为 在minix_super超级快初始化函数当中最后一刻才调用了iget函数，此时超级快其实已经初始化的差不多了，并且iget函数内部也并没有用到 super-&gt;iroot 成员，所以逻辑并没有什么问题。\n虚拟文件的打开&#x2F;关闭onix当中，使用open函数打开一个文件，如果要创建以一个文件，实际上使用的是带 O_CREAT | O_TRUNC 标志的open函数，我们只需要重点研究open系统调用 sys_open 的实现，伪代码如下：\nfd_t sys_open(char *filename, int flags, int mode) &#123;    char *next;    inode_t *dir = NULL;    // 获取filename父目录i节点代理，并且，将最后的文件名填到next当中。    dir = named(filename, &amp;next);    // 如果路径本身是目录（/a/b/）。跳过minix文件系统open回调    if (!*next) &#123;        inode = dir;        dir-&gt;count++;        goto makeup;    &#125;    // 调用minix文件系统open回调。打开/创建文件。    int ret = dir-&gt;op-&gt;open(dir, next, flags, mode, &amp;inode);    // 从进程文件表（数组）当中获取一个空闲的文件描述符，并且初始化其元素file_t。    file_t *file;    fd_t fd = fd_get(&amp;file);    file-&gt;inode = inode;    file-&gt;flags = flags;    file-&gt;count = 1;    file-&gt;offset = 0;    // 有追加参数就将file的偏移移到文件末尾。    if (flags &amp; O_APPEND) &#123;        file-&gt;offset = file-&gt;inode-&gt;size;    &#125;    return fd;&#125;\n\n\n\n调用named函数，获取filename父目录i节点代理，并且，将最后的文件名填到next当中。\n如果路径本身是目录（&#x2F;a&#x2F;b&#x2F;）。跳过minix文件系统open回调。进入4。\n调用minix的open回调，打开&#x2F;创建文件。获取文件的i节点代理。\n从进程文件表（数组）当中获取一个空闲的文件描述符，并且初始化其元素file_t。\n如果flags标志包含 O_APPEND ，将file的偏移移到文件末尾。\n\n\nnamed函数的作用就是根据路径找到文件父目录的i节点，并且将最后的文件名填到next当中。named函数代码稍微有些复杂，特别是这一段：\n// 获取 pathname 对应的父目录 inodeinode_t *named(char *pathname, char **next) &#123;    // ...    while (true) &#123;        // 返回上一级目录且上一级目录是个挂载点        if (match_name(name, &quot;..&quot;, next) &amp;&amp; dir == dir-&gt;super-&gt;iroot) &#123;            super_t *super = dir-&gt;super;            inode = super-&gt;imount;            inode-&gt;count++;            iput(dir);            dir = inode;        &#125;    &#125;    // ...&#125;\n\n如果路径 name（基于pathname经过预处理的路径） 解析到了 .. 文件名并且（同一文件系统下）其父目录正好是设备的根节点，就会进入该if分支。因为linux当中一切皆文件的思想，（跨文件系统）父目录可能是作为子目录的挂载点而存在，导致父目录和子目录不在一个设备（文件系统）上。故我们在跨文件系统的情况下执行 .. 目录回退操作时，必须执行这样一段 “换根” 代码。\n这段if换根代码强烈建议读者结合 sys_mknod 和 sys_mount 函数实现去理解。在onix当中（参考：onix-dev&#x2F;src&#x2F;fs&#x2F;dev.c），挂载一个设备首先要使用 sys_mknod 系统调用为设备创建一个设备文件，然后，在根文件系统下寻找一个目录作为挂载点，最后使用 sys_mount 系统调用挂载设备。\n下面深入介绍一下onix当中如何实现设备挂载的：\n挂载点内部数据结构链接细节如下图：\n\nsys_mknod会接收设备号作为参数。它的实现最终会调用minix_mknod函数，这会创建一个特殊的文件，我们称它为设备文件，然后文件的磁盘i节点当中zone数组只使用第一个元素，存放传入的设备号，在将磁盘i节点转成i节点代理过程当中，i节点代理的rdev成员就会被赋值成minode-&gt;zone[0]，minix_mknod伪代码如下：\nint minix_mknod(inode_t *dir, char *name, int mode, int dev) &#123;    // ...    // 创建一个特殊的文件。    // ...    if (ISBLK(mode) || ISCHR(mode))        minode-&gt;zone[0] = dev;    // ...&#125;\n\nsys_mount函数负责将一个设备文件挂载到指定目录下。比如sd卡，一般来说，它的设备文件名为&#x2F;dev&#x2F;sdb1，当我们要访问sd卡时，需要执行命令 mount /dev/sdb1 /mnt 。这里需要注意的是，sd卡所有的文件系统 和 我们的根目录所挂载的文件系统一定是两个独立的互不相干的两个文件系统，他们各自拥有自己独立、自包含的i节点编号空间。在同一个Linux系统下，SD卡上文件的i节点号与根文件系统下文件的i节点号即使数字相同，也绝对不会发生冲突。系统能够清晰地区分它们。 onix sys_mount函数实现伪代码如下：\nint sys_mount(char *devname, char *dirname, int flags) &#123;    // 获取设备文件i节点代理    devinode = namei(devname);    // 获取挂载点i节点代理    dirinode = namei(dirname);    // 获取设备超级块    super = read_super(devinode-&gt;rdev);    // 设备超级快指向挂载点    super-&gt;imount = dirinode;    // 挂载点指向挂载的设备    dirinode-&gt;mount = devinode-&gt;rdev;    iput(devinode);    return EOK;&#125;\n\n核心逻辑如下：\n\n\n获取设备文件i节点代理。\n获取挂载点i节点代理。\n获取设备超级块。\n将设备超级块指向挂载点。\n挂载点指向挂载的设备。\n\n\nsys_open当中核心函数还是中间调用的minix_open回调，它的伪代码如下：\nstatic int minix_open(inode_t *dir, char *name, int flags, int mode, inode_t **result) &#123;    minix_dentry_t *entry = NULL;    // 在dir下尝试查找name文件，并将目录项存到entry。    buf = find_entry(dir, name, &amp;next, &amp;entry);    if (buf) &#123;        // 如果找到了根据目录项将 i节点号转为i节点代理。iget函数是老朋友了，前面小结经常用到。        inode = iget(dir-&gt;dev, entry-&gt;nr);        assert(inode);        goto makeup;    &#125; // 不存在    // 不带创建标志，返回文件不存在的错误。    if (!(flags &amp; O_CREAT)) &#123;        ret = -EEXIST;        return res;    &#125;    // 向父目录当中添加名为name的文件项。并且返回其文件项缓存块地址，确保entry有效。    buf = add_entry(dir, name, &amp;entry);    // 申请一个i节点号。    entry-&gt;nr = minix_ialloc(dir-&gt;super);    // 初始化磁盘i节点和i节点代理。    inode = new_inode(dir-&gt;dev, entry-&gt;nr);makeup:    // 传给输出参数。    *result = inode;    return EOK;&#125;\n\n首先明确：sys_open调用minix_open，传入的name是单个文件名，dir是name文件的父目录i节点。\n\n\n调用find_entry函数，首先在dir下尝试查找name文件，并将目录项存到entry。如果找到了，根据目录项将 i节点号转为i节点代理，并快进到 makeup 阶段。iget函数是老朋友了，前面小结经常用到。如果没找到，尝试去创建一个新文件。\n检测是否带 O_CREAT 标志，不带标志就返回文件不存在的错误。\n调用add_entry函数向父目录当中添加名为name的文件项。并且返回其文件项缓存块地址，确保entry有效。\n申请一个i节点号。\n初始化磁盘i节点和i节点代理。\n\n\n有关 find_entry 和 add_entry 函数，这两函数属于目录操作函数，在 目录的操作 小结将深入展开讲解。\n踪上，一个进程打开一个文件后，系统状态图如下：\n\n虚拟文件的读&#x2F;写如果说超级块是更高维度的能解析文件系统的磁盘结构，那么i节点其实可以理解成低维度的能解析文件的磁盘结构。\n在 i节点的分配与释放 小结已经列出过i节点的磁盘结构，但因为文章结构原因我们没有多费口舌去讲解它具体成员的作用。在本节，因为文件的操作是极度依赖i节点磁盘结构的。所以，下面用一张图片直观了解i节点的磁盘结构：\n\n一般的，i节点磁盘数据结构各个字段作用如下：\n\n\n\ni节点字段名\n作用\n\n\n\nminix_inode_t::mode\n文件类型和属性位，onix支持的文件类型包含：常规文件、目录文件、字符设备文件、块设备文件。文件屬性包含：读、写、可执行。\n\n\nminix_inode_t::uid\n用户id，也即文件拥有者标识符\n\n\nminix_inode_t::size\n文件大小（Byte）\n\n\nminix_inode_t::mtime\n最后修改的时间戳\n\n\nminix_inode_t::gid\n用户所属组id\n\n\nminix_inode_t::nlinks\n链接数，表示有多少目录项指向该i节点\n\n\nminix_inode_t::zone\n长度为9的uint16_t类型数组，前7个元素代表直接块，直接指向磁盘数据块，第8个元素为一级间接块，第9个元素为二级间接块\n\n\n磁盘i节点的设计非常精妙，前面部分存放文件属性，后面的数组 minix_inode_t::zone 是你打开每一个文件，都能从0偏移开始读取的根本原因。如果你有仔细研究过onix&#x2F;xv6内存管理当中内存虚拟内存机制，你会发现，i节点对文件数据块的管理 和 虚拟内存的页表-页框-页目录机制思想一模一样。\n尽管物理上磁盘只有一块，i节点磁盘数据结构这种 直接块-间接块-双重间接块 机制让虚拟文件系统当中的每个文件都像独占一块磁盘一样，使（实际上）非连续空间连续化，可以从偏移0位置开始顺序读写数据。同时，在这种机制的加持下，更便于磁盘块的管理，增加零散磁盘块的利用率。\n有了上面i节点管理数据块的理论知识，下面让我们一步步了解 sys_read 和 sys_write 系统调用的实现。\n首先是 sys_read 函数，伪代码如下：\nint sys_read(fd_t fd, char *buf, int count) &#123;    // ...    if ((ret = fd_check(fd, &amp;file)) &lt; EOK)        return ret;    inode_t *inode = file-&gt;inode;    int len = inode-&gt;op-&gt;read(inode, buf, count, file-&gt;offset);    if (len &gt; 0)        file-&gt;offset += len;    return len;&#125;\n\n\n\n首先将fd转换为文件表项 file_t 。\n由文件表项获得文件的i节点代理。\n调用 minix read 回调读取数据。\n\n\n直接来到了minix read 回调——minix_read。该函数根据传入的要读的数据长度len和文件偏移offset来来逐块读取数据。如下：\n// 从 inode 的 offset 处，读 len 个字节到 bufstatic int minix_read(inode_t *inode, char *data, int len, off_t offset) &#123;    minix_inode_t *minode = (minix_inode_t *)inode-&gt;desc;    if (ISCHR(minode-&gt;mode)) &#123;        assert(minode-&gt;zone[0]);        return device_read(minode-&gt;zone[0], data, len, 0, 0);    &#125; else if (ISBLK(minode-&gt;mode)) &#123;        assert(minode-&gt;zone[0]);        device_t *device = device_get(minode-&gt;zone[0]);        assert(len % BLOCK_SIZE == 0);        assert(device_read(minode-&gt;zone[0], data, len / BLOCK_SIZE, offset / BLOCK_SIZE, 0) == EOK);        return len;    &#125;    assert(ISFILE(minode-&gt;mode) || ISDIR(minode-&gt;mode));    // 如果偏移量超过文件大小，返回 EOF    if (offset &gt;= minode-&gt;size) &#123;        return EOF;    &#125;    // 开始读取的位置    u32 begin = offset;    // 剩余字节数    u32 left = MIN(len, minode-&gt;size - offset);    while (left) &#123;        // 找到对应的文件便宜，所在文件块        idx_t nr = minix_bmap(inode, offset / BLOCK_SIZE, false);        assert(nr);        // 读取文件块缓冲        buffer_t *buf = bread(inode-&gt;dev, nr);        // 文件块中的偏移量        u32 start = offset % BLOCK_SIZE;        // 本次需要读取的字节数        u32 chars = MIN(BLOCK_SIZE - start, left);        // 更新 偏移量 和 剩余字节数        offset += chars;        left -= chars;        // 文件块中的指针        char *ptr = buf-&gt;data + start;        // 拷贝内容        memcpy(data, ptr, chars);        // 更新缓存位置        data += chars;        // 释放文件块缓冲        brelse(buf);    &#125;    // 更新访问时间    inode-&gt;atime = time();    // 返回读取数量    return offset - begin;&#125;\n\n函数逻辑如下：\n\n\n判断文件类型，如果是字符设备或者块设备，就根据minode-&gt;zone[0]获取到设备的设备号，然后调用虚拟设备回调函数，去读取数据并返回。如果是普通文件&#x2F;目录，继续下面步骤。\n普通文件的话，进入while循环，调用minix_bmap函数基于i节点的zone数组将offset转换为数据所在磁盘块。\n获取磁盘块缓存。\n读取 MIN(BLOCK_SIZE - start, left) 长的数据。\n如果读取的数据不够len字节，重复2~4步骤。\n\n\n那么，minix_bmap函数是怎么实现的呢？onix对minix_bmap稍微有些复杂。这里附上我的想法，我觉得可以重用 这才是计科之 Onix &amp; XV6 源码分析（2、类Unix的内存管理） 的做法。思想就是基于 i节点的磁盘结构图 对offset进行划分，如下：\n\n\n如果 offset 在直接块覆盖范围内，直接通过zone数组读取数据块即可。\n如果 offset 在一级间接块覆盖范围内。\n首先将offset减去直接块所能表示的最大地址。\n将offset看成虚拟磁盘地址，然后由 (offset &amp; 0x7FC00) &gt;&gt; 10 公式获取虚拟磁盘地址高9位作为一级间接块的数组（元素为uint16_t）索引，然后取得磁盘数据块号后，将offset末尾的10位作为磁盘数据块内偏移。 获取块内偏移的公式：offset &amp; 0x3FF ，最终获得到物理磁盘地址。\n\n\n如果 offset 在二级间接块覆盖范围内。\n首先将offset减去直接块 + 一级间接块所能表示的最大地址。\n将offset看成虚拟磁盘地址，然后由 (offset &amp; 0xFF80000) &gt;&gt; 10 公式获取虚拟磁盘地址高9位作为一级间接块的数组（元素为uint16_t）索引，取得二级间接块号后，根据公式 (offset &amp; 0x7FC00) &gt;&gt; 10 得到二级间接块数组（元素同样为uint16_t）索引，取得磁盘数据块号后，将offset末尾的10位作为磁盘数据块内偏移。 获取块内偏移的公式：offset &amp; 0x3FF ，最终获得到物理磁盘地址。\n\n\n\n\n但是， 由于onix规定zone数组元素类型为uint16_t，所以这决定了onix最大能表示的磁盘块号为2^16 - 1， 故onix的磁盘有效大小的上限为 64MB。 当然，作为学习操作系统，我们就不必太过于精益求精。\n文件系统的写，是读的逆过程，代码逻辑极其相似，行文至此，篇幅过多，这里就不带着大家读代码了，建议读者亲自去扒一扒onix的源码。\n目录的操作最后就是文件系统目录的操作，这里主要讲解两个函数：find_entry 和 add_entry。面对目录，只需要记住一点，目录其实就是一个 文件内容被规定死的 普通文件。目录文件会存放一个个目录条目，onix当中目录条目数据结构如下：\n// 文件目录项结构typedef struct minix_dentry_t&#123;    u16 nr;              // i 节点    char name[NAME_LEN]; // 文件名&#125; minix_dentry_t;\n\n目录项图解如下：\n\n在onix当中，find_entry函数为了找到目标目录项会使用for循环遍历传进来的目录的所有目录项。\nadd_entry同样会遍历目录的目录项，直到找到一个 nr 成员为0的目录项，在onix当中，nr为0意味这该目录项未被占用。所以相应的，如果你想删除一个目录项，只需将对应的nr置为0即可。onix的minix_rmdir函数也正是这样做的。\n\n本章完结\n","tags":["类Unix源码剖析"]},{"title":"这才是计科之 Onix & XV6 源码分析（2、类Unix的内存管理）","url":"/2024/06/07/xv6/MemoryManager/","content":"前言前一章我们介绍了XV6的启动流程，在本章，我们就要开始介绍类Unix操作系统最重要的一部分——内存管理模块。\n本文以XV6里面的内存管理为主，后面会提炼Onix中内存管理比较好的部分，来对XV6进行一个补充。\nOnix相关链接：\n\ngithub仓库链接。\n\nB站配套视频链接。\n\n\nXV6-x86的github链接：\n\n链接。\n\n\n\n分页首先了解一下unix-like系统虚拟内存做法。\nx86中分页涉及两个主要寄存器：cr0、cr3，cr0主要用于控制是否开启分页，cr3用于存放页目录的地址。当cr0寄存器设置开启分页，cpu所使用的一切地址皆是虚拟地址，在分页机制中，一个虚拟地址被划分成3个部分，如下图：\n\n如图中所注释的那样，地址的高10位代表在页目录中的索引（一个目录项占4Byte）、中间10位代表页表上的索引（一个页表项占4Byte）、低12位代表在页框上的偏移（1Byte为单位）。一个典型的页目录大小是4096字节（一个Page的大小），页表和页目录大小一样。并且低12位的寻址范围正好也是4096，这样一切就恰到好处。此时：sizeof(页目录) &#x3D;&#x3D; sizeof(页表) &#x3D;&#x3D; sizeof(页框) &#x3D;&#x3D; 4096。\n此外，由于页目录（页表）每一项都是4Byte为单位。所以每个页目录（页表）正好有1024个entry。\n一个页目录（页表）的每一个索引项都是4Byte，这4Byte格式如下图：\n\n从entry的结构图可以看出，它的高20位指向页框号（物理上的），在4G的物理地址空间内，一个页面按4096个字节算的话，正好有1M个页面（页框）。所以20位正好能为每一个页框从0开始编址。\nentry的低12位是一些控制位，比如0位指示该entry所指向的物理地址是否有效，第1位指示entry所指的物理页是否可写，第3位代表用户态能否访问，等等。我们可以利用这些位来实现很多unix-like中的骚操作，最有名的应该是利用第1位实现写时拷贝。\nCPU会通过MMU来完成虚拟地址到物理地址的翻译，记要翻译的虚拟地址是vaddr，过程如下：\n\n拿到cr3指向的页目录地址（物理地址），dir_paddr。\n\n位运算取vaddr的高10位，将高10位作为页目录的索引，得到dir_entry，dir_entry会指向下一级的页表，所以通过dir_entry可以得到页表的地址（物理地址），tab_paddr。s\n\n位运算取vaddr中间10位，将中间10位作为页表的索引，得到tab_entry，tab_entry会指向最后一级的页框，所以通过tab_entry可以得到页框的地址（物理地址），frm_paddr。\n\n最后，位运算取vaddr的低12位，这12位作为页框的物理地址frm_addr的偏移，于此得到了虚拟地址对应的物理地址：frm_paddr + offset。\n\n\n整个流程可以使用xv6官方提供的图来描绘：\n\n内存管理首先回顾一下，在进入main前，cpu的状态如下：关中断 &amp; 进入了保护模式 &amp; 安装了一个临时全局描述符 &amp; BSP分配了一个4K的内核栈 &amp; 开启了4M big page的临时分页。\nXV6在main函数中会进行内存管理相关的初始化：\nintmain(void)&#123;  kinit1(end, P2V(4*1024*1024)); // phys page allocator  kvmalloc();      // kernel page table  seginit();       // segment descriptors // 扩张了用户【数据&amp;代码】段描述符，源码很简单，读者可自行阅读，这里就不去帖代码了  // ...  startothers();   // start other processors  // 唤醒其他处理器，本文不会做过多讲解，后面的文章再深入探讨。  kinit2(P2V(4*1024*1024), P2V(PHYSTOP)); // must come after startothers()  // ...&#125;\n\n因为cpu现在是开启分页的，所以，我们利用P2V宏来获得4M物理地址对应的虚拟地址，实现也很简单，就是物理地址加上内核基地址的偏移就能得到物理地址对应的虚拟地址：\n#define V2P_WO(x) ((x) - KERNBASE)    // same as V2P, but without casts#define P2V_WO(x) ((x) + KERNBASE)    // same as P2V, but without casts\n\nkinit1：初始化内核内存分配器（阶段1）。xv6的注释将kalloc称为物理内存分配器，这是第一个吐槽点，从它的源码上来看，我认为把它称为内核内存分配器更加合适。因为xv6把物理内存、内核内存的管理全都交给kalloc做，并且内存粒度也固定为4K。由于所有的物理地址也由kalloc管理，为此内核不得不去在自己的页表上去映射所有的物理内存！，其优点就是代码实现上会很简单（特别是在软件实现修改页表的操作上），但是这样做是非常暴力的。如果你阅读过Onix、或者其他更完善的unix-like操作系统的话会发现，更加严谨的做法是：get_page + alloc_kpage + kmalloc：\n// Onix// src/kernel/memory.c// 底层实现为：使用连续的256页来跟踪每个页框，每个页框会对应1Byte的entry// 使用1Byte的好处是可以记录每个页框的索引。这里方便实现copy on write// 和xv6不同，我们在后面就会看到，Onix在实现fork的copy on write时，子进程的// 页表也是动态复制的。// 分配一页物理内存static u32 get_page() &#123; /* ... */ &#125;// 释放一页物理内存static void put_page(u32 addr) &#123;/* ... */&#125;// 该函数是基于位图实现，利用位图来管理内核的内存，只有页目录的内存分配会使用内核内存，// 页表、页框的内存分配全部通过get_page()函数。内核页表也不用相xv6那样去映射所有物理页，// 另外一个优点是该函数可以连续分配多个内核页，xv6就存在最大只能分配1个page的限制！// src/kernel/memory.c// 分配 count 个连续的内核页u32 alloc_kpage(u32 count) &#123; /* ... */ &#125;// 释放 count 个连续的内核页void free_kpage(u32 vaddr, u32 count) &#123; /* ... */ &#125;// src/kernel/area.c// 该函数是更加细粒度的分配内核内存，可以分配小于4K的小块内存。内部有一个内存池，// 对细小内存进行池化管理。void *kmalloc(size_t size) &#123; /* ... */ &#125;void kfree(void *ptr) &#123; /* ... */ &#125;\n\n这里可以计算一下：如果以1byte为单位去管理1M的页（1M * pagesieze（4096）&#x3D;&#x3D; 4G），1页可以管理4096页内存，所以1M的页需要 （1024 * 1024） &#x2F; （4 * 1024） &#x3D; 256页。\n回到kinit1函数，kinit1函数执行第一个阶段的内核内存管理的初始化。它会把从内核代码结束的位置开始往上4M的位置为止，以page为单位将每一页利用链表串起来。这部分涉及代码非常简单，直接看源码会比较清晰，这里就不贴它的代码了。大概的结构图如下：\n\nkvmalloc：构造并安装更加细粒度的页表kpgdir首先明确，到目前为止cpu使用的entrypgdir是4M big page的页表。kvmalloc会构造一个页表kpgdir，该页表会使用4K的页，分页粒度更加细粒，相对于entrypgdir来说做了进一步细分。kpgdir的内核映射的地址是没有任何变化的，切换到kpgdir页表后，内核的地址空间和之前没有什么区别。唯一的区别就是kpgdir的地址映射范围更大，多了一些entrypgdir页表所不可访问的范围。从数组kmap可以看到，pgdir将原来4M的内核空间扩张到了PHYSTOP（224M），同时将物理内存的高位的设备空间直接映射到内核高位虚拟地址空间。XV6在启动阶段没有进行内存探测，只是暴力的将可分配物理内存规定为PHYSTOP（224M）大小，我们可以通过修改PHYSTOP来动态调整XV6的物理内存上限。但是由于PHYSTOP被映射到了KERNBASE之上，所以PHYSTOP的大小是有限制的，即使物理内存是4G，KERNBASE + PHYSTOP的大小也不能覆盖设备地址空间。\nstatic struct kmap &#123;  void *virt;  uint phys_start;  uint phys_end;  int perm;&#125; kmap[] = &#123; &#123; (void*)KERNBASE, 0,             EXTMEM,    PTE_W&#125;, // I/O space &#123; (void*)KERNLINK, V2P(KERNLINK), V2P(data), 0&#125;,     // kern text+rodata &#123; (void*)data,     V2P(data),     PHYSTOP,   PTE_W&#125;, // kern data+memory &#123; (void*)DEVSPACE, DEVSPACE,      0,         PTE_W&#125;, // more devices&#125;;\n\n直接上代码：\n下面这个函数主要作用就是根据页表，用软件模拟MMU的方式，找到传进来的虚拟地址va对应的页表项的虚拟地址，这里用语言表述的可能有点绕。代码从某种程度上比语言好懂。如果alloc为1，在对应的页表不存在时就创建一个页表：\n// 简单讲，就是返回va对应页表项的虚拟地址static pte_t *walkpgdir(pde_t *pgdir, const void *va, int alloc)&#123;  pde_t *pde;   // vaddr 虚拟地址  pte_t *pgtab; // vaddr 虚拟地址  pde = &amp;pgdir[PDX(va)];                          // 虚拟地址va对应的页表所在页目录项  if(*pde &amp; PTE_P)&#123;    pgtab = (pte_t*)P2V(PTE_ADDR(*pde));          // 存在，转换页表物理地址为虚拟地址  &#125; else &#123;    if(!alloc || (pgtab = (pte_t*)kalloc()) == 0) // 不存在就创建      return 0;    // Make sure all those PTE_P bits are zero.    memset(pgtab, 0, PGSIZE);                     // 清零，防止垃圾值干扰    // The permissions here are overly generous, but they can    // be further restricted by the permissions in the page table    // entries, if necessary.    *pde = V2P(pgtab) | PTE_P | PTE_W | PTE_U;    // 用物理地址填充页目录项  &#125;  return &amp;pgtab[PTX(va)];&#125;\n\nmappages函数作用就简单了，利用walkpgdir函数将传进来的虚拟地址映射到物理地址上。当然，如果页框的页表还没被映射，walkpgdir就会创建页表项：\n// Create PTEs for virtual addresses starting at va that refer to// physical addresses starting at pa. va and size might not// be page-aligned.static intmappages(pde_t *pgdir, void *va, uint size, uint pa, int perm)&#123;  char *a, *last;  pte_t *pte;  a = (char*)PGROUNDDOWN((uint)va);                 // 4k对齐  last = (char*)PGROUNDDOWN(((uint)va) + size - 1); // 结尾对齐  for(;;)&#123;    if((pte = walkpgdir(pgdir, a, 1)) == 0)         // 查询虚拟地址a的页表项，不存在就创建。      return -1;    if(*pte &amp; PTE_P)      panic(&quot;remap&quot;);    *pte = pa | perm | PTE_P;                       // 填充页表项，让页表项指向页框。    if(a == last)      break;    a += PGSIZE;    pa += PGSIZE;  &#125;  return 0;&#125;\n\nsetupkvm函数利用mappages根据数组kmap来构造一个正式的内核页表：kpgdir：\n// Set up kernel part of a page table.pde_t*setupkvm(void)&#123;  pde_t *pgdir;  struct kmap *k;  if((pgdir = (pde_t*)kalloc()) == 0)   // 分配页目录    return 0;  memset(pgdir, 0, PGSIZE);             // 清零，以免野值干扰  if (P2V(PHYSTOP) &gt; (void*)DEVSPACE)   // PHYSTOP要合理，不能占用设备io空间    panic(&quot;PHYSTOP too high&quot;);  for(k = kmap; k &lt; &amp;kmap[NELEM(kmap)]; k++)    // 这里主要就是对内核映射进行细化，因为最开始内核是4M big page映射    if(mappages(pgdir, k-&gt;virt, k-&gt;phys_end - k-&gt;phys_start,                (uint)k-&gt;phys_start, k-&gt;perm) &lt; 0) &#123;      freevm(pgdir);      return 0;    &#125;  return pgdir;&#125;// Allocate one page table for the machine for the kernel address// space for scheduler processes.voidkvmalloc(void)&#123;  kpgdir = setupkvm();  switchkvm();&#125;\n\n在阅读上面这段代码时，一定要分清哪里是物理地址，哪里是物理地址。读者一定要时刻铭记，只要开启分页后，一切地址都是虚拟地址，但是页目录项、页表项都是存的物理地址，我们在拿到物理地址后，在访问前需要将他们进一步转换为虚拟地址。V2P、P2V的实现代码上面已经贴过，就是减去或者加上一个偏移量即可。\n我们的内存空间又有了一些变化：\n\nkinit2：初始化内核内存分配器（阶段2）。根据注释要求：该函数是在所有其他AP（从处理器）处理器启动之后调用。在查看startothers的实现得知，BSP会为每个AP核都会单独分配一个（scheduler的）内核栈，所以结合kfree的头插法实现，我猜测，XV6是要让这（scheduler的）内核栈在KERNBASE + 4M以内，这暗示了只有KERNBASE + 4M才是内核真正的内存，尽管物理内存和内核内存都在kmem上混合管理。\n回归正题，和kinit1类似，kinit2就是将KERNBASE + 4M到KERNBASE + PHYSTOP之间的物理内存页正式挂到kmem的freelist上。进程页表、页目录、页框的内存分配统一由kalloc来做。（又一次强调！）\n最后，cpu的状态如下：关中断 &amp; 进入了保护模式 &amp; 安装了包括【内核&amp;用户】【代码&amp;数据】段描述符 &amp; BSP分配了一个4K的内核栈 &amp; 开启了以kmap为基础的粒度为4K的内核分页。\n至此XV6的内存管理的介绍差不多结束，\n简单聊一下内存管理在Onix中的做法首先再次强调，XV6将所有物理地址映射到内核页表并且统一交由kalloc管理的做法极大简化了我们对页目录、页表的修改。但这种将所有物理内存都映射到内核页表的做法是不明智的！\nOnix内核页表采用的是直接映射，直接将Onix内核虚拟地址空间映射到物理地址空间。前面提到过，Onix操作系统对内存的管理采用：get_page + alloc_kpage + kmalloc的方式。get_page（管理单位：page）管理的是整个4G物理内存空间，底层是一个256页的大数组，每一byte记录4G物理内存的一个page的引用计数；alloc_kpage（管理单位：page）管理的就是Onix内核（虚拟）页内存，底层是位图，Onix初始化时会向大数组中申请内核所需要的连续的物理内存（将物理数组的byte置为1），申请的内核页会以位图的方式交由alloc_kpage管理；kmalloc（管理单位：小于page的内存碎片）管理内核的小块（虚拟）内存分配与释放，实现较为复杂，底层它是一个小块内存的内存池，实现上和C++中标准库的内存分配器类似。感兴趣的读者可以去深入阅读Onix的代码。这里了解Onix的内存管理基本架构即可。\nOnix内存布局如下，盗取了Onix作者的图：\n\n关于虚拟内存，Onix同样采用了两级页表的形式，所不同的是，Onix中进程的页表是由内核页内存分配器alloc_kpage负责，而像进程的页表、页框则是由get_page负责。\n这就引入了一个问题，Onix内核页表并不会像XV6那样，把所有的物理内存页都映射到内核页上，那么Onix是如何实现修改页表、页框的操作的呢？\n答案就是将页目录的最后一个entry指向自己，这里还是引用Onix作者画的页表映射图为例：\n\n从图中我们可以看到，页目录的最后一个entry指向了页目录本身。（0x1007中低12位是控制位！\n有了这个前提，我们的页目录就有两种访问方式：\n\n直接通过页目录的虚拟地址访问。\n\n通过虚拟地址：0xfffff000访问。\n\n\n你可以在脑海中模拟MMU映射地址过程，试着去翻译地址0xfffff000，最终你一定会发现，翻译后的物理地址确实指向页目录本身。这真的是一种很神奇的方法。\n同理在Onix中，如果你想访问一个虚拟地址的页表，你可以且只能使用：\n\n虚拟地址：0xFFC00000 | 虚拟地址在页目录中的索引（虚拟地址高10位）\n\n至此，我们通过“欺骗”MMU的方式实现了，即使我们的内核页表没有映射进程的页表、页框的物理地址，我门的程序还是可以去修改进程的页表。\nOnix操作页表、页框的核心函数如下：\n// 获取页目录vdstatic page_entry_t *get_pde()&#123;    return (page_entry_t *)(0xfffff000);&#125;// 获取虚拟地址 vaddr 对应的页表vdstatic page_entry_t *get_pte(u32 vaddr, bool create)&#123;    page_entry_t *pde = get_pde();    u32 idx = DIDX(vaddr);    page_entry_t *entry = &amp;pde[idx];    assert(create || (!create &amp;&amp; entry-&gt;present));    page_entry_t *table = (page_entry_t *)(PDE_MASK | (idx &lt;&lt; 12));    if (!entry-&gt;present)    &#123;        LOGK(&quot;Get and create page table entry for 0x%p\\n&quot;, vaddr);        u32 page = get_page();        //页目录、页表项的项必须存物理地址!!!        entry_init(entry, IDX(page));        memset(table, 0, PAGE_SIZE);    &#125;    return table;&#125;page_entry_t *get_entry(u32 vaddr, bool create)&#123;    page_entry_t *pte = get_pte(vaddr, create);    return &amp;pte[TIDX(vaddr)];&#125;\n\n再有一次，强烈建议各位想深入学习Unix-like操作系统的同学去阅读一下Onix的源码。链接放在文章的开头了。\n参考文档：\nXV6中文文档：https://th0ar.gitbooks.io/xv6-chinese/content/\nOnix分页机制（含）的文档：https://github.com/StevenBaby/onix/blob/dev/docs/05%20%E5%86%85%E5%AD%98%E7%AE%A1%E7%90%86/041%20%E5%86%85%E5%AD%98%E6%98%A0%E5%B0%84%E5%8E%9F%E7%90%86.md\nCR0寄存器详解：https://blog.csdn.net/qq_30528603/article/details/131143850\n\n本章完结\n","tags":["类Unix源码剖析"]},{"title":"这才是计科之 Onix & XV6 源码分析（3、Unix-like系统的进程调度模块）","url":"/2024/06/08/xv6/Scheduler/","content":"前言前面已经分析了XV6的启动流程以及内存管理，接下来，我们探究进程调度的实现。与其说进程调度，我觉得可以顺应内存的虚拟化的叫法，将进程调度称为“CPU的虚拟化”更加贴切。\n首先明确目前XV6的cpu的状态如下：关中断 &amp; 进入了保护模式 &amp; 安装了包括【内核&amp;用户】【代码&amp;数据】段描述符 &amp; BSP分配了一个4K的内核栈 &amp; 开启了以kmap为基础的粒度为4K的内核分页。\nOnix相关链接：\n\ngithub仓库链接。\n\nB站配套视频链接。\n\n\nXV6-x86的github链接：\n\n链接。\n\n\n\n中断机制对于中断的部分，这里会涉及大量硬件相关的知识，由于博主的目的是了解OS的基本框架，所以硬件相关的知识储备可能不会太深，如果你是想弄清某个硬件具体实现，这篇博客可能不适合你。\n这里总结一下我对中断的理解：\n引发中断的方式有三种：外中断、异常、软中断。\n\n外中断：就是由外部中断控制器通知 CPU 某个事件完成了，比如：磁盘寻道完成可以进行读写了、UART输入寄存器非空（可读）、UART输出寄存器为空（可写）、键盘缓冲有数据了（可读）等等。\n\n异常是 CPU 在执行过程中，因为出错而执行不下去了，比如：除零异常、因为虚拟页面还没映射发生缺页异常、对只读段进行写操作触发段错误异常等等。\n\n软中断，可以认为是应用程序和操作系统沟通的一种方式，运行在低优先级程序想要对硬件做IO，但是由于只有处于特权级的内核能够直接和设备打交道，从而低优先级程序必须通过某种机制来完成特权级转换，这种机制就是软中断。我们也可以将实现这种功能的函数称为系统调用。\n\n\n如有些教科书那样，我们也可以把异常和软中断统称为 内中断，也就是这个中断时 CPU 和 软件内部产生的，与外部硬件无关。\nOnix单核处理器的中断原理单核PC机上，一般会采用（主从）两片 8259a PIC（programmable interrupt controller），将PIC的INT引脚接到CPU的一个引脚上，如下图，图片引用自onix的文档，如有侵权，可告知删除：\n\n从图中可以看到有两个8259a，上面那个8259a是主PIC，它的INT引脚直接接到CPU上；下面那个8259a的INT引脚接到主PIC的IR2引脚，所以它是从PIC。 每一个PIC的引脚会接一个外设，（如果对应的引脚没被屏蔽的话）外设会通过PIC间接向CPU发中断。\n在PIC正式工作前，需要对其进行一系列初始化。初始化操作由cpu发送一系列的控制字完成。有两类控制字：\n\n初始化命令字 (Initialization Command Words, ICW), ICW 共 4 个， ICW1 ~ ICW4；\n\n操作命令字 (Operation Command Word, OCW), OCW 共 3 个， OCW1 ~ OCW3；\n\n\nICW 做初始化，用来确定是否需要级联，设置起始中断向量号，设置中断结束模式。因为某些设置之间是具有依赖性的，也许后面的某个设置会依赖前面某个 ICW 写入的设置，所以这部分要求严格的顺序，必须依次写入 ICW1、ICW2、ICW3、ICW4；\nOCW 来操作控制 8259A，中断的屏蔽和中断处理结束就是通过往 8259A 端口发送 OCW 实现的。OCW 的发送顺序不固定，3 个之中先发送哪个都可以。\n具体细节非常推荐读者去阅读一下Onix文档，讲的真的很细致：https://github.com/StevenBaby/onix/blob/dev/docs/04%20%E4%B8%AD%E6%96%AD%E5%92%8C%E6%97%B6%E9%92%9F/033%20%E5%A4%96%E4%B8%AD%E6%96%AD%E6%8E%A7%E5%88%B6%E5%99%A8.md\n8259a中断控制器的初始化就是一种固定套路，截取Onix代码如下：\n#define PIC_M_CTRL 0x20 // 主片的控制端口#define PIC_M_DATA 0x21 // 主片的数据端口#define PIC_S_CTRL 0xa0 // 从片的控制端口#define PIC_S_DATA 0xa1 // 从片的数据端口#define PIC_EOI 0x20    // 通知中断控制器中断结束// 初始化中断控制器void pic_init()&#123;    // 主PIC    outb(PIC_M_CTRL, 0b00010001); // ICW1: 边沿触发, 级联 8259, 需要ICW4.    outb(PIC_M_DATA, 0x20);       // ICW2: 起始中断向量号 0x20    outb(PIC_M_DATA, 0b00000100); // ICW3: IR2接从片.    outb(PIC_M_DATA, 0b00000001); // ICW4: 8086模式, 正常EOI    // 从PIC    outb(PIC_S_CTRL, 0b00010001); // ICW1: 边沿触发, 级联 8259, 需要ICW4.    outb(PIC_S_DATA, 0x28);       // ICW2: 起始中断向量号 0x28    outb(PIC_S_DATA, 2);          // ICW3: 设置从片连接到主片的 IR2 引脚    outb(PIC_S_DATA, 0b00000001); // ICW4: 8086模式, 正常EOI    outb(PIC_M_DATA, 0b11111111); // OCW1：屏蔽字，关闭主PIC所有中断，后面需要什么中断再依据需求打开。    outb(PIC_S_DATA, 0b11111111); // OCW1：屏蔽字，关闭从PIC所有中断，后面需要什么中断再依据需求打开。&#125;\n\nCPU在处理每个外中断后，需要向PIC发生一个结束字为的是通知PIC中断处理结束，具体中断结束方式由OCW2 来设置。代码如下：\n// 通知中断控制器，中断处理结束void send_eoi(int vector)&#123;    if (vector &gt;= 0x20 &amp;&amp; vector &lt; 0x28)    &#123;        outb(PIC_M_CTRL, PIC_EOI);    &#125;    if (vector &gt;= 0x28 &amp;&amp; vector &lt; 0x30)    &#123;        outb(PIC_M_CTRL, PIC_EOI);        outb(PIC_S_CTRL, PIC_EOI);    &#125;&#125;\n\n至此单核OS中断的初始化到这里就结束了。\nXV6多核处理的中断原理多核处理器中断控制器的结构更为复杂，因为偏向硬件，这里就只记录一下我对APIC的理解，理解不会太深，如果有错误，非常欢迎读者纠正！\n首先还是供上架构框图：\n\n图片截取自：https://pdos.csail.mit.edu/6.828/2014/readings/ia32/MPspec.pdf\n注释：\nBSP：bootstrap processor，可以简单理解为主处理器。\nAPx：application processors ，可以简单理解为从处理器。\n关于BSP和APx的关系这里先埋个伏笔。在【AP（从）处理器的启动】段落会进行详细叙述。\n从图中的上半部分可以了解到，每个CPU各自接着一个Local APIC（Advanced Programmable Interrupt Controller）。注意，每个lapic和cpu是封装在一起的（这里并不严谨，是否和cpu封装在一起其实和cpu的架构有关）。后面我们会看到，不止APIC，在多核cpu上，它的每一个cpu都有自己的一套cpu寄存器，比如：esp、eip、eflag等等。具体情况我们在“进程调度”段落进行详细讲解。\n从图的下半部分，我们可以了解到所有的lapic都接到了ICC（interrupt controller communications） Bus上，并且，总线上还接了一个IO APIC，这里的ioapic是和cpu分离的，它被接在cpu的外部。ioapic会接收来自各个外设的中断。然后对各个外设发来的中断做一些判断和处理，再将中断的IRQ号和lapic的ID封装在一条“报文”中分发給对应的lapic，具体发给哪些lapic，我们可以通过配置ioapic来进行设置。ioapic左边其实是有很多引脚（实际16个，如果级联了8259a可能会更多）每个引脚都可以接外设。有趣的是，从图中ioapic的左边可以看到，ioapic的引脚还可以接8259a PIC控制器，这非常完美的兼容了单核cpu的中断控制器的架构。\niopic是依据重定向表项 RTE(Redirection Table Entry)来构建“报文“，RTE对每一个中断都会有一项64位的entry。通过entry，可以单独设置ioapic在收到中断后对中断的操作。每一项entry描述：中断对应的中断向量？中断有没有使能？中断传输状态？发给哪个lapic？\n每个lapic都有一个唯一的ID，cpu可以在特定的内存（device space）上来查询自己所对应的lapic的ID号，lapic的ID其实也唯一标识了一个cpu。lapic会根据自己的ID从ICC Bus上接收属于自己的中断”报文“，然后经过一系列检查最后将中断发给cpu，当cpu处理完中断后，会反馈给自己的lapic，lapic收到cpu的回复后，同样将中断处理完毕的消息通知给ioapic，这点和单核架构中，cpu处理完中断后向master pic发送PIC_EOI是一样的道理。\n特别的是，lapic也可以像ioapic那样作为中断“源”（这里可能不严谨，但是可以类比去理解），向其他的lapic发送中断“报文”，这是通过ICR(Interrupt Command Register)寄存器实现，ICR的结构和ioapic的RTE表的entry结构类似，也有中断向量号、lapic的ID等字段。lapic主动向其他lapic发送中断“报文”最常见的场景就是BSP去启动其他APs，这一般通过会发送INIT or STARTUP IPI（interprocessor interrupts）。\n由于XV6中cpu对lapic、iopic初始化代码上，依赖于mpinit函数，而mpinit和多处理器内容相关，所以lapic、iopic初始化我们放到”AP（从）处理器的启动”段落进行讨论。\n关于Local APIC和IO APIIC详细内容可以参考博客：https://blog.csdn.net/weixin_46645613/article/details/119207945\n中断描述符表之前一直在介绍中断相关的外设，接下来我们看看cpu内部是怎么利用寄存器来定义中断的。\n因为中断不止一个，所以，和全局描述符类表似，中断表也是通过一个大的数组来记录每一个中断的属性。数组中每一个Entry格式如下图，每一个Entry同样是8个字节：\n\n\nOffset：记录中断门或陷阱门的处理函数的地址。\n\nSelector：处理函数的段选择子。\n\nType：标记是中断门还是陷阱门。 注意：中断门会自动清除eflag寄存器的FL_IF标志位，而陷阱门则保留eflag的FL_IF标志位。 也即中断门会i自动关（外）中断，而陷阱门则不会有关中断的操作！\n\nS：必须为0。\n\nDPL：描述符可以被哪个特权级使用。对于中断门一般是0x0，对于陷阱门就是0x3（DPL_USER）。\n\nP：是否有效，固定填1.\n\n\nXV6相关代码注释写的非常好，上面的中文注释也是参考XV6的注释写的，如下：\n// Gate descriptors for interrupts and trapsstruct gatedesc &#123;  uint off_15_0 : 16;   // low 16 bits of offset in segment  uint cs : 16;         // code segment selector  uint args : 5;        // # args, 0 for interrupt/trap gates  uint rsv1 : 3;        // reserved(should be zero I guess)  uint type : 4;        // type(STS_&#123;IG32,TG32&#125;)  uint s : 1;           // must be 0 (system)  uint dpl : 2;         // descriptor(meaning new) privilege level  uint p : 1;           // Present  uint off_31_16 : 16;  // high bits of offset in segment&#125;;// Set up a normal interrupt/trap gate descriptor.// - istrap: 1 for a trap (= exception) gate, 0 for an interrupt gate.//   interrupt gate clears FL_IF, trap gate leaves FL_IF alone// - sel: Code segment selector for interrupt/trap handler// - off: Offset in code segment for interrupt/trap handler// - dpl: Descriptor Privilege Level -//        the privilege level required for software to invoke//        this interrupt/trap gate explicitly using an int instruction.#define SETGATE(gate, istrap, sel, off, d)                \\&#123;                                                         \\  (gate).off_15_0 = (uint)(off) &amp; 0xffff;                \\  (gate).cs = (sel);                                      \\  (gate).args = 0;                                        \\  (gate).rsv1 = 0;                                        \\  (gate).type = (istrap) ? STS_TG32 : STS_IG32;           \\  (gate).s = 0;                                           \\  (gate).dpl = (d);                                       \\  (gate).p = 1;                                           \\  (gate).off_31_16 = (uint)(off) &gt;&gt; 16;                  \\&#125;\n\n中断描述符寄存器如下：\n高32位存放中断描述符表的基地址，低16位存放中断描述符表的大小（字节为单位）。\n\nAP（从）处理器的启动首先还是回归main函数：\n// Bootstrap processor starts running C code here.// Allocate a real stack and switch to it, first// doing some setup required for memory allocator to work.intmain(void)&#123;  kinit1(end, P2V(4*1024*1024)); // 内存内存管理已讲  kvmalloc();      // 内存内存管理已讲  mpinit();        // detect other processors  lapicinit();     // interrupt controller  seginit();       // 内存内存管理已讲  picinit();       // disable pic // 禁用单核架构下的8259A，实现很简单，这里就不去贴代码了  ioapicinit();    // another interrupt controller  // ...  pinit();         // process table // 实现很简单就是对自ptable的自旋锁进行一个初始化。读者可以自行阅读代码，这里就不过多赘述。  tvinit();        // trap vectors  // ...  startothers();   // start other processors  kinit2(P2V(4*1024*1024), P2V(PHYSTOP)); // 内存内存管理已讲  userinit();      // first user process // 就是对init进程的内核栈做一个初始化。利用了ROP（面向返回点编程）编程思想。因为这又是一个大主题，本文不会过多讲解。以后有机会在另开一篇博客专门讨论。  mpmain();        // finish this processor&#x27;s setup // 进入mpmain&#125;// Other AP CPUs jump here from entryother.S.static voidmpenter(void)&#123;  switchkvm();  // AP的内核页表换成kpgdir  seginit();    // 内存内存管理已讲  lapicinit();  // 作用同main函数中BSP核执行的lapicinit函数  mpmain();     // 进入mpmain&#125;// BSP、APS最终都会进入改函数。// 主要工作就是//    1、加载中断描述表。//    2、设置状态，已启动。//    3、 进入调度循环。// Common CPU setup code.static voidmpmain(void)&#123;  cprintf(&quot;cpu%d: starting %d\\n&quot;, cpuid(), cpuid());  idtinit();       // load idt register // 使用lidt命令加载中断描述符  xchg(&amp;(mycpu()-&gt;started), 1); // tell startothers() we&#x27;re up  scheduler();     // start running processes&#125;\n\n接下我们要从BSP执行的maiin函数开始，深入分析以上代码的作用。\nmpinit：探测各个cpu该部分主要参考：多处理器规范，因为我英语也是很菜，所以硬着头皮捡重点去看了一部分。\n这里对多核处理器的启动流程做一个简单总结：我们可以理解为，多核CPU中，有一个CPU被设计成BSP，其他的CPU都被设计成AP。当然，在实际硬件设计上为了考虑容错性，任何一个CPU都能成为BSP核。 系统最开始，BSP有对硬件的绝对控制权，包括去控制其他AP的启动和停止。为了启动其他AP核，BSP首先通过三种可能的方式搜索MP floating pointer structure，如果找到了一个有效的MP floating pointer structure就去遍历MP configuration table查询处理器信息和ioapic的信息；如果无法找到一个有效MP floating pointer structure，那就认为系统只有一个CPU——BSP。在所有CPU启动后，BSP就退化成AP，系统不存在BSP、AP之分。 当然，我们需要要记录BSP CPU的lapic的ID（这个ID也唯一标识着CPU），这样我们才知道谁可以去其控制其他CPU的停止。在BSP启动其他AP前，因为AP CPU是暂停状态，所以其他AP无法执行OS代码，并且大部分中断都是被禁用，但是INIT or STARTUP interprocessor interrupts (IPIs)不会被屏蔽，当AP收到来自BSP的INIT or STARTUP中断，就会启动它自己。 AP在收到BSP的启动中断后，也会进入保护模式、有自己的独立的一套寄存器、设置自己的全局描述符、开启分页、有自己的堆栈等。\n首先BSP会通过三种方式去搜索MP floating pointer structure，三种搜索范围都在1M以内，因为MP floating pointer structure就是由BIOS提供，而BISO寻址范围就1M：\n\nIn the first kilobyte of Extended BIOS Data Area (EBDA), or\nWithin the last kilobyte of system base memory, or\nIn the BIOS ROM address space between 0F0000h and 0FFFFFh.\n\n低1M内存的内存映射参考：https://wiki.osdev.org/BDA#BIOS_Data_Area_.28BDA.29\nMP Configuration Data Structures整体框架如下图，图解了MP floating pointer structure、MP Configuration Table Header、Table Entries三者之间的一个关系，先了解一下大致的框架，接下来我门逐一剖析。\n\nMP floating pointer structure图解如下：\n\n主要关注它的PHYSICAL ADDRESS POINTER，它指向MP config table的物理地址。\nMP Configuration Table Header结构如下：\n\n主要关注几个字段：\n\nMEMORY-MAPPED ADDRESS OF LOCAL APIC：描述 cpu（每个CPU都将它的lapic映射到了同一个物理地址）的lapic的寄存器物理地址。注意这里是”每个cpu”，虽然是同一个物理地址，但是在每一个cpu去读的时候，分别映射到了各自的lapic的寄存器地址上了。\n\nBASE TABLE LENGTH：整个table的长度，虽然存在扩展表长度，但是我们还用不到。\n\n\nMP Configuration Table Header后面会跟上各自类型的Base MP Configuration Table Entries，每个Entry的第一个字节会标明其类型，并且每种Entry的长度都各自固定，所以我们可以通过一个循环来遍历每个Entry，一共有5种类型的Entry，如下图：\n\nXV6中我们主要关注Processor Entries和I&#x2F;O APIC两种类型的Entry。\nProcessor Entries结构如下：\n\n主要关注LOCAL APIC ID，如该字段名字那样，就是代表和CPU绑定的lapic的ID，通过它我们也可以唯一标识一个CPU。\nI&#x2F;O APIC Entries结构如下：\n\n主要关注I&#x2F;O APIC ID，代表I&#x2F;O APIC的ID。\n然后上代码：\nvoidmpinit(void)&#123;  uchar *p, *e;  int ismp;  struct mp *mp;              // 前面提到的MP floating pointer structure  struct mpconf *conf;        // 前面提到的MP Configuration Table Header  struct mpproc *proc;        // 前面提到的Processor Entries  struct mpioapic *ioapic;    // 前面提到的I/O APIC Entries  if((conf = mpconfig(&amp;mp)) == 0)     // 用上面提到的三种方法寻找MP floating pointer structure，并且判断它的合法性，然后将结构体里面的PHYSICAL ADDRESS POINTER（指向MP Configuration Table Header）作为返回值    panic(&quot;Expect to run on an SMP&quot;);  ismp = 1;  lapic = (uint*)conf-&gt;lapicaddr;     // 将lapic的寄存器地址放到全局变量lapic中  for(p=(uchar*)(conf+1), e=(uchar*)conf+conf-&gt;length; p&lt;e; )&#123;  // 依据MP Configuration Table Header遍历每一个Table Entry。    switch(*p)&#123;    case MPPROC:                      // Processor Entries      proc = (struct mpproc*)p;      if(ncpu &lt; NCPU) &#123;        cpus[ncpu].apicid = proc-&gt;apicid;  // apicid may differ from ncpu // 保存cpu的lapic的id        ncpu++;                       // 找到一个CPU      &#125;      p += sizeof(struct mpproc);      continue;    case MPIOAPIC:                    // I/O APIC Entries      ioapic = (struct mpioapic*)p;      ioapicid = ioapic-&gt;apicno;      // 将ioapicid存到全局变量，将来初始化的时候会用      p += sizeof(struct mpioapic);      continue;    case MPBUS:                       // 其他的不关注，加上他们的大小去找下一个Table Entry。    case MPIOINTR:    case MPLINTR:      p += 8;      continue;    default:      ismp = 0;      break;    &#125;  &#125;  if(!ismp)    panic(&quot;Didn&#x27;t find a suitable machine&quot;);  if(mp-&gt;imcrp)&#123;    // 如果之前是PIC Mode，就切换到APIC模式     // interrupt mode configuration register --&gt; IMCR    // Bochs doesn&#x27;t support IMCR, so this doesn&#x27;t run on Bochs.    // But it would on real hardware.    outb(0x22, 0x70);   // Select IMCR    outb(0x23, inb(0x23) | 1);  // Mask external interrupts.  // 进入APIC模式  &#125;&#125;\n\nmp-&gt;imcrp字段的解释如下：\n\n详细信息可以了解一下：多处理器规范\nlapicinit：BSP初始化自己cpu的lapic这部分和硬件强相关，我也了解不是特别多，尽可能的讲清楚吧。硬件相关的初始化深入下去也是一个无底洞。如果读者感兴趣的话，可以去查intel 64 and IA-32 卷3开发手册。 \nvoidlapicinit(void)&#123;  if(!lapic)    // lapic就是上面mpinit函数在MP Configuration Table Header中得到的lapic寄存器的物理映射地址，所以说初始化的顺序非常严格！    return;  // Enable local APIC; set spurious interrupt vector.  lapicw(SVR, ENABLE | (T_IRQ0 + IRQ_SPURIOUS));      // 开启此cpu的APIC  // 配置时钟，属于lapic的本地中断  // The timer repeatedly counts down at bus frequency  // from lapic[TICR] and then issues an interrupt.  // If xv6 cared more about precise timekeeping,  // TICR would be calibrated using an external time source.  lapicw(TDCR, X1);                               // 分频系数为X1  lapicw(TIMER, PERIODIC | (T_IRQ0 + IRQ_TIMER)); // 以PERIODIC为周期，映射到0x20中断  lapicw(TICR, 10000000);                         // 时钟频率  // Disable logical interrupt lines.  lapicw(LINT0, MASKED);                          // 屏蔽LINT0（lapic本地中断  lapicw(LINT1, MASKED);                          // 屏蔽LINT1（lapic本地中断  // Disable performance counter overflow interrupts  // on machines that provide that interrupt entry.  if(((lapic[VER]&gt;&gt;16) &amp; 0xFF) &gt;= 4)    lapicw(PCINT, MASKED);                        // 屏蔽PCINT（好像是废话？  // Map error interrupt to IRQ_ERROR.  lapicw(ERROR, T_IRQ0 + IRQ_ERROR);            // 0x20 + 0x13（ERROR）  // Clear error status register (requires back-to-back writes).  lapicw(ESR, 0);  lapicw(ESR, 0);  // Ack any outstanding interrupts.  lapicw(EOI, 0);                               // 向ioapic发送一个EOI，以免中断丢失  // Send an Init Level De-Assert to synchronise arbitration ID&#x27;s.  lapicw(ICRHI, 0);                             // ？？？// 这里应该是向其他apic广播一条“报文”，通知“我”启动了  lapicw(ICRLO, BCAST | INIT | LEVEL);          // 广播的形式 &amp; 传送模式为INIT &amp; 水平触发  while(lapic[ICRLO] &amp; DELIVS)                  // 中断的传输状态：是否已经发送？    ;  // Enable interrupts on the APIC (but not on the processor).  lapicw(TPR, 0);&#125;\n\n以上代码就是初始化BSP的lapic，在其他AP启动后，都要执行一遍这段代码。\nioapicinit：初始化ioapicioapic的作用和单核架构下master pic很像，但是对于ioapic的初始化步骤很简单，不需要发送一系列的控制字。对于ioapic的初始化就是简单的配置一下重定向表项 RTE(Redirection Table Entry)，给RTE的每一项一个初值，设置它的中断向量号（起始T_IRQ0，T_IRQ0 &#x3D;&#x3D; 0x20），并且默认是中断屏蔽的。后续需要什么中断再对相应的Entry做配置即可，比如consoleinit为了使用键盘调用了ioapicenable去配置对应的Entry打开中断等。关于重定向表项 RTE(Redirection Table Entry)的解释，读者可以看一下这篇文章，讲的非常详细：https://blog.csdn.net/weixin_46645613/article/details/119207945\nvoidioapicinit(void)&#123;  int i, id, maxintr;  ioapic = (volatile struct ioapic*)IOAPIC;           // 默认地址：0xFEC00000   // Default physical address of IO APIC  maxintr = (ioapicread(REG_VER) &gt;&gt; 16) &amp; 0xFF;       // ioapic支持的最大中断号  id = ioapicread(REG_ID) &gt;&gt; 24;                      // 从寄存器读到的ioapic的ID  if(id != ioapicid)                                  // 必须一致    cprintf(&quot;ioapicinit: id isn&#x27;t equal to ioapicid; not a MP\\n&quot;);  // Mark all interrupts edge-triggered, active high, disabled,  // and not routed to any CPUs.  for(i = 0; i &lt;= maxintr; i++)&#123;    // 初始化重定向表项 RTE(Redirection Table Entry)，这里和单核架构下主从PIC一样，先将所有的中断屏蔽掉，在后面初始化的时候再按需使能。    ioapicwrite(REG_TABLE+2*i, INT_DISABLED | (T_IRQ0 + i));    ioapicwrite(REG_TABLE+2*i+1, 0);  &#125;&#125;\n\ntvinit：初始化中断向量表这里自顶向下介绍XV6的中断向量表是如何构造的。\n涉及到的变量如下：\n// Interrupt descriptor table (shared by all CPUs).struct gatedesc idt[256];   // 中断描述符表extern uint vectors[];  // vectors的定义文件是：由vectors.pl生成的汇编代码文件。\n\n首先是tvinit函数，它是最顶层负责构造中断向量表的函数，SETGATE宏在上面已经贴过它的实现，这里简单介绍一个各个参数的作用。\nSETGATE(gate, istrap, sel, off, d) ：\n\n参数1：对应idt[i]，表示每一项entry。\n\n参数2：标记是中断门还是陷阱门。\n\n参数3：段选择子。\n\n参数4：中断处理函数地址。\n\n参数5：中断描述符被哪个特权级使用。\n\n\n结合代码来看：\nvoidtvinit(void)&#123;  int i;  // 构造中断描述符表  for(i = 0; i &lt; 256; i++)    SETGATE(idt[i], 0, SEG_KCODE&lt;&lt;3, vectors[i], 0);  // 初始化系统调用中断描述符，类型是陷阱门（发生中断不会关中断），特权级是DPL_USER。  SETGATE(idt[T_SYSCALL], 1, SEG_KCODE&lt;&lt;3, vectors[T_SYSCALL], DPL_USER);  initlock(&amp;tickslock, &quot;time&quot;);&#125;\n\n然后是vectors.pl文件生成汇编代码的过程，pl我之前也没有了解过，不过从它的代码可以看出，有点像字符串拼接的处理语言，简化了重复性代码的编写，代码如下：\n#!/usr/bin/perl -w# Generate vectors.S, the trap/interrupt entry points.# There has to be one entry point per interrupt number# since otherwise there&#x27;s no way for trap() to discover# the interrupt number.print &quot;# generated by vectors.pl - do not edit\\n&quot;;print &quot;# handlers\\n&quot;;print &quot;.globl alltraps\\n&quot;;for(my $i = 0; $i &lt; 256; $i++)&#123;    print &quot;.globl vector$i\\n&quot;;    print &quot;vector$i:\\n&quot;;    if(!($i == 8 || ($i &gt;= 10 &amp;&amp; $i &lt;= 14) || $i == 17))&#123;        # 这些中断cpu自动压入errcode参数，        # 为保证中断栈帧的统一，所以我们在        # 这些特殊中断手动压入0值，这样就能        # 统一使用trapret来恢复上下文。        print &quot;  pushl \\$0\\n&quot;;    &#125;    print &quot;  pushl \\$$i\\n&quot;;    print &quot;  jmp alltraps\\n&quot;;&#125;print &quot;\\n# vector table\\n&quot;;print &quot;.data\\n&quot;;print &quot;.globl vectors\\n&quot;;print &quot;vectors:\\n&quot;;for(my $i = 0; $i &lt; 256; $i++)&#123;    print &quot;  .long vector$i\\n&quot;;&#125;\n\n从pl代码上我们可以看到，就是利用for循环构造vectors数组，该数组专门存放中断处理函数。我们先来分析一下它如何构造vectors的，首先最上面有一个for循环，for循环中使用了一个if判断，因为有些中断cpu不会自动压入错误码，所以我们需要手动压入一个占位值，方便trapret的处理。在for循环下面最后压入了一个jmp指令，所以pl生成的汇编并不是中断处理函数最终代码，pl生成的中断处理函数会跳到alltraps，alltraps代码我们下面再进行分析。pl在最后生成的汇编代码定义了一个vectors数组，数组里面元素就是上面定义的256个vectori（i&#x3D;1、2、…）。\n最后就是trapasm.S文件对alltraps的实现，常规的进行上下文保护：\n#include &quot;mmu.h&quot;  # vectors.S sends all traps here..globl alltrapsalltraps:          # 保存上下文  # Build trap frame.  pushl %ds  pushl %es  pushl %fs  pushl %gs  pushal    # Set up data segments.  movw $(SEG_KDATA&lt;&lt;3), %ax  movw %ax, %ds  movw %ax, %es  # Call trap(tf), where tf=%esp  pushl %esp  call trap       # 进入trap函数  addl $4, %esp  # Return falls through to trapret....globl traprettrapret:          # 恢复上下文  popal  popl %gs  popl %fs  popl %es  popl %ds  addl $0x8, %esp  # trapno and errcode  iret\n\ntvinit、pl、alltraps三者之间的关系总览图如下：\n\n关于中断帧，这里要注意，涉及特权级转换的中断帧和不涉及特权级转换的中断帧有些许不一样。如下：\n用户态触发中断（陷阱门或中断门）过程如下：\n\n用户查询TSS（任务状态段）段，找到用户进程在内核态的栈段和栈顶指针（ss0、esp0）。\n\ncpu将ss、esp压入（内核）栈中。（硬件\n\ncpu将eflags、cs、eip压入栈中。中断门还要关中断，陷阱门不用。（硬件\n\n执行用户中断处理函数alltraps的上下文保护的代码。（软件\n\n调用trap函数，处理各种中断。\n\n执行用户中断处理函数trapret的上下文恢复的代码。（软件\n\n调用iret，cpu恢复eflags、cs、eip。（硬件\n\ncpu恢复ss、esp。（硬件\n\n\n而内核线程发生中断（注意，内核态不会发生系统调用，这不应该也不合理），过程如下：\n\ncpu将eflags、cs、eip压入栈中。中断门还要关中断，陷阱门不用。（硬件\n\n执行用户中断处理函数alltraps的上下文保护的代码。（软件\n\n调用trap函数，处理各种中断。\n\n执行用户中断处理函数trapret的上下文恢复的代码。（软件\n\n调用iret，cpu恢复eflags、cs、eip。（硬件\n\n\nXV6中定义的栈帧结构体如下：\n//PAGEBREAK: 36// Layout of the trap frame built on the stack by the// hardware and by trapasm.S, and passed to trap().struct trapframe &#123;  // registers as pushed by pusha  uint edi;  uint esi;  uint ebp;  uint oesp;      // useless &amp; ignored  uint ebx;  uint edx;  uint ecx;  uint eax;  // rest of trap frame  ushort gs;  ushort padding1;  ushort fs;  ushort padding2;  ushort es;  ushort padding3;  ushort ds;  ushort padding4;  uint trapno;  // below here defined by x86 hardware  uint err;  uint eip;  ushort cs;  ushort padding5;  uint eflags;  // below here only when crossing rings, such as from user to kernel  uint esp;  ushort ss;  ushort padding6;&#125;;\n\n中断帧如下图：\n\n图片引用自：https://pdos.csail.mit.edu/6.828/2018/xv6/book-rev10.pdf\nstartothers：激活其他AP处理器到这里终于要开始启动其他AP核了，AP核的启动也是一种固定套路，在多处理器规范中这种套路称为universal algorithm。XV6中这个算法实现在lapicstartap函数中。\n流程如下：\n\nXV6代码启动其他AP处理器的核心流程如下：\n// Start the non-boot (AP) processors.static voidstartothers(void)&#123;  extern uchar _binary_entryother_start[], _binary_entryother_size[];  uchar *code;  struct cpu *c;  char *stack;  // Write entry code to unused memory at 0x7000.  // The linker has placed the image of entryother.S in  // _binary_entryother_start.  // AP核的entryother代码的入口点，entryother 就是 boot + entry的结合体  code = P2V(0x7000);  // 这里将entryother的代码移到物理地址0x7000处  memmove(code, _binary_entryother_start, (uint)_binary_entryother_size);  // BSP for循环向每个AP发送中断。  for(c = cpus; c &lt; cpus+ncpu; c++)&#123;    if(c == mycpu())  // We&#x27;ve started already.      continue;    // Tell entryother.S what stack to use, where to enter, and what    // pgdir to use. We cannot use kpgdir yet, because the AP processor    // is running in low  memory, so we use entrypgdir for the APs too.    stack = kalloc();   // 为每个AP核分配一个执行scheduler函数的内核栈    *(void**)(code-4) = stack + KSTACKSIZE;    *(void(**)(void))(code-8) = mpenter;  // 指定AP特有main（mpenter）函数入口点。    // 同BSP刚启动一样，使用使用4M big page页表。    *(int**)(code-12) = (void *) V2P(entrypgdir);    // lapicstartap就是实现了universal algorithm，因为我也不太懂，这里就不贴了。    lapicstartap(c-&gt;apicid, V2P(code));       // 等待AP初始化完毕    // wait for cpu to finish mpmain()    while(c-&gt;started == 0)        ;  &#125;&#125;\n\n在每个AP boot点和BSP的boot点类似，BSP是在0x7c00启动，AP是在0x7000启动。同样，0x7000也会执行一段汇编代码，这段汇编代码作用就是bootasm.S + entry.S代码的结合体。这里简单总结一下：\n\n加载临时全局描述符，进入保护模式\n\n使用entrypgdir开启分页。\n\n切换到预分配的内核（scheduler）栈。\n\n进入mpenter。\n\n\n考虑到文章太长，代码就不放了。文件是entryother.S，有兴趣的读者可自行研究。\nmpenter代码如下：\n// Other CPUs jump here from entryother.S.static voidmpenter(void)&#123;  switchkvm();    // 切换到粒度更小（4k）的内核页，kpgdir。  seginit();       //效果同BSP，在main中BSP也调用了该函数。就是第二次初始化段描述符。（第一次是在entryother.S  lapicinit();    // 效果同BSP  mpmain();       // 进入mpmain&#125;\n\n上面那段代码就是：AP会像BSP那样，调用一遍所有的和CPU相关初始化函数，最终进入mpmain。（BSP在main最后也会进入mpmain，前面提到过BSP启动其他后，也成为了一个AP）\nmpmain在加载中断描述符表后，最终就会进入scheduler，CPU正式开启操作系统的任务调度！\nmpmain如下：\n// Common CPU setup code.static voidmpmain(void)&#123;  cprintf(&quot;cpu%d: starting %d\\n&quot;, cpuid(), cpuid());  idtinit();       // load idt register  xchg(&amp;(mycpu()-&gt;started), 1); // tell startothers() we&#x27;re up  scheduler();     // start running processes&#125;\n\n进程调度首先了解一下XV6对CPU的定义，注释写的非常详细：\n// Per-CPU statestruct cpu &#123;  // lapci的id  uchar apicid;                // Local APIC ID  // 执行调度器栈指针  struct context *scheduler;   // swtch() here to enter scheduler  // 任务状态段、为进程从用户态陷入内核态切栈使用  struct taskstate ts;         // Used by x86 to find stack for interrupt  // 全局描述符  struct segdesc gdt[NSEGS];   // x86 global descriptor table  // 指示cpu是否启动  volatile uint started;       // Has the CPU started?  // 关中断的深度  int ncli;                    // Depth of pushcli nesting.  // 记录在cpu第一层关中断前，中断关闭情况  int intena;                  // Were interrupts enabled before pushcli?  // cpu当前运行的进程PCB  struct proc *proc;           // The process running on this cpu or null&#125;;extern struct cpu cpus[NCPU]; // 多个cpuextern int ncpu;              // cpu的个数\n\n下面列出了各个CPU资源的共享情况，可以做一个参考：\n\n\n\n资源\n共享\n不共享\n\n\n\n中断描述符表\n√\n\n\n\nlapic（也指外中断，包括定时器等）\n\n√\n\n\nioapic\n√\n\n\n\ncpu的各种寄存器，包括eip、esp、eflag等等\n\n√\n\n\n全局描述符表（包括任务状态段）\n\n√\n\n\nkpgdir（内核调度器使用的页表）\n√\n\n\n\n物理内存\n√\n\n\n\n任务队列（ptable）\n√\n\n\n\n调度器的执行栈\n\n√\n\n\n外设\n\n√\n\n\n接下来是XV6的PCB（进程控制块），之前老是在教科书上看到它，当时感觉很难理解，在分析过OS源码后，再回过头去看，就感觉特别通透。XV6的PCB就是一个结构体，里面存放了很多成员，XV6的PCB和进程的内核栈是分开的，PCB结构体是通过一个指针来指向进程的内核栈。相比之下，Onix的PCB和内核栈是连在一起的，内核栈的低地址就是存放的PDB结构体，因为esp是线下增长，所以esp指向高地址处，并且典型的内核栈大小是一页（4K）（于是esp指向页面的4K处）。\nXV6的PCB定义如下：\n// proc.henum procstate &#123; UNUSED, EMBRYO, SLEEPING, RUNNABLE, RUNNING, ZOMBIE &#125;;// Per-process statestruct proc &#123;  // 进程（映射了页框的）虚拟内存大小  uint sz;                     // Size of process memory (bytes)  // 进程的页表  pde_t* pgdir;                // Page table  // 进程的内核栈  char *kstack;                // Bottom of kernel stack for this process  // 进程状态  enum procstate state;        // Process state  // 进程pid  int pid;                     // Process ID  // 进程的父进程  struct proc *parent;         // Parent process  // 调用系统调用时的栈帧  struct trapframe *tf;        // Trap frame for current syscall  // 内核态的上下文  struct context *context;     // swtch() here to run process  // 等待条件  void *chan;                  // If non-zero, sleeping on chan  // 是否被杀死  int killed;                  // If non-zero, have been killed  // 打开的文件  struct file *ofile[NOFILE];  // Open files  // 工作目录的inode  struct inode *cwd;           // Current directory  // 进程名  char name[16];               // Process name (debugging)&#125;;// proc.cstruct &#123;  struct spinlock lock;       // 自旋锁  struct proc proc[NPROC];&#125; ptable;                     // 多个cpu之间共享该结构static struct proc *initproc; // 存放init进程PCBint nextpid = 1;              // 分配唯一pid\n\nXV6为进程定义了6种状态：UNUSED（PCB未使用）, EMBRYO（初始化中）, SLEEPING（阻塞休眠）, RUNNABLE（可调度）, RUNNING（运行中）, ZOMBIE（僵尸&#x2F;待回收）。 这里特别说明一下PCB的chan成员，该成员一个进程的等待条件。XV6中一个进程可能会调用sleep、wait系統調用，或者在調用read系統調用时间接调用了sleeplock，这些函数都会使一个进程进入阻塞状态，XV6的阻塞状态统一使用SLEEPING来表示，阻塞就是为了等待某个条件发生，当等待的条件发生时，阻塞的进程就会被唤醒，但是ptable有那么多阻塞的进程，我应该唤醒ptable中的哪些进程呢？此时chan就起到关键作用，在进程进入阻塞之前，会将chan设置为某一个变量的地址，当条件满足XV6就是通过chan来唤醒对应的进程的，当然这个变量的选取是很有讲究的，比如在XV6中因为sleep而休眠的进程，它的chan会被设置成ticks（作用类似jefrris，定时器中断的计数器）的地址。具体细节就不深入讨论，感兴趣的读者可以看看XV6的源码。\n从AP（从）处理器的启动段落我们知道，BSP、AP最终都进入scheduler函数，铺垫了这么久，scheduler函数也是本文的主题，那么先来看看它的代码实现吧：\n//PAGEBREAK: 42// Per-CPU process scheduler.// Each CPU calls scheduler() after setting itself up.// Scheduler never returns.  It loops, doing://  - choose a process to run//  - swtch to start running that process//  - eventually that process transfers control//      via swtch back to the scheduler.voidscheduler(void)&#123;  struct proc *p;  struct cpu *c = mycpu();  // 找到scheduler正运行在哪个cpu上  c-&gt;proc = 0;              // 将cpu结构体的当前运行的进程清零    for(;;)&#123;    // Enable interrupts on this processor.    sti();                  // 打开中断、之前中断一直是关闭的！    // 这里pushcli保存的mycpu()-&gt;intena其实没有意义，在切换后会被外面的sched直接覆盖。    // Loop over process table looking for process to run.    acquire(&amp;ptable.lock); // 共用ptable，所以需要获取自旋锁，互斥访问。    // 遍历ptable    for(p = ptable.proc; p &lt; &amp;ptable.proc[NPROC]; p++)&#123;      if(p-&gt;state != RUNNABLE)  // 进程状态不可运行        continue;      // else 找到了一个可运行的进程      // Switch to chosen process.  It is the process&#x27;s job      // to release ptable.lock and then reacquire it      // before jumping back to us.      c-&gt;proc = p;  // 设置p为cpu当前运行进程      //  这个函数非常关键，主要做两个三个操作：      // 1、设置tss，并将其追加到全局描述符      // 2、设置tss的选择子      // 3、让cpu使用进程p的页表。      switchuvm(p);      p-&gt;state = RUNNING; // 设置p为运行状态      swtch(&amp;(c-&gt;scheduler), p-&gt;context); // 正式开始切换，等到p放弃cpu才会返回      switchkvm();        // 切换回内核页表kpgdir      // Process is done running for now.      // It should have changed its p-&gt;state before coming back.      c-&gt;proc = 0;        // 当前运行的进程清零    &#125;    release(&amp;ptable.lock);  // 释放自旋锁  &#125;&#125;\n\nXV6的调度算法非常简单，就是简单的round robin算法。主要精华是整个调度的过程，至于它具体的调度算法其实显得并不是特别重要。\nswitchuvm函数实现非常关键，它里面会设置tss，并且设置cpu使用进程p的页表。tss全称是任务状态段，它可以帮助处于用户态的进程回到内核态，因为一个进程有两个栈，一个是出于用户态使用，另外一个是处于内核态使用，进程从内核态转变成用户态容易。只需要将中断帧弹出恢复上下文即可，但是从用户态回到内核态就难了，因为进入用户态后，进程的用户态空间不会保留进程任何内核态信息，所以，我们需要一个东西来帮助处于用户态的进程在需要陷入内核态时，找到它的内核态的栈，这个东西就是TSS，TSS会记录一个进程的内核栈的栈指针esp和栈段ss，switchuvm函数正是完成了这样的功能。实现如下：\n// Switch TSS and h/w page table to correspond to process p.voidswitchuvm(struct proc *p)&#123;  if(p == 0)    panic(&quot;switchuvm: no process&quot;);  if(p-&gt;kstack == 0)    panic(&quot;switchuvm: no kstack&quot;);  if(p-&gt;pgdir == 0)    panic(&quot;switchuvm: no pgdir&quot;);  pushcli();  // 因为tss也是一个段，所以向cpu的全局描述符中追加tss的描述符  mycpu()-&gt;gdt[SEG_TSS] = SEG16(STS_T32A, &amp;mycpu()-&gt;ts,                                sizeof(mycpu()-&gt;ts)-1, 0);  mycpu()-&gt;gdt[SEG_TSS].s = 0;  mycpu()-&gt;ts.ss0 = SEG_KDATA &lt;&lt; 3;                 // 内核栈段  mycpu()-&gt;ts.esp0 = (uint)p-&gt;kstack + KSTACKSIZE;  // 内核栈指针  // setting IOPL=0 in eflags *and* iomb beyond the tss segment limit  // forbids I/O instructions (e.g., inb and outb) from user space  mycpu()-&gt;ts.iomb = (ushort) 0xFFFF;  ltr(SEG_TSS &lt;&lt; 3);                                // tss段选择子  // 切换到p的页表  lcr3(V2P(p-&gt;pgdir));  // switch to process&#x27;s address space  popcli();&#125;\n\n关于任务状态段的详情描述可以参考：https://github.com/StevenBaby/onix/blob/dev/docs/07%20%E4%BB%BB%E5%8A%A1%E7%AE%A1%E7%90%86/062%20%E4%BB%BB%E5%8A%A1%E7%8A%B6%E6%80%81%E6%AE%B5%E7%AE%80%E4%BB%8B.md\nswtch函数由汇编实现，是进程切换的核心函数，实现如下：\n# Context switch##   void swtch(struct context **old, struct context *new);# # Save the current registers on the stack, creating# a struct context, and save its address in *old.# Switch stacks to new and pop previously-saved registers..globl swtchswtch:                # 这里有个pushl eip，cpu帮我们自动执行了  movl 4(%esp), %eax  # 第一个参数，struct context**  movl 8(%esp), %edx  # 第二个参数  struct context*  # Save old callee-saved registers  pushl %ebp  pushl %ebx  pushl %esi  pushl %edi  # Switch stacks  movl %esp, (%eax) # 加括号和在指针前面加*一个道理，这里保存当前内核栈指针到第一个参数上  movl %edx, %esp   # 将esp切换到第二个参数指向的内核栈上  # Load new callee-saved registers  popl %edi  popl %esi  popl %ebx  popl %ebp  ret\n\nstruct context结构体定义如下：\nstruct context &#123;  uint edi;  uint esi;  uint ebx;  uint ebp;  uint eip;&#125;;\n\n一个进程在（因为时间片用完）需要放弃cpu执行权限，如何回到scheduler呢？答案就是使用sched函数，本文我们以普遍的事件来分析————因为时间片用完而放弃的cpu。处于用户态的进程因为时间片用完会发生定时器中断，定时器中断又会引发从用户态到内核态的切栈、保存上下文、执行trap函数，trap函数中最后调用了yield，yield最终会调用sched，trap函数伪代码如下：\nstruct spinlock tickslock;uint ticks;//PAGEBREAK: 41voidtrap(struct trapframe *tf)&#123;  if(tf-&gt;trapno == T_SYSCALL)&#123;  // 是系统调用    // ...    return;  &#125;  switch(tf-&gt;trapno)&#123;  case T_IRQ0 + IRQ_TIMER:    if(cpuid() == 0)&#123;      acquire(&amp;tickslock);      ticks++;            // 每次定时器中断都自增1      wakeup(&amp;ticks);     // 唤醒sleeping进程，检查休眠是否到期      release(&amp;tickslock);    &#125;    lapiceoi();           // 通知lapic中断处理完毕    break;  // ...  &#125;  // ...  // Force process to give up CPU on clock tick.  // If interrupts were on while locks held, would need to check nlock.  if(myproc() &amp;&amp; myproc()-&gt;state == RUNNING &amp;&amp;     tf-&gt;trapno == T_IRQ0+IRQ_TIMER)    yield();    // 时间片用完，cpu让给下一个进程  // ...&#125;\n\nyield函数实现就是封装了一下sched函数，在调用sched之前，将进程的状态设置成了RUNNABLE状态：\n// Enter scheduler.  Must hold only ptable.lock// and have changed proc-&gt;state. Saves and restores// intena because intena is a property of this// kernel thread, not this CPU. It should// be proc-&gt;intena and proc-&gt;ncli, but that would// break in the few places where a lock is held but// there&#x27;s no process.voidsched(void)&#123;  int intena;  struct proc *p = myproc();  if(!holding(&amp;ptable.lock))    panic(&quot;sched ptable.lock&quot;);  if(mycpu()-&gt;ncli != 1)        // 最多一层关中断    panic(&quot;sched locks&quot;);  if(p-&gt;state == RUNNING)       // 在调用sched前，应该改变进程状态    panic(&quot;sched running&quot;);  if(readeflags()&amp;FL_IF)        // 中断必须被关闭！    panic(&quot;sched interruptible&quot;);  intena = mycpu()-&gt;intena;     // intena是进程私有的  swtch(&amp;p-&gt;context, mycpu()-&gt;scheduler); // 切换到scheduler  mycpu()-&gt;intena = intena;     // 还原intena&#125;// Give up the CPU for one scheduling round.voidyield(void)&#123;  acquire(&amp;ptable.lock);  //DOC: yieldlock  myproc()-&gt;state = RUNNABLE;  sched();  release(&amp;ptable.lock);&#125;\n\nsched中保存intena状态到进程的内核栈中的做法，好像把intena变量放到PCB中更合适，但是XV6没有这么做。从shced函数注释中了解到，如果把intena变量放到PCB中的话，有些情况下会有问题。具体呢，就不去细究了（我也每深究），本文内容太长了，还是以调度为主。这里主要是想表达一个点：scheduler函数给ptable.lock加锁时，pushcli保存的intena没有任何意义。因为最终在切换进程时，会被sched中进程的intena给覆盖掉。同样，在进程回到scheduler函数后，scheduler函数给ptable.lock解锁时，popcli还原的intena也没有任何意义，因为无论intena原来是否开中断，外层的for都会开中断！\n最终，一个待调度的进程的内核栈帧就形成了：\n\n一张图片概括yield、scheduler的加锁关系。如下图，进程利用yield进入调度器时会获取ptable的自旋锁（自旋锁内部会关中断，并且将关中断之前的中断状态保存到intena中），在切换到scheduler后（可能）会由scheduler解锁。在从scheduler切换到下一个任务前，（可能）scheduler会获取ptable的自旋锁，在却换到下一个任务后，由任务进行解除ptable的自选锁，注意这里是可能，因为还有可能scheduler的内层循环还没有执行完，以至于内层循环还可以找到下一个待执行的日任务，此时ptable的锁，就是：老进程加锁，新进程解锁：\n\n总的来说，XV6进程调度整体流程是：每个cpu上都运行调度线程，调度线程运行sheduler函数，scheduler不断从ptable取进行任务，然后（swapIn）切换去执行进程任务，当进程任务用完时间片（通过定时器中断）就会放弃cpu的执行权限，（swapOut）切换到内核调度线程继续去调度下一个进程任务。\n如果类比于用户态的协程的：对称协程和非对称协程之分吗的话，结合非对称协程的特点：协程的切换需要经过调度协程，而由于XV6进程的调度都必须经内核的过调度线程，所以XV6的调度器模型更像一种“非对称进程”。\n作为对比，如果你阅读过Onix的代码，你会发现Onix的调度模型更像是一种”对称进程“，因为Onix的进程切换是两个进程之间直接进行，不存在中间的调度线程。\n这里我可以用一张类似sylar的协程调度器模型来总结XV6进程调度模型：\n\n其实CPU Pool和线程池非常像，XV6的每个CPU都互斥到ptable中去取进程，然后去消化进程。唯一的区别就是CPU要和很多寄存器、硬件打交道，但是最终整体的框架思想都是一同百通，\n如果你看过sylar的源码，你会深有感触！ sylar的协程调度器模型和XV6进程的调度模型不能说像，只能是真的一模一样！ sylar是一个C++的基于协程的网络框架。我之前也有写过sylar的博客，这里推荐大家去看看：https://blog.csdn.net/m0_52566365/article/details/135991331。\n总结XV6有很多地方写的很暴力，有很大的优化空间，比如：\n\n\n\n\nXV6做法\nOnix做法\n\n\n\n内存管理\n不管是物理内存还是内核内存一股脑使用kalloc，用户页表所有的内容都靠kalloc\nget_page（使用256个页管理4G物理页，专门给页表和页框分配内存&#x2F;page） + alloc_kpage（专门给内核分配内核页所有的页目录都采用alloc_kpage&#x2F;page） + kmalloc（使用了内存池专门管理内核中的小快内存&#x2F;byte）\n\n\n内核对系统调用参数的获取\n直接访问用户栈空间\n使用ebp、edi、esi、edx、ecx、ebx寄存器获取系统调用参数\n\n\n软件定时器\n没有实现软件定时器\n利用链表实现了软件定时器\n\n\n内存探测\n未实现内存探测\nloader实现了内存探测\n\n\nidle任务\n没实现idle任务\n实现了idle任务\n\n\n…\n…\n…\n\n\n从上表可以看到Onix每一项都是存在优势的。但是Onix唯一的缺点，也是我读了Onix源码又来读XV6源码的原因：Onix是一个单核OS。其实读完XV6了解了多核OS的实现后，也没感到很大的震撼，多核CPU无非就是比单核CPU多了几套eip、esp、eflag等cpu相关的寄存器，cpu访问共享资源的的时候注意加锁就好了。\n最后谈谈XV6调度模型的优化：从XV6进程调度模型图我们可以看到，XV6的调度模型可以参考Muduo的One loop per thread 思想（可能说Muduo的One loop per thread思想参考了现代Linux对CPU的调度模型更合适？），因为XV6进程调度模型非常暴力，所有cpu共享有一个任务池（ptable），锁的竞争非常激烈。我们可以考虑让每个cpu都拥有一个自己独立的ptable（当然里面还是有自旋锁），由一个cpu负责负载均衡，将任务均匀的分发给各个cpu，需要修改cpux内部数据结构时，其他cpu只需向cpux的回调队列中添加操作函数即可，具体的操作还是由cpux自己完成。也即One loop per CPU。如下图：\n\n终于4干完了这篇文章，字数预计上万了，第一次写这么长的文章，也是真的用心了。创作不易，赏个赞把！\n参考资料\n多处理器规范：https://pdos.csail.mit.edu/6.828/2014/readings/ia32/MPspec.pdf\nXV6的官方中文文档：https://pdos.csail.mit.edu/6.828/2018/xv6/book-rev10.pdf\nOnix单核操作系统：https://github.com/StevenBaby/onix/\nAPIC中断讲解比较好的范文：https://blog.csdn.net/weixin_46645613/article/details/119207945\n多核处理器启动博客1：https://zhuanlan.zhihu.com/p/394247844\n\n本章完结\n","tags":["类Unix源码剖析"]}]